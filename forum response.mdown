DS Forum Response
====
[Is anyone else finding R and TM just be punishingly slow/memory wasteful?](https://class.coursera.org/dsscapstone-002/forum/thread?thread_id=125)

Jeff HeatonSignature TrackÂ· a day ago 
I am using a Mac Book Pro w/16gig ram and a latest generation i7 quadcore.  But doing this text mining stuff in R makes it feel like a Commodore 64.  I spent several days learning to sample the data down to sizes that would fit into memory, even though I am dealing with data less than 400meg.  I spent quite a bit of time watching R's TM package run single-threaded for hours only the chew through my 16gig of ram, crash and burn hours later.  I guess I could throw some Amazon EC2 muscle at it, with more ram.    I did get a tree-based n-gram frequency model working in R.  But I wanted to do more than just a 3-gram.

So I took another approach.  I just used Java, wrote my own ngram tree and also a context-based weighted model to use all "significant" words in a sentence.  I did not have to sample, I could use the entire dataset.  I cross-validated over the whole thing to tune my model params/sensitivity for the context, etc.  I was able to go through quite a few iterations to fix/adjust things.  Retraining takes under 5 min, using a thread pool.  I figure in the end I will either build my model in Java and move it into a shiny app, or worst case just port my final code to R, and resort to sampling, rather than cross validating over the whole dataset like I do now. 

My model worked great, I got a 100% on quiz 2. I guess, using an unorthodox approach for this class.  I did a number of exploratory graphs in R that I can use for the milestone report.

Am I missing something?  I really like R for modeling and graphing, once the data has been put into a form where it is ready to go to the model.  But doing heavy-lifting data transformations, with several 100 meg of data in R makes me feel like I am somewhat shackled. 

Jeff

# Reply
Hi Jeff, 

You sound like a much more advanced a programmer than I, so forgive me if this is obvious for you, but, I seem to have gone through the same issues and come out with some answers. 

Namely, after many days of struggle, my R operations run infinitely faster after I both broke the data up into slices of a reasonable size and loaded them into a list and did the operations on that list all at once, using lapply or sometimes that's not even needed, rather than using a for loop or doing the data all together.
 
For example, when I tried to load one of the large text files into a variable, then make corpus then make TDM, I had the same issue as you: everything was slow and finally some operation would run for an hour or two then freeze my PC completely (scary). 

Total turnaround occurred when I broke the file up to many files into 20,000 line chunks, (using unix split, though I'm sure there is a way to do this in R, anyone?) then loaded those into a list in R, then worked on that list as a whole, all my operations took about 5 minutes!

I am still not sure if this worked simply because it was in a list, or if it was because I broke the chunks into a good size that was neither too large or too small to be efficient. I will test further at some point but for now I'm just trying to solve Quiz 2. 

This link made my lightbulb go off: http://musicallyut.blogspot.com/2012/07/pre-allocate-your-vectors.html