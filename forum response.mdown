DS Forum Response
====
[Is anyone else finding R and TM just be punishingly slow/memory wasteful?](https://class.coursera.org/dsscapstone-002/forum/thread?thread_id=125)

Jeff HeatonSignature Track· a day ago 
I am using a Mac Book Pro w/16gig ram and a latest generation i7 quadcore.  But doing this text mining stuff in R makes it feel like a Commodore 64.  I spent several days learning to sample the data down to sizes that would fit into memory, even though I am dealing with data less than 400meg.  I spent quite a bit of time watching R's TM package run single-threaded for hours only the chew through my 16gig of ram, crash and burn hours later.  I guess I could throw some Amazon EC2 muscle at it, with more ram.    I did get a tree-based n-gram frequency model working in R.  But I wanted to do more than just a 3-gram.

So I took another approach.  I just used Java, wrote my own ngram tree and also a context-based weighted model to use all "significant" words in a sentence.  I did not have to sample, I could use the entire dataset.  I cross-validated over the whole thing to tune my model params/sensitivity for the context, etc.  I was able to go through quite a few iterations to fix/adjust things.  Retraining takes under 5 min, using a thread pool.  I figure in the end I will either build my model in Java and move it into a shiny app, or worst case just port my final code to R, and resort to sampling, rather than cross validating over the whole dataset like I do now. 

My model worked great, I got a 100% on quiz 2. I guess, using an unorthodox approach for this class.  I did a number of exploratory graphs in R that I can use for the milestone report.

Am I missing something?  I really like R for modeling and graphing, once the data has been put into a form where it is ready to go to the model.  But doing heavy-lifting data transformations, with several 100 meg of data in R makes me feel like I am somewhat shackled. 

Jeff

# Reply
Hi Jeff, 

You sound like a much more advanced a programmer than I, so forgive me if this is obvious for you, but, I seem to have gone through the same issues and come out with some answers. 

Namely, after many days of struggle, my R operations run infinitely faster after I both broke the data up into slices of a reasonable size and loaded them into a list and did the operations on that list all at once, using lapply or sometimes that's not even needed, rather than using a for loop or doing the data all together.
 
For example, when I tried to load one of the large text files into a variable, then make corpus then make TDM, I had the same issue as you: everything was slow and finally some operation would run for an hour or two then freeze my PC completely (scary). 

Total turnaround occurred when I broke the file up to many files into 20,000 line chunks, (using unix split, though I'm sure there is a way to do this in R, anyone?) then loaded those into a list in R, then worked on that list as a whole, all my operations took about 5 minutes!

I am still not sure if this worked simply because it was in a list, or if it was because I broke the chunks into a good size that was neither too large or too small to be efficient. I will test further at some point but for now I'm just trying to solve Quiz 2. 

This link made my lightbulb go off: http://musicallyut.blogspot.com/2012/07/pre-allocate-your-vectors.html

# Patrick ChamberlainSignature Track· 8 hours ago 
I've created a TDM from 100,000 lines of the twitter file without any problem - but 1/3 of the twitter file just leads to either an immediate out of memory error, or worse hours go by then nothing and I have to kill it. 

@Michael do you mean split the files up, put them in one directory and then use tm to load them all into one Corpus?   

## Response

Hi Patrick,  

I think that loading to one corpus would cause the same problem, I think the crashing point is when you try to make a TDM from a very large corpus. What I did is to keep the chunks separated until TDM function so that you have a list of smaller corpora instead of one large corpus.

First, I split up into many files (20,000 lines each, but 100,000 may work as well). Then I used a for loop to load these into a list, so myList[1] is the first 20,000 lines, myList[2] is the next etc.

Then I used lapply for some cleanup using iconv and gsub on the whole list.

Then, I made a custom function which runs all of my tm package functions in order: make a corpus, perform tm_map transformations, then make TDM.

Then I simply ran that custom function on the list and it works very fast. This turns your list of text into a list of corpora,  rather than one giant one. Then, the TermDocumentMatrix function works much faster and combines them in the end to one TDM somewhat magically. (I was surprised that my end result was one large TDM instead of a list of TDMs, but that seems to be the case, and is my intention of course, so, great!)