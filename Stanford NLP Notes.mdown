# OK I need to go through this stanford NLP more carefully

I do not understand how they make bigrams. They skip a word?
Seems like maybe bigrams use stopword elimination but trigrams and unigrams do not.

# [intro n-grams](https://class.coursera.org/nlp/lecture/14)

1. we should be able to calculate the joint probability of an entire sentence by P(A)P(B|A)P(C|A,B) etc. (chain rule)

2. Markov assumption skips over words to generate probabilities, perhaps just the first and last two words of a sentence.

# [Estimating n-gram probabilities](https://class.coursera.org/nlp/lecture/128)

1. bigram estimates:
*First, key note that they use <s> and </s> so that start and end of sentence are considered part of a bigram.*

if sentence is "A B C D E"
P(A|<s>)\*P(B|A)\*P(C|B)...P(</s|E)

so here it's all bigrams.

use log probabilities because adding them is faster than multiplying.

## [log probability](http://en.wikipedia.org/wiki/Log_probability)
Since the log of a number in [0,1] is negative, negative log probabilities are more commonly used.

x' in probability means negation of an event, so the probability it does NOT happen (1-P)
-

SRILM toolkit, google n-gram
# [Evaluation and Perplexity](https://class.coursera.org/nlp/lecture/129)

Perplexity is the probability of the test set, normalized by the number of words.

Normalization is done by putting the probability to the -1/N power, where N is the number of bigrams in a sentence (same as number of words? Not if using tokens it would be words+1.
Idea being that many words are less likely...

Perplexity is inversely related to probability.
"How many things could occur next?""
average branching factor

So of course, the tree/branching idea would be that we are reducing the perplexity as it goes along.

perplexity = weighted equivalent branching factor

so PP(x)=P(x)^(-1/N)

# [Generalization and Zeros](https://class.coursera.org/nlp/lecture/17)

Any zero probability bigram (simply not in the test corpus), will be assigned 0 probability and cause the sentence to be 0 probability, and perplexity uncalculable (divide by zero)

# [Smoothing: Add One](https://class.coursera.org/nlp/lecture/18)

We need to take away from counted bigrams if we want to add to 0 bigrams to keep ratios the same.

Simplest way is Laplace smoothing - just add 1 to everything.

MLE Maximum Likelihood Estimate - the one that maximizes the likelihood of the training set T given the model M - just the count of something over the total number. key point here is just that it is for a training corpus, may be totally off in another corpus.

MLE LM (language model) assigns highest probability to the training corpus thus has lowest perplexity. Smoothed PP will be higher.

We DO NOT USE add-1 smoothing for n-grams because there are TOO many zeroes, it totally skews our data.

# [INTERPOLATION / BACKOFF](https://class.coursera.org/nlp/lecture/19)

Backoff means - just use trigram if you have a good match, but otherwise stepdown to bigrams 
(this is the model I'm using now I believe)

*INTERPOLATION WORKS BETTER THAN BACKOFF.*
Simple Interpolation - add together uni, bi, and trigram each multiplied by weights (lambda1,2,3) and summed
Lambdas conditional on context - lambdas multiplied by probabilities of word one and two before it.


Interpolation says use a mix of unigram, bigram, trigram always.
Lambda must sum to 1.
So with 3 types of grams, we have lambda=1/3 for each.

So the linear interpolation probability model is:
P(Sam|I am) = P(Sam)/3+P(Sam|am)/3+P(Sam|I am)/3

in other words, we take the probability of the trigram over all sentences, and this is lambda. THen we multiply the unigram, bigram and trigram all by that... ? *Or is it 1/3 because of 3 types of grams??*

P(Sam) = #Sam/#total WORDS (unigrams)
P(Sam|am) = #S
P(Sam|I am) = #(Sam I am)/#ALL SENTENCES.

Choose lambdas that maximize probability of held-out data (like a second test set...)

so 1. fix n-gram probabilities on training data
2. search for lambdas that give largest probability to held-out set

	log P(w1...wn)|M(lambda1...lambdak) = Sigma log PM(lamba1...lambdaK(w1...wn)

## OOV = out of vocabulary words
create unknown word token <UNK>

So we filter out low-probability words, a fixed lexicon L of size V.

then any words not in training set are <UNK>

## Pruning

only keep n-grams with count>threshold
remove singletons of higher-order n-grams
entropy-based pruning - advanced.

## Back-off

Stupid back-off - not probabilities, scores (greater than 1 is possible). Only relative frequencies multiplied by a constant rate.

So if trigram occurs, I use that, if not, I use bigram probability.
I don't discount whether the probability comes from bigrams or trigrams.

Stupid back-off good for very large n-grams. (web-scale)

Extended interpolated kneser-ney - more used, discussed later.

## Advanced Language Models
Discriminative models - choose n-gram weights to improve a task.

Parsing-based models - discussed later

Caching models - recently used words more likely 

#[Good-Turing Smoothing](https://class.coursera.org/nlp/lecture/32)
Probability

N1 is the number of things that occurred once.
we use that probability as our estimate
(reserve probability mass for the unseen things)

c is count, n is number, so they are really the same?
N is the total number, N1 is number of 1 counted (c=1 items), etc.

P*GT (things with zero frequency = c* = (c+1)*Nc+1 / Nc)
we add one to all because of smoothing.

so if N1 is 3, total is 18, 3/18 is P*GT (unseen)
If trout was seen once before, it was 1/18, but now it's 2*N2 (the number of things seen twice) divided by N1.

So the Good-Turing estimated count of your highest count will be 0, because the count of Nc+1 will be 0.

so for large k in Nk, too jumpy and zeros wreck estimates.
So in Simple Good-Turing we replace Nk with a best-fit power law once it gets unreliable (smooth curve)

# [Kneser Ney](https://class.coursera.org/nlp/lecture/20)
better estimate for probabilites of lower-order unigrams.
Find probability that a unigram is likely to appear as a novel continuation (an unseen bigram)

of all word bigram types, how many have w as a continuation

|{wi-1:c(wi-1,w)>0}|

So Francisco has a high unigram probability because of San Francisco, but if it only occurs after San, it will have a very low continuation probability.

## cardinality, set notation
the cardinality of a set is a measure of the "number of elements of the set". For example, the set A = {2, 4, 6} contains 3 elements, and therefore A has a cardinality of 3. 

|{wi-1:c(wi-1,w)>0}|
It says "the set of all wi-1's, such that the count of wi-1, w is greater than 0". Sometimes | is used in place of :.


----
# Ref
[Ref?](http://nlp.stanford.edu/IR-book/html/htmledition/contents-1.html)