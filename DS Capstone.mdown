#DS Capstone
============

The capstone will be evaluated based on the following assessments:

1. An introductory quiz to test whether you have downloaded and can manipulate the data.
2. An intermediate R markdown report that describes in plain language, plots, and code your exploratory analysis of the course data set.
3. Two natural language processing quizzes, where you apply your predictive model to real data to check how it is working.
4. A Shiny app that takes as input a phrase (multiple words), one clicks submit, and it predicts the next word.
5. A 5 slide deck created with R presentations pitching your algorithm and app to your boss or investor. 

#Assessments and Grading
Your course score will be based on three pieces of work
(Chicago will stay on CDT until Sunday, November 2, 2014 going to CST)
CST=UTC-6 
*11:30PM UTC = 5:30PM CST*
Quiz 1: Getting Started (5%) - Due November 9th @ 11:30 PM UTC
A milestone report on exploratory analysis of the data set (20%) - Due November 16th @ 11:30 PM UTC
Quiz 2: Natural Language Processing I (10%) - Due November 16th @ 11:30 PM UTC
Quiz 3: Natural Language Processing II (10%) - Due November 23rd @ 11:30 PM UTC
Final Project: your data product and a presentation describing your final data product (55%) - Due December 14th @ 11:30 PM UTC

## Lateness: Quizzes only!
5 Late Days for quizzes only
The reported due date is the soft deadline for each quiz. You may turn in quizzes up to two days after the soft deadline. The hard deadline is the Tuesday after the Quiz is due at 23:30 UTC-5:00. Each day late will incur a 10% penalty, but if you use a late day, the penalty will not be applied to that day.

# Quiz 1: Getting Started

## 1. The en_US_blogs.txt file is 200MB
### Explanation:
Do `ls -alh` in the `Coursera-Swiftkey/final/en_US` directory.

## 2. The en_US.twitter.txt has how 2360148 lines (plus one blank)
### Explanation:
Do `wc -l en_US.twitter.txt` at the prompt (or git bash on windows) or `length(readLines("en_US.twitter.txt"))` in R

## Question 3: What is the length of the longest line seen in any of the three en_US data sets?

1. OK, this code reads in the lines to arrays:

```r
setwd("C:/Users/Michael/SkyDrive/Code/GitHub/DSCapstone/Coursera-SwiftKey/final/en_US")
fileName="en_US.blogs.txt"
con=file(fileName,open="r")
lineBlogs=readLines(con) 
longBlogs=length(line)
close(con)

fileName="en_US.news.txt"
con=file(fileName,open="r")
lineNews=readLines(con) 
longNews=length(line)
close(con)

fileName="en_US.twitter.txt"
con=file(fileName,open="r")
lineTwitter=readLines(con) 
longTwitter=length(line)
close(con)

<!-- for (i in 1:long){
    linn=readLines(con,1)
    print(linn)
}
 -->
```

2. Now, I don't really need the length though, I need the longest line in each array.
	1. Can I get the length of ONE item? length(as.character()) does NOT work.
		`nchar` ? Yes, this works.
	2. I can make a new array for each of nchars.

```r
longBlogs = nchar(longBlogs) #Sweet, this works.
```
	3. So, just this? 
```r
max(nchar(longBlogs)) # incorrect.
```
Beware nchar(NA) –  hadley Mar 19 at 16:02
  	 	
@hadley Indeed, or for that matter any character vector with one or more NAs. (Though this is documented to be so). –  Gavin Simpson Mar 19 at 16:19 
1	 	
Or use stri_length from stringi - it works fine with NA's and it is faster :) Check my post! –  bartektartanus Apr 4 at 16:37

	4. Use stringi package and stri_length function
	
	stri_length(c("ala ma kota","ABC",NA))
	[1] 11  3 NA
Why? Because it is the FASTEST among presented solutions :)
and also works fine with NA's

OK so apparently 40835 is max of lineBlogs

```r
require(stringi)
longBlogs<-stri_length(lineBlogs)
max(longBlogs) #40835

###
#to find which line that is:
which(a==40835)
[1] 483415
###

longNews<-stri_length(lineNews)
max(longNews) #5760

longTwitter<-stri_length(lineTwitter)
max(longTwitter) #213

```
### Explanation:
Again a simple wc command suffices `wc -L *.txt` in the directory with the three files. Note, we had a small discrepancy between doing this in R versus WC.

## Question 4 In the en_US twitter data set, if you divide the number of lines where the word "love" (all lowercase) occurs by the number of lines the word "hate" (all lowercase) occurs, about what do you get?

1. Find out how to get count of lines with "love" (case-sensitive), store in variable
	grep? 

```r
loveBlogs<-grep("love",lineBlogs)
length(loveBlogs) #49167
# oops, i only need twitter!
loveTwitter<-grep("love",lineTwitter)
length(loveTwitter) #90956
# 2. do same for "hate"
hateTwitter<-grep("hate",lineTwitter)
length(hateTwitter) #22138
# 3. divide love by hate
90956/22138 #4.108592
```

`grep "love" en_US.twitter | wc -l` and `grep "hate" en_US.twitter | wc -l` gives you the counts. Then you could divide in whatever. If you never want to leave the console, you can use bc (not present on gitbash in windows). You could also read into R (readLines) and use character search. This worked on gitbash `love=$(grep "love" en_US.twitter.txt | wc -l) then hate=$(grep "hate" en_US.twitter.txt | wc -l)` then `let m=love/hate` then `echo $m`
```
## Question 5
The one tweet in the en_US twitter data set that matches the word "biostats" says what?

```r
biostatsTwitter<-grep("biostats",lineTwitter)
lineTwitter[biostatsTwitter]
```
### Explanation:
`grep -i "biostat" en_US.twitter.txt` (note the -i doesn't matter since there's only one line ignoring case).

## Question 6
How many tweets have the exact characters "A computer once beat me at chess, but it was no match for me at kickboxing". (I.e. the line matches those characters exactly.)

```r
sentenceTwitter<-grep("A computer once beat me at chess, but it was no match for me at kickboxing",lineTwitter)
length(sentenceTwitter) #

```
### Explanation:

`grep -x "A computer once beat me at chess, but it was no match for me at kickboxing" en_US.twitter.txt | wc -l`

# Task 1: Data acquisition and cleaning
## Tasks to accomplish

1. Tokenization - identifying appropriate tokens such as words, punctuation, and numbers. Writing a function that takes a file as input and returns a tokenized version of it.

2. Profanity filtering - removing profanity and other words you do not want to predict.

## 1. Tokenization - identifying appropriate tokens such as words, punctuation, and numbers. Writing a function that takes a file as input and returns a tokenized version of it.

	Let's work with the smallest dataset first for speed reasons.
This is *news* with 77259 lines vs. twitter 2360148 vs. blogs 899288

	1. clean up workspace from quiz 1, removing some variables.
This leaves just 6: line and long for each dataset. 
*line are the lines, long are the line lengths*
	
	2. Write tokenizing functions
### Isolate Words
	1. let's start with one sentence.
```r
a<-lineNews[1]
 a
[1] "He wasn't home alone, apparently."
```
	2. How to split those words into an array? 

[](http://stackoverflow.com/questions/11927121/r-convert-string-to-vector-tokenize-using)
You can use strsplit to accomplish this task.

```r
string1 <- "This is my string"
strsplit(string1, " ")[[1]]
#[1] "This"   "is"     "my"     "string"
```

"\\s+" will split for any amount of white space including newlines.

Try that:

```r
b<-strsplit(a, " ")
b
[[1]]
[1] "He"          "wasn't"      "home"        "alone,"      "apparently."
```
Can I access each word individually?
>not easily, needs further manipulation, see lists link in reference.

#### Document Term Matrix?
[](http://www.inside-r.org/packages/cran/tm/docs/DocumentTermMatrix)

[](http://www.unt.edu/rss/class/Jon/Benchmarks/TextMining_L_JDS_Jan2014.pdf)

Now we can load the ‘tm’ package (Feinerer & Hornik, 2013) and convert our vector of character strings into a recognizable corpus of text using the ‘VectorSource’ function and the ‘Corpus’ function.

```r
txt<-VectorSource(lineNews)
txt.corpus<-Corpus(txt)
inspect(txt.corpus)
```
*OK i think that worked.*

Next, we make some adjustments to the text; making everything lower case, removing punctuation, removing numbers, and removing common English stop words. The ‘tm map’ function allows us to apply transformation functions to a corpus.

```r
txt.corpus<- tm_map(txt.corpus,tolower)
txt.corpus<- tm_map(txt.corpus,removePunctuation)
txt.corpus<- tm_map(txt.corpus,removeNumbers)
txt.corpus<- tm_map(txt.corpus,removeWords,stopwords("english"))
```
Next we perform stemming, which truncates words (e.g., “compute”, “computes” & “computing” all become “comput”). However, we need to load the ‘SnowballC’ package (Bouchet-Valat, 2013) which allows us to identify specific stem elements using the ‘tm map’ function of the ‘tm’ package.
*Not going to do this at this time*

##### Below is not clear, XXX:

```r
require(tm)
corpus<- lineNews
(f <- content_transformer(function(x, pattern) gsub(pattern, "", x)))
tm_map(crude, f, "[[:digit:]]+")[[1]]

corpus <- tm_map(corpus, content_transformer(tolower))
tdm <- TermDocumentMatrix(corpus,
                          control = list(removePunctuation = TRUE,
                                         stopwords = TRUE))
dtm <- DocumentTermMatrix(corpus,
                          control = list(weighting =
                                         function(x)
                                         weightTfIdf(x, normalize =
                                                     FALSE),
                                         stopwords = TRUE))
inspect(tdm[155:160,1:5])
inspect(tdm[c("price", "texas"),c("127","144","191","194")])
inspect(dtm[1:5,155:160])
```
### Isolate Punctuation
### Isolate Numbers

# Reference
## Unix / Bash / Console / Gitbash
`wc` (short for word count) - generates one or more of the following statistics: newline count, word count, and byte count. (all, in that order)
	Newer versions of wc can differentiate between byte and character count. This difference arises with Unicode which includes multi-byte characters. The desired behaviour is selected with the -c or -m switch.
	wc -l <filename> print the line count (note that if the last line does not have \n, it will not be counted)
	wc -c <filename> print the byte count
	wc -m <filename> print the character count
	wc -L <filename> print the length of longest line
	wc -w <filename> print the word count



## R
### Lists
[](http://stackoverflow.com/questions/14376506/how-to-extract-elements-from-list-of-lists)

### Double Brackets in R? [[]] [[1]]
The [R Language Definition](http://cran.r-project.org/doc/manuals/R-lang.html#Indexing) is handy for answering these types of questions:

R has three basic indexing operators, with syntax displayed by the following examples

```r
    x[i]
    x[i, j]
    x[[i]]
    x[[i, j]]
    x$a
    x$"a"
 ```
For vectors and matrices the [[ forms are rarely used, although they have some slight semantic differences from the [ form (e.g. it drops any names or dimnames attribute, and that partial matching is used for character indices). When indexing multi-dimensional structures with a single index, x[[i]] or x[i] will return the ith sequential element of x.

For lists, one generally uses [[ to select any single element *(character)*, whereas [ returns a *list* of the selected elements.

The [[ form allows only a single element to be selected using integer or character indices, whereas [ allows indexing by vectors. Note though that for a list, the index can be a vector and each element of the vector is applied in turn to the list, the selected component, the selected component of that component, and so on. The result is still a single element.
### Basic character string functions provided by R:

	nchar
	string length
	paste
	concatenate strings
	substr
	substring
	toupper
	convert entire string to uppercase
	tolower
	convert entire string to lowercase
	chartr
	character map replacement (like "tr")
	strtrim
	truncates string
	nchar, substr, toupper, tolower will accept string vectors as arguments and return vector results.
	strtrim accepts both a vector of strings and a vector of truncation positions.

### Functions which work with regular expression patterns

	strsplit
	split string into substrings at occurances of regexp
	grep
	search for a regular expression within a string
	sub
	search and then replace an occurance of a regular expression in a strng
	gsub
	global search and replace all occurances of a regular expression in a string
