Task 2 To do List
----
# Next steps
0. Perhaps I should insert the sentence tokens, 
because this enables trigrams that would not otherwise exist because they only start or end sentences. Although that would mean I would sometimes be predicting the end of the sentence which I do not want to do...

1. Right now I'm runing twitter on 1e6 lines after previously only using 1e5. 1e5 took 13.78 minutes, so this: 130.78. Probably I should have done less, especially knowing that this may crash at some point.

----
1. Let's try to combine the corpora tables to have something to do while we read the PDF?

2. Read PDF

3. Spell correction would be a great addition, and reprocess with some specifics, but that's not top priority right this moment.

4. do log probabilities in the data.table rather than on prediction?

## Results testing on prediction 5, first Upred.nbt.RDS:
Quiz2.1 - beer was #10 with just trigrams, bigrams screwed it up.
a case of AMERICA?? 
Unigrams: a case of SURE??? WTF?

I think i should do trigrams only, posibly even quadrigrams and above instead of this.

----------------

data.table... 
Try Model8.6.5grams.blogsCorpus.R with data.tables instead.
potential to run findAssoc ever?

1. a key issue is how much memory i can use.
Apparently I could use up to 4096 MB on shiny by running:
`shinyapps::configureApp("APPNAME", size="xxlarge")`
but potential for being forced down to 256.
http://shiny.rstudio.com/articles/shinyapps.html
For now though I guess we should develop for *4096* because any shrinking will sacrifice speed and accuracy.

We shall see but it seems U,B,Tfreq may be about 1GB per corpus so that would work.

2. 
Sample using biased coin?
for each line:
rbinom 


-------------------

2. 	Abandoning disk-writing approaches because they are way too slow.

3. Back to original TDM model, but use removesparseterms to get down to total 4 GB in each TDM at least, potentially in all combined. Don't expect 100% accuracy.

4. Need to separate tri/bi/unigrams or all in one TDM?

5. Prediction:
basic model is the probability = count of trigram/count of bigram

6. Tree structure:
Naturally, much speedier than searching through all trigrams, all bigrams and all unigrams separately would be a tree structure.

it would be built unigram-bigram-trigram, but then you would go up it (backwards) for the prediction.

Peng indicaes this is not effective.

#Overview:
Our aim is to get a finite list of probabilities for what the next word is. We are building an equation that tells us this.

1. Figure out how to efficiently (memory-wise) do a unigram model to start. 
	
	1. MapReduce - https://class.coursera.org/datasci-002/lecture/67

2. Add bigrams and trigrams, interpolation and smoothing techniques from Week 1 of https://class.coursera.org/nlp/lecture.
	
	1. Start and end sentence token insert as in Stanford course? (<s>, </s>)

3. Classification / unusual situations

	1. Make a database which finds all sentences which have 2 unusual words together, (like offense/defense).
	Another way to say it - are there certain words that are highly probable to be together in one sentence, though they are unusual overall?

----
In fact, perhaps building a sentence database is the first step. [Inverted Index](https://class.coursera.org/datasci-002/lecture/77)

1. Question: Can i use SQLShare? or SQL in general?
	Hm... maybe in the future but as is I don't think it is setup for this?
	

# Break to sentences first. DONE

## Question: Should I insert start and end tokens? 
I think this would only be useful in a full fledged model where I was trying to really predict people typing... i dunno really what is the point?

# Using this sentence data, we can create map to 4 different databases:

## ASSOCIATED WORDS:  returns a list of all words that fall in the same sentence as that word?
Lets build this first since it is a new concept...
So for this, we can use findAssoc.

### Is stemDocument really that great?

## TRI,BI,UNIGRAMS.

## After we have the basic model to build all above, we will need to work on making it work over all the data.

Options:
1. AWS
2. a formula that goes piece by piece then offloads to disk. At the end, the disk pieces can be combined in a separate operation?
3. ff package or other. *PCorpus? filehash??*
4. SIMPLY CUTTING a lot out... Replacing them with <UNK>, which could be a trigger to look towards related sentence words only?

### filehash
This seems very promising.
But i still don't understand best way to get Corpus into that.

*I guess PCorpus is not useful because it will be the TDM and frequency tables that I want to access permanently, not the Corpus.*

# OK next step: 
1. I need to make the filehashing a function that runs on each chunk individually so that memory is not overloaded.

So, I got a running omdel of findAssoc BUT it takes 3 minutes per 500 lines therefore running twitter data would be: So, if i go 500 at a time. well hell just make 500 chunsize then. # so that will give me 4720 chunks, and take 16520 HOURS to process lol. so 3 years. Yeah that's not real tenable.

So for now let's work on the trigrams and leave findAssoc possibly only for the most relevant words or even maybe just the quiz.

2. Then I can run it on all Twitter to see how feasible it is size/speed-wise.

Predictions:
1. will need to deal with unfound terms like "ain't"..

#*all of these things take too long: but would it be faster if I somehow did all in memory and then wrote it all at once?*

To test this, let's time it on 10000 and see how it is. 
Because 500 was 94 seconds. no let pick a smaller test.
what should be 10 minutes is: 3191 lines.
so we'll run that and time it.
now, everything in database is getting double-counted which could have some affect. well only the first 500 though so negligible.

Now, AWS gives 750 hours free, and I could use those in parallel.
That would be enough to process the Trigram at least.

I believe the reason trigram was so fast before is that I was abandoning the TDM document aspect? Maybe I need to do that again (probably).

But actually, the slow part is not the TDM making, it is the writing to database?
This is taking even more than 10 minutes I think (3191 lines)

Can I skip the TDM process?

# spelling correction would help...