Task 2 To do List
----

Overview:
Our aim is to get a finite list of probabilities for what the next word is. We are building an equation that tells us this.

1. Figure out how to efficiently (memory-wise) do a unigram model to start. 
	
	1. MapReduce - https://class.coursera.org/datasci-002/lecture/67

2. Add bigrams and trigrams, interpolation and smoothing techniques from Week 1 of https://class.coursera.org/nlp/lecture.
	
	1. Start and end sentence token insert as in Stanford course? (<s>, </s>)

3. Classification / unusual situations

	1. Make a database which finds all sentences which have 2 unusual words together, (like offense/defense).
	Another way to say it - are there certain words that are highly probable to be together in one sentence, though they are unusual overall?

----
In fact, perhaps building a sentence database is the first step. [Inverted Index](https://class.coursera.org/datasci-002/lecture/77)

1. Question: Can i use SQLShare? or SQL in general?
	Hm... maybe in the future but as is I don't think it is setup for this?
	

# Break to sentences first. DONE

## Question: Should I insert start and end tokens? 
I think this would only be useful in a full fledged model where I was trying to really predict people typing... i dunno really what is the point?

# Using this sentence data, we can create map to 4 different databases:

## ASSOCIATED WORDS:  returns a list of all words that fall in the same sentence as that word?
Lets build this first since it is a new concept...
So for this, we can use findAssoc.

### Is stemDocument really that great?

## TRI,BI,UNIGRAMS.

## After we have the basic model to build all above, we will need to work on making it work over all the data.

Options:
1. AWS
2. a formula that goes piece by piece then offloads to disk. At the end, the disk pieces can be combined in a separate operation?
3. ff package or other. *PCorpus? filehash??*
4. SIMPLY CUTTING a lot out... Replacing them with <UNK>, which could be a trigger to look towards related sentence words only?

### filehash
This seems very promising.
But i still don't understand best way to get Corpus into that.

*I guess PCorpus is not useful because it will be the TDM and frequency tables that I want to access permanently, not the Corpus.*

# OK next step: 
1. make sure I can get the results in a format I can use.... 
2. will need to deal with unfound terms like "ain't"..
2. Then I can run it on all Twitter to see how feasible it is size/speed-wise.

3. 