# Understanding `tm` Package

# Next
Next, we can begin to explore the TDM, using the ‘findFreqTerms’ function, to find which words were used most. Below we specify that we want term / word stems which were used 8 or more times (in all documents / paragraphs).
`findFreqTerms(x = tdm, lowfreq=18, highfreq=Inf)`

Next, we can use the ‘findAssocs’ function to find words which associate together. Here, we are specifying
the TDM to use, the term we want to find associates for, and the lowest acceptable correlation limit
with that term. This returns a vector of terms which are associated with ‘comput’ at r = 0.60 or more
(correlation) – and reports each association in descending order.
`findAssocs(x=tdm,term="comput",corlimit=0.6)`

*This means words associated in 60% of documents to be displayed, hence for many small documents like you are doing, not a good number to use!*

If desired, terms which occur very infrequently (i.e. sparse terms) can be removed; leaving only the‘common’ terms. Below, the ‘sparse’ argument refers to the MAXIMUM sparse-ness allowed for a term to be in the returned matrix; in other words, the larger the percentage, the more terms will be retained (the smaller the percentage, the fewer [but more common] terms will be retained).

`tdm.common.60<-removeSparseTerms(x=tdm,sparse=0.60)`

========
# BASICS
## Term-Document Matrix vs. Document-Term Matrix vs. ?
In the tm package the classes TermDocumentMatrix and DocumentTermMatrix (depending on whether you want terms as rows and documents as columns, or vice versa) employ sparse matrices for corpora.


## Introduction to the tm Package.pdf
methods for data import, corpus handling, preprocessing, metadata management,
and creation of term-document matrices.
Our focus is on the main aspects of getting started with text mining
in R - an in-depth description of the text mining infrastructure offered by tm was published in the Journal of Statistical Software (Feinerer et al., 2008)[]("C:\Users\Michael\SkyDrive\Code\GitHub\DSCapstone\Text Mining Infrastructure in R.pdf"). An introductory article on text mining in R was published in R News (Feinerer, 2008).

## Data Import / Structures / VCorpus vs. PCorpus
The main structure for managing documents in tm is a so-called Corpus, representing a collection of text documents. A corpus is an abstract concept, and there can exist several implementations in parallel. The default implementation is the so-called *VCorpus (short for Volatile Corpus)* which realizes a semantics as known from most R objects: corpora are R objects held fully in memory. We denote this as volatile since once the R object is destroyed, the whole corpus is gone. Such a volatile corpus can be created via the constructor `VCorpus(x, readerControl)`. Another implementation is the *PCorpus which implements a Permanent Corpus* semantics, i.e., the documents are physically stored outside of R (e.g., in a database), corresponding R objects are basically only pointers to external structures, and changes to the underlying corpus are reflected to all R objects associated with it. Compared to the volatile corpus the corpus encapsulated by a permanent corpus object is not destroyed if the corresponding R object is released.

Within the corpus constructor, `x` must be a Source object which abstracts the input location. tm provides a set of predefined sources, e.g., `DirSource, VectorSource, or DataframeSource`, which handle a directory, a vector interpreting each component as document, or data frame like structures (like CSV files), respectively. Except DirSource, which is designed solely for directories on a file system, and VectorSource, which only accepts (character) vectors, most other implemented sources can take connections as input (a character string is interpreted as file path). `getSources()` lists available sources, and users can create their own sources.

The second argument `readerControl` of the corpus constructor has to be a list with the named components `reader and language`.

# Inspecting Corpora

`print()` gives a concise overview whereas the full content of text documents is displayed with `inspect()`.

Individual documents can be accessed via [[, either via the position in the corpus, or via their identifier.
	
	meta(ovid[[2]], "id")
	[1] "ovid_2.txt"
	identical(ovid[[2]], ovid[["ovid_2.txt"]])
	[1] TRUE

The content() and meta() getters and setters aim at providing a consistent high-level interface to the respective information (abstracting from how classes internally represent the information).

# Transformations

Once we have a corpus we typically want to modify the documents in it, e.g., stemming, stopword removal, et cetera. In tm, all this functionality is subsumed into the concept of a transformation. Transformations are done via the `tm_map()` function which applies (maps) a function to all elements of the corpus. Basically, all transformations work on single text documents and `tm_map()` just applies them to all documents in a corpus.

## Eliminating Extra Whitespace
Extra whitespace is eliminated by:
	
	reuters <- tm_map(reuters, stripWhitespace)

## Convert to Lower Case
Conversion to lower case by:
	
	reuters <- tm_map(reuters, content_transformer(tolower))

We can use arbitrary character processing functions as transformations as long as the function returns a text
document. In this case we use content_transformer() which provides a convenience wrapper to access and
set the content of a document. Consequently most text manipulation functions from base R can directly be used
with this wrapper. This works for tolower() as used here but also e.g. for gsub() which comes quite handy
for a broad range of text manipulation tasks.

## Remove Stopwords
Removal of stopwords by:

	reuters <- tm_map(reuters, removeWords, stopwords("english"))

## Stemming

Stemming is done by:

	tm_map(reuters, stemDocument)

# Issues to consider / Improvements to make in future

## Before lowercasing we should identify proper nouns 
*decision: come back to this later after basic model is built*

```r
## NOW I see a problem already, which is that by lowercasing all we have made confusing names which seem like words now...
#So, let's go back to corpus1 and try tagging first:
require("openNLP")
tagPOS(corpus1)
```

such as corpus[[1000]] where Emotes Furet becomes emotes furet and this seems like words incorrectly. Solution?

[](Text Mining Infrastructure in R.pdf) 4.6:
		In computational linguistics a common task is tagging words with their part of speech for further analysis. Via an interface with the openNLP package to the OpenNLP tool kit tm integrates part of speech tagging functionality based on maximum entropy machine learned models. openNLP ships transformations wrapping OpenNLP's internal Java system calls for our convenience, e.g.,
		
	library("openNLP")
	tagPOS(acq[[10]])

shows the tagged words using a set of predefined tags identifying nouns, verbs, adjectives,
adverbs, et cetera depending on their context in the text. The tags are Penn Treebank
tags (Mitchell et al. 1993), so e.g., NNP stands for proper noun, singular, or e.g., VBD stands
for verb, past tense.

# Questions
- What are stop words? 
	words which are filtered out before or after processing of natural language data (text), often because too common (the).
	Any group of words can be chosen as the stop words for a given purpose. For some search engines, these are some of the most common, short function words, such as the, is, at, which, and on. In this case, stop words can cause problems when searching for phrases that include them, particularly in names such as 'The Who', 'The The', or 'Take That'. Other search engines remove some of the most common words—including lexical words, such as "want"—from a query in order to improve performance.

- What is meant by metadata:7 ?
All extension classes must provide accessors to extract subsets ([), individual documents ([[), and
metadata (meta).

A corpus can have two types of metadata (accessible via meta). Corpus metadata contains corpus
specific metadata in form of tag-value pairs. Document level metadata contains document specific
metadata but is stored in the corpus as a data frame. Document level metadata is typically used
for semantic reasons (e.g., classifications of documents form an own entity due to some high-level
information like the range of possible values) or for performance reasons (single access instead of
extracting metadata of each document).

http://dublincore.org/documents/dces/