DS Phase 2
====
# WHAT IS HAPPENING NOW

>I am running "process training set.6.ProcessFix3.TriGramsOnly.RunningonNewsNow.R" to create n.Tfreq4.RDS. Then I will have three corpus frequency tables approx 30 MB each.

*Next steps*
1. develop algorithm which can classify and test accuracy by going back in sentence and isolating/testing each known trigram.

1.  OK what's my last prediction file? Predictions6.Large.t.Tfreq6..2.functionalize.R
	Now working as *Predictions7.ClassificationTesting.R*

2. Write an algorithm which classifies text by testing all previous trigrams.

So it will load t.TFreq4.RDS first.


* UNSTEM the prediction finally.
		


----

# A is for ALGORITHM - Now I want to improve my model.

## Measures of accuracy

* cross-entropy

* Perplexity

In 1991, a trigram model was used on a large cor-
pus of one million English words to achieve a perplex-
ity score of 247 per word, corresponding to a cross en-
tropy of 7.95 bits per word or 1.75 bits per letter (Brown,
Della Pietra, Della Pietra, Lai, Mercer, 1992). On this
corpus, ASCII coding has a cross entropy of 8 bits per
character, Huffman coding has 4.46, and the UNIX com-
mand compress has 4.43. On more specialized cor-
pora it is possible to achieve lower perplexity scores than
for more general corpora. Recently, a word perplexity
score of 96.9 was reported on the Associated Press corpus
using a technique called stochastic memoization (Wood,
Archambeau, Gasthaus, James, and Teh, 2009). This is
significantly lower than the perplexity scores reported for
competing approaches.

## Classification

 each training category can be used as a separate
training set for the prediction/classification algorithm. In
the case of prediction, the prediction error for the data
is compared using each category as a training set  The
data can be classified as being in the category which re-
sults in the lowest prediction error.

*SO, what you would do is, run each trigram of the sentence which is KNOWN through the two sets to find the lowest error*

ok cool, that is actionable info.

Steps to build algorithm:

1. Take a test sentence
2. Break into trigrams
3. Run Prediction.
4. Get a measure of accuracy.
So: n.train4.RDS & t.Tfreq6.RDS ?
did i never build the freq table for news perhaps?
nope.




# B. DESIGN
keep the default screen extremely simple Google and Apple but have a button that turns on advanced visualizations where you can put some cool word cloud or something like inverse perfect list of the other options and the probability is even perhaps perhaps and maybe something that tells the identification whether this is from Twitter or news
================
# Initial loading pop-up of some sort

http://stackoverflow.com/questions/18237987/show-that-shiny-is-busy-or-loading-when-changing-tab-panels

# Model Analytics

* Perplexity

* performance vs. accuracy

* Create a test set you can run on different corpus to get automatic accuracies.

* Use 1, 2, and 3 n-grams?

* Markov Chains?

* smooth probabilities

* katz back-off model

* Can you estimate how uncertain you are about the words you are predicting? 

* What are the most commonly missed n-grams? Can you think of a reason why they would be missed and fix that? 

* Other Data Visualizations?
	
	* Word Cloud

	* Graphs of probability?

# *Elements for Presentation*

* Fix for commonly missed

* text cleanup, unstemming

* Different corpuses, classifying

* estimating uncertainty

* How should you document the use of your data product (separately from how you created it) so that others can rapidly deploy your algorithm?

# Improving accuracy


#Unstem:
"dog is my" produces "favorit"

#Shinysky:
----
* Busy indicator for initial loading

```r
# Server.R:
output$plot1 <- renderPlot({
	if (input$busyBtn == 0) 
		return()
	Sys.sleep(3)
	hist(rnorm(10^3))
})
# UI
busyIndicator("Calculation In progress",wait = 0)
	,actionButton("busyBtn","Show busyInidcator")
	,plotOutput("plot1")
```

* Typeahead for word selection?

https://github.com/AnalytixWare/ShinySky

#Examples:
----
https://www.google.com/search?client=opera&q=site%3A+shinyapps.io+prediction&sourceid=opera&ie=UTF-8&oe=UTF-8

# TO DO
----
0. 

* Loading message?

# display something else for NA?


# Classify the text 
to use a particular corpus based on how highly the words score in the unigram frequency database of that corpus (stop words removed)

# X Get the basics of the Shiny App up and running:
	1. A Shiny app that takes as input a phrase (multiple words) in a text box input and outputs a prediction of the next word (SINGLE).

NOTE: top score requires only correct prediction in 20% of cases, 1/5! (Drawn from Twitter and news)

# Ways to make it more awesome:
	
* have text prediction appear inline, letter-by-letter even?

* have a word cloud or top 5 list appear separately?


----
#Shiny issues

setwd("C:/Users/Michael/SkyDrive/Code/GitHub/DSCapstone/Predictor")
shinyapps::configureApp("Predictor", size="xxlarge") 


You can access the error log with ShinyApps::showLogs()

By default, Shiny limits file uploads to 5MB per file. You can modify this limit by using the shiny.maxRequestSize option. For example, adding options(shiny.maxRequestSize=30*1024^2) to the top of server.R would increase the limit to 30MB.

http://shiny.rstudio.com/articles/shinyapps.html