Post-Tokenization
====
# large corpus finding nothing...

```r
findFreqTerms(x = tdm, lowfreq=1758, highfreq=Inf)
 [1] "also"    "back"    "call"    "can"     "citi"    "come"   
 [7] "compani" "counti"  "day"     "even"    "first"   "game"   
[13] "get"     "good"    "help"    "high"    "home"    "includ" 
[19] "just"    "know"    "last"    "like"    "look"    "made"   
[25] "make"    "mani"    "may"     "million" "month"   "much"   
[31] "need"    "new"     "now"     "one"     "open"    "peopl"  
[37] "percent" "plan"    "play"    "point"   "polic"   "public" 
[43] "report"  "right"   "run"     "said"    "say"     "school" 
[49] "season"  "see"     "show"    "sinc"    "start"   "state"  
[55] "still"   "take"    "team"    "think"   "three"   "time"   
[61] "tri"     "two"     "use"     "want"    "way"     "week"   
[67] "well"    "will"    "work"    "year"

findAssocs(tdm,"first",corlimit=0.5)
# how can this be 0... too many words?
findAssocs(tdm,"first",corlimit=0.95)
# nope
findAssocs(tdm,"first",corlimit=0.01)
# a ha, ok it's just that i have 77,000 documents and nothing shows up more than maybe 1758 times...
findAssocs(tdm,"first",corlimit=0.02)

#let's try this with "case"
findAssocs(tdm,"case",corlimit=0.03)
# everything is legal related. looks like bigrams will be needed.
```

# bigram
RWeka package
bigram tokenizer...

```r
#Tokenizer for n-grams and passed on to the term-document matrix constructor
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
txtTdmBi <- TermDocumentMatrix(crude, control = list(tokenize = BigramTokenizer))
```

# SPEED ISSUES

```r
findAssocsBig <- function(u, term, corlimit){
  suppressWarnings(x.cor <-  gamlr::corr(t(u[ !u$dimnames$Terms == term, ]),        
                                         as.matrix(t(u[  u$dimnames$Terms == term, ]))  ))  
  x <- sort(round(x.cor[(x.cor[, term] > corlimit), ], 2), decreasing = TRUE)
  return(x)
}
```

# Idea
Load the test sentence as a TDM.
Use findAssoc to get words?

```r
#Set working directory
setwd("C:/Users/Michael/SkyDrive/Code/GitHub/DSCapstone/Coursera-SwiftKey/final/en_US")

fileName="testData.txt"
testData <- readLines(fileName)

## REMOVAL OF STRANGE CHARACTERS##
# Replace unicode characters with spaces.
cleanData<-iconv(testData, to='ASCII', sub=' ')
# Replace numbers and ''
cleanData2 <- gsub("'{2}", " ", cleanData)
cleanData3 <- gsub("[0-9]", " ", cleanData2)

## MAKE CORPUS ##
require(tm)
corpus <- Corpus(VectorSource(cleanData3))

## TOKENIZATION ## 

# REMOVE WHITESPACE:
corpus1 <- tm_map(corpus, stripWhitespace)
inspect(corpus1) #don't see a big difference

# LOWERCASE:
corpus2 <- tm_map(corpus1, content_transformer(tolower))
inspect(corpus2) #works

# REMOVE STOPWORDS
corpus3 <- tm_map(corpus2, removeWords, stopwords("english"))
inspect(corpus3) # ok the has been removed...

# STEMMING
corpus4 <- tm_map(corpus3, stemDocument)
inspect(corpus4) # Looks stemmed.

# REMOVE PUNCTUATION
corpus5<- tm_map(corpus4,removePunctuation)

# REMOVE NUMBERS
corpus6<- tm_map(corpus5,removeNumbers)

## END TOKENIZATION ##

## TOKEN ANALYSIS ##

# MAKE TERM DOCUMENT MATRIX (TDM) - a matrix of frequency counts for each word used in the corpus.
tdm<- TermDocumentMatrix(corpus6)
dtm<- DocumentTermMatrix(corpus6)
dtm

```

# Implement "Bag of Words" model

I need to weight the words.


[](http://anythingbutrbitrary.blogspot.com/2013/03/build-search-engine-in-20-minutes-or.html)

# SOME COMMANDS:

```r
findFreqTerms(x = tdm, lowfreq=58, highfreq=Inf)
findAssocs(x=tdm,term="also",corlimit=0.1)

# 1. take a sentence and iterate through the words

# 2. Look up associated words 

# 3. Give a score to each association with higher score to the words near the blank. 

## POSSIBILITIES:

# 1. REMOVE SPARSE TERMS
TDM.common = removeSparseTerms(TDM, 0.1)
dim(TDM)
# [1] 18651   182
dim(TDM.common)
# [1]  71 182
```

# Quiz 2
## Question 1
The guy in front of me just bought a pound of bacon, a bouquet, and a case of
soda
pretzels
cheese
beer

# Vocab
*orthogonal* - right-angle but in statistics means not related.
*dot product* - an algebraic operation that takes two equal-length sequences of numbers (usually coordinate vectors) and returns a single number.
*Euclidean space* - encompasses the two-dimensional Euclidean plane, the three-dimensional space of Euclidean geometry, and certain other spaces
