Post-Tokenization
====
# NEXT STEPS

1. Probably should work on fine-tuning the model before loading so much data? Not sure of this...

	-`proof of concept` - yes build a small corpus where you are trying to get a certain prediction to work first... How would this go?

```txt
"Talking heads are my favorite band"
"I hate the talking heads on tv like Wolf Blitzer"
"My favorite band is Aerosmith"
"Wolf Blitzer is not my favorite"
```

OK so that corpus should be able to predict that:
"Talking heads are my favorite" gives "band" ?

>You are looking for a (statistical) language model.

A statistical language model assigns a probability to a sequence of m words P(w_1,...,w_m) by means of a probability distribution...

So perhaps do it with only trigrams, 

So what i need to do is:

```r
# take an input:
input<-"my favorite band is talking"
# 1. reduce to last two words.
input2<-unlist(strsplit(input," "))
two<-length(input2)
one<-two-1
bigram<-paste(input2[one]," ",input2[two])
# 2. now somehow find bigrams within trigrams... 
```

## MUST UNDERSTAND:
1. Cross-validation / sampling / bootstrapping

2. *Markov Model / Markov Chain*
	

3. Smoothing by assigning non-zero probabilities?

4. multinomial or categorial distribution

5. (n-1) gram

6. Katz's Back-off model

7. Good-Turing frequency estimation

-Can I find all 2- and 3-grams which include a word? not just search for a word...

## Speed Issues
http://www.r-bloggers.com/faster-higher-stonger-a-guide-to-speeding-up-r-code-for-busy-people/

-Run gc() throughout?

-`tm_combine` Combine Corpora, Documents, Term-Document Matrices, and Term
Frequency Vectors

So, I could load portions of the files then combine.

```r
c(myTDM,myTDM2)
```

## QUIZ 2:

>Right now, I am using grep() to search all matched tokens and print out corresponding frequency in decreasing order. This is good enough to finish quizz ( I think). But the main disadvantage is the low speed. It takes about one minute to search.

# MODEL:
## Assigning weights/points:

See tm package documentation.pdf:
`tm_term_score` - compute a score based on the number of matching terms.
`weightSMART` SMART Weightings

# Modeling problem.
[Main forum](https://class.coursera.org/dsscapstone-002/forum/list?forum_id=10010)

## [Stanford NLP Lectures](https://class.coursera.org/nlp/lecture)

What I'm learning:
my original corpus might be okay, but I need a way to call out by a number each word in a sentence.
Then, add up all the probabilities of unigrams, 
but also check the n-1

### -Maybe I should examine the code of tm package or rtexttools
[](https://code.google.com/p/rtexttools/source/browse/RTextTools/R/create_matrix.R?r=02bd8e07c0df54e44aff83a05dff1cc74d82c4de)
Looks like it's just a wrapper for tm anyway!

[](http://stackoverflow.com/questions/8898521/finding-2-3-word-phrases-using-r-tm-package?lq=1)

```r
library(tm); library(tau);

tokenize_ngrams <- function(x, n=3) return(rownames(as.data.frame(unclass(textcnt(x,method="string",n=n)))))

texts <- c("This is the first document.", "This is the second file.", "This is the third text.")
corpus <- Corpus(VectorSource(texts))
matrix <- DocumentTermMatrix(corpus,control=list(tokenize=tokenize_ngrams))
```

### Intro to N-grams

LM = language modeling

What we are doing is assigning probabilities to different sentences, words.

#### Conditional Probability definition
P(A|B) = P(A,B)/P(B)

#### Chain rule of conditional probability
P(x1,x2,x3,xn) = P(x1)P(x2|x1)P(x3|x1,x2)...P(xn|x1,...,xn-1)

#### Markov Assumption
P("the" | "its water is so transparent that") â‰ˆ P(the|that) or maybe (the|"transparent that")

#### Unigram model
Since it simply assigns probability to each word, the nonsense sentence "the the the the" would be highest probability.

#### N-grams 

language has long-distance dependencies, the verb can come very far after the subject which really is an important predictor "the computer which I had just placed on the fifth floor *crashed*"

Yet, 2-5-gram models will do pretty well anyway.

### Bigram model

P(wi|wi-1) = count(wi-1,wi)/count(wi-1)

#### Types of knowledge that cause bigram probabilities
World knowledge (chinese food more popular then english food)
Grammar knowledge ("want" often followed by infinitive like "to")

Structural - something really is that probability
Contingent - result of the particular corpus sample

#### Practical Issues

adding log probabilities is faster than multiplying regular probabilities.

Toolkits:
SRILM
Google N-gram release

### Evaluation and Perplexity

training set, test set, evaluation metric
A task is the best evaluation for comparing multiple models:
	-spelling corrector, speech recognizer, MT system
Accuracy measure	
	-how many misspelled words corrected properly
	-how many words translated correctly

Above is extrinsic or in-vivo evaluation. Problem is, time-consuming slow.

#### Intrinsic evaluation - perplexity

unless test data very similar to training data, not very useful, still need to do extrinsic.

#### The Shannon Game - how well can we predict the next word.



## Most helpful thread: [](https://class.coursera.org/dsscapstone-002/forum/thread?thread_id=93#post-461)

-Right now, I am using grep() to search all matched tokens and print out corresponding frequency in decreasing order.

-The Stanford NLP video lectures were very helpful on tokenisation and creating ngrams. Think I'll take a look at the modelling ones.

-I just went through the Language Modeling lectures from the Natural Language Processing course by Dan Jurafsky, Christopher Manning. They are VERY helpful. I suggest watching the entire week's worth.  

## This? [Build a search engine in 20 minutes or less](http://anythingbutrbitrary.blogspot.com/2013/03/build-search-engine-in-20-minutes-or.html)
## Trying rtexttools instead of tm to make TDM
```r
setwd("C:/Users/Michael/SkyDrive/Code/GitHub/DSCapstone/Coursera-SwiftKey/final/en_US")

fileName="en_US.twitter.txt"
lineNews <- readLines(fileName, n=100000)

### NOTE: I SHOULD REMOVE PROFANITY AS WELL SOMEWHERE ###

## REMOVAL OF STRANGE CHARACTERS##
# Replace unicode characters with spaces.
cleanData<-iconv(lineNews, to='ASCII', sub=' ')
# Replace numbers and ''
cleanData2 <- gsub("'{2}", " ", cleanData)
cleanData3 <- gsub("[0-9]", " ", cleanData2)

require(RTextTools)
myTDM <- create_matrix(cleanData3, language="english", 
                    removeNumbers=TRUE, stemWords=TRUE, ngramLength=c(1,2,3),  
                    removePunctuation=TRUE, removeSparseTerms=0.60, 
                    removeStopwords=TRUE, stripWhitespace=TRUE, toLower=TRUE)
#creates a matrix where terms are columns, documents are rows.
colnames(myTDM) #lists all terms
# ok above results in only 32 words!
myTDM <- create_matrix(cleanData3, language="english", 
                    removeNumbers=TRUE, stemWords=TRUE, ngramLength=c(1,2,3),  
                    removePunctuation=TRUE, removeStopwords=TRUE, stripWhitespace=TRUE, toLower=TRUE)

# Could problem be that I did not load tm or tau?
# okay, that worked and produced 48399 columns

## VIEW ##
options(max.print=100000)
sink("myTDM.txt")
findFreqTerms(x = myTDM, lowfreq=1, highfreq=Inf)
sink()

# so it seems i have only unigrams, and that the reason may be that words have been mashed together for some reason. I will try again without all the other options.

myTDM <- create_matrix(cleanData3, language="english", 
                    removeNumbers=FALSE, stemWords=FALSE, ngramLength=c(2),  
                    removePunctuation=FALSE, removeStopwords=FALSE, stripWhitespace=FALSE, toLower=TRUE)
# Error in FUN(X[[2L]], ...) : non-character argument

debug(create_matrix)
undebug(create_matrix)
#OR debugonce(create_matrix)

capture.output(create_matrix(cleanData3, language="english", 
                    removeNumbers=FALSE, stemWords=FALSE, ngramLength=c(2),  
                    removePunctuation=FALSE, removeStopwords=FALSE, stripWhitespace=FALSE, toLower=TRUE), file='source_create_matrix.r')
```

```r
tokenize_ngrams <- function(x, n=3) return(rownames(as.data.frame(unclass(textcnt(x,method="string",n=n)))))

texts <- c("This is the first document.", "This is the second file.", "This is the third text.")
corpus <- Corpus(VectorSource(texts))
matrix <- DocumentTermMatrix(corpus,control=list(tokenize=tokenize_ngrams))
```
This creates the same error, therefore, the problem is within this, since create_matrix calls this.

```r
tokenize_ngrams("This is the first document.")
# [1] "is the first"       "the first document" "this is the"
```
so that works... 
 so the error is in the control= or tokenize= thing.
SO, the easiest way for me to work this i guess would be to simply tokenize ahead of time? 
I could create separate matrix for bigrams. But how to keep documents together.

# LET"S READ THE DOCS FOR tm CONTROL=
```r
matrix <- TermDocumentMatrix(corpus)
```
So the problem seems to be that a custom function for tokenize... well i guess it is allowed but perhaps this one doesn't do it right.

```r
library(rJava)
library(RWeka)
library(tm)

bigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
tdm <- TermDocumentMatrix(corpus, control = list(tokenize = bigramTokenizer))
```
Woot! that worked...

```r
library(RTextTools)
TDM2 <- create_matrix(corpus, language="english", 
                    removeNumbers=TRUE, stemWords=TRUE, ngramLength=c(1,2,3),  
                    removePunctuation=TRUE, removeStopwords=TRUE, stripWhitespace=TRUE, toLower=TRUE)
#Warning message:
#In if (ngramLength > 1) { :
  #the condition has length > 1 and only the first element will be used
```
So, that way does not work.

```r
library(rJava)
library(RWeka)
library(tm)

TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 3))
tdm <- TermDocumentMatrix(corpus, control = list(tokenize = TrigramTokenizer))
```
#*Sweet! That seems to do it.*#

#--#
'added today' %in% Terms(myTDM)
FALSE
findAssocs(myTDM,'know',0)

options(max.print=100000)
sink("myTDM.txt")		
inspect(myTDM)
sink()

```


# "case of" is not in the dataset en_US.news.txt

```r
'case of' %in% Terms(txtTdmBi)
FALSE
'added today' %in% Terms(txtTdmBi)
TRUE
findAssocs(txtTdmBi2, "added today", 0)

rowSums(as.matrix(txtTdmBi))
```

#OK let's see the top bigrams;

```r
findFreqTerms(x = txtTdmBi2, lowfreq=158, highfreq=Inf)
findAssocs(txtTdmBi2, "barack obama", 0.1)
```

# SPEED ISSUES

```r
library(gamlr)
findAssocsBig <- function(u, term, corlimit){
  suppressWarnings(x.cor <-  gamlr:corr(t(u[ !u$dimnames$Terms == term, ]),        
                                         as.matrix(t(u[  u$dimnames$Terms == term, ]))  ))  
  x <- sort(round(x.cor[(x.cor[, term] > corlimit), ], 2), decreasing = TRUE)
  return(x)
}
findAssocsBig(txtTdmBi, "added today", 0.01)
#Error in get(name, envir = asNamespace(pkg), inherits = FALSE) : 
#  object 'corr' not found
```
# large corpus finding nothing...

```r
findFreqTerms(x = tdm, lowfreq=1758, highfreq=Inf)
 [1] "also"    "back"    "call"    "can"     "citi"    "come"   
 [7] "compani" "counti"  "day"     "even"    "first"   "game"   
[13] "get"     "good"    "help"    "high"    "home"    "includ" 
[19] "just"    "know"    "last"    "like"    "look"    "made"   
[25] "make"    "mani"    "may"     "million" "month"   "much"   
[31] "need"    "new"     "now"     "one"     "open"    "peopl"  
[37] "percent" "plan"    "play"    "point"   "polic"   "public" 
[43] "report"  "right"   "run"     "said"    "say"     "school" 
[49] "season"  "see"     "show"    "sinc"    "start"   "state"  
[55] "still"   "take"    "team"    "think"   "three"   "time"   
[61] "tri"     "two"     "use"     "want"    "way"     "week"   
[67] "well"    "will"    "work"    "year"

findAssocs(tdm,"first",corlimit=0.5)
# how can this be 0... too many words?
findAssocs(tdm,"first",corlimit=0.95)
# nope
findAssocs(tdm,"first",corlimit=0.01)
# a ha, ok it's just that i have 77,000 documents and nothing shows up more than maybe 1758 times...
findAssocs(tdm,"first",corlimit=0.02)

#let's try this with "case"
findAssocs(tdm,"case",corlimit=0.03)
# everything is legal related. looks like bigrams will be needed.

```

# bigram
RWeka package
bigram tokenizer...

```r
#Tokenizer for n-grams and passed on to the term-document matrix constructor
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
txtTdmBi <- TermDocumentMatrix(crude, control = list(tokenize = BigramTokenizer))
```

# Idea
Load the test sentence as a TDM.
Use findAssoc to get words?

```r
#Set working directory
setwd("C:/Users/Michael/SkyDrive/Code/GitHub/DSCapstone/Coursera-SwiftKey/final/en_US")

fileName="testData.txt"
testData <- readLines(fileName)

## REMOVAL OF STRANGE CHARACTERS##
# Replace unicode characters with spaces.
cleanData<-iconv(testData, to='ASCII', sub=' ')
# Replace numbers and ''
cleanData2 <- gsub("'{2}", " ", cleanData)
cleanData3 <- gsub("[0-9]", " ", cleanData2)

## MAKE CORPUS ##
require(tm)
corpus <- Corpus(VectorSource(cleanData3))

## TOKENIZATION ## 

# REMOVE WHITESPACE:
corpus1 <- tm_map(corpus, stripWhitespace)
inspect(corpus1) #don't see a big difference

# LOWERCASE:
corpus2 <- tm_map(corpus1, content_transformer(tolower))
inspect(corpus2) #works

# REMOVE STOPWORDS
corpus3 <- tm_map(corpus2, removeWords, stopwords("english"))
inspect(corpus3) # ok the has been removed...

# STEMMING
corpus4 <- tm_map(corpus3, stemDocument)
inspect(corpus4) # Looks stemmed.

# REMOVE PUNCTUATION
corpus5<- tm_map(corpus4,removePunctuation)

# REMOVE NUMBERS
corpus6<- tm_map(corpus5,removeNumbers)

## END TOKENIZATION ##

## TOKEN ANALYSIS ##

# MAKE TERM DOCUMENT MATRIX (TDM) - a matrix of frequency counts for each word used in the corpus.
tdm<- TermDocumentMatrix(corpus6)
dtm<- DocumentTermMatrix(corpus6)
dtm

```

# Implement "Bag of Words" model

I need to weight the words.


[](http://anythingbutrbitrary.blogspot.com/2013/03/build-search-engine-in-20-minutes-or.html)

# SOME COMMANDS:

```r
findFreqTerms(x = tdm, lowfreq=58, highfreq=Inf)
findAssocs(x=tdm,term="also",corlimit=0.1)

# 1. take a sentence and iterate through the words

# 2. Look up associated words 

# 3. Give a score to each association with higher score to the words near the blank. 

## POSSIBILITIES:

# 1. REMOVE SPARSE TERMS
TDM.common = removeSparseTerms(TDM, 0.1)
dim(TDM)
# [1] 18651   182
dim(TDM.common)
# [1]  71 182
```

# Quiz 2
## Question 1
The guy in front of me just bought a pound of bacon, a bouquet, and a case of
soda
pretzels
cheese
beer

# Vocab
*orthogonal* - right-angle but in statistics means not related.
*dot product* - an algebraic operation that takes two equal-length sequences of numbers (usually coordinate vectors) and returns a single number.
*Euclidean space* - encompasses the two-dimensional Euclidean plane, the three-dimensional space of Euclidean geometry, and certain other spaces
