Post-Tokenization
====

# Modeling problem.
[Main forum](https://class.coursera.org/dsscapstone-002/forum/list?forum_id=10010)

## [Stanford NLP Lectures](https://class.coursera.org/nlp/lecture)

### Conditional Probability definition
P(A|B) = P(A,B)/P(B)

### Chain rule of conditional probability
P(x1,x2,x3,xn) = P(x1)P(x2|x1)P(x3|x1,x2)...P(xn|x1,...,xn-1)



LM = language modeling


## Most helpful thread: [](https://class.coursera.org/dsscapstone-002/forum/thread?thread_id=93#post-461)
-Right now, I am using grep() to search all matched tokens and print out corresponding frequency in decreasing order.
-The Stanford NLP video lectures were very helpful on tokenisation and creating ngrams. Think I'll take a look at the modelling ones.
-I just went through the Language Modeling lectures from the Natural Language Processing course by Dan Jurafsky, Christopher Manning. They are VERY helpful. I suggest watching the entire week's worth.  

## This? [Build a search engine in 20 minutes or less](http://anythingbutrbitrary.blogspot.com/2013/03/build-search-engine-in-20-minutes-or.html)
## Trying rtexttools instead of tm to make TDM
```r
setwd("C:/Users/Michael/SkyDrive/Code/GitHub/DSCapstone/Coursera-SwiftKey/final/en_US")

fileName="en_US.twitter.txt"
lineNews <- readLines(fileName, n=100000)

### NOTE: I SHOULD REMOVE PROFANITY AS WELL SOMEWHERE ###

## REMOVAL OF STRANGE CHARACTERS##
# Replace unicode characters with spaces.
cleanData<-iconv(lineNews, to='ASCII', sub=' ')
# Replace numbers and ''
cleanData2 <- gsub("'{2}", " ", cleanData)
cleanData3 <- gsub("[0-9]", " ", cleanData2)

require(RTextTools)
myTDM <- create_matrix(cleanData3, language="english", 
                    removeNumbers=TRUE, stemWords=TRUE, ngramLength=c(1,2,3),  
                    removePunctuation=TRUE, removeSparseTerms=0.60, 
                    removeStopwords=TRUE, stripWhitespace=TRUE, toLower=TRUE)
#creates a matrix where terms are columns, documents are rows.
colnames(myTDM) #lists all terms
# ok above results in only 32 words!
myTDM <- create_matrix(cleanData3, language="english", 
                    removeNumbers=TRUE, stemWords=TRUE, ngramLength=c(1,2,3),  
                    removePunctuation=TRUE, removeStopwords=TRUE, stripWhitespace=TRUE, toLower=TRUE)


#--#
'added today' %in% Terms(myTDM)
FALSE
findAssocs(myTDM,'know',0)

options(max.print=100000)
sink("myTDM.txt")
inspect(myTDM)
sink()

```


# "case of" is not in the dataset en_US.news.txt

```r
'case of' %in% Terms(txtTdmBi)
FALSE
'added today' %in% Terms(txtTdmBi)
TRUE
findAssocs(txtTdmBi2, "added today", 0)

rowSums(as.matrix(txtTdmBi))
```

#OK let's see the top bigrams;

```r
findFreqTerms(x = txtTdmBi2, lowfreq=158, highfreq=Inf)
findAssocs(txtTdmBi2, "barack obama", 0.1)
```

# SPEED ISSUES

```r
require(gamlr)
findAssocsBig <- function(u, term, corlimit){
  suppressWarnings(x.cor <-  gamlr:corr(t(u[ !u$dimnames$Terms == term, ]),        
                                         as.matrix(t(u[  u$dimnames$Terms == term, ]))  ))  
  x <- sort(round(x.cor[(x.cor[, term] > corlimit), ], 2), decreasing = TRUE)
  return(x)
}
findAssocsBig(txtTdmBi, "added today", 0.01)
#Error in get(name, envir = asNamespace(pkg), inherits = FALSE) : 
#  object 'corr' not found
```
# large corpus finding nothing...

```r
findFreqTerms(x = tdm, lowfreq=1758, highfreq=Inf)
 [1] "also"    "back"    "call"    "can"     "citi"    "come"   
 [7] "compani" "counti"  "day"     "even"    "first"   "game"   
[13] "get"     "good"    "help"    "high"    "home"    "includ" 
[19] "just"    "know"    "last"    "like"    "look"    "made"   
[25] "make"    "mani"    "may"     "million" "month"   "much"   
[31] "need"    "new"     "now"     "one"     "open"    "peopl"  
[37] "percent" "plan"    "play"    "point"   "polic"   "public" 
[43] "report"  "right"   "run"     "said"    "say"     "school" 
[49] "season"  "see"     "show"    "sinc"    "start"   "state"  
[55] "still"   "take"    "team"    "think"   "three"   "time"   
[61] "tri"     "two"     "use"     "want"    "way"     "week"   
[67] "well"    "will"    "work"    "year"

findAssocs(tdm,"first",corlimit=0.5)
# how can this be 0... too many words?
findAssocs(tdm,"first",corlimit=0.95)
# nope
findAssocs(tdm,"first",corlimit=0.01)
# a ha, ok it's just that i have 77,000 documents and nothing shows up more than maybe 1758 times...
findAssocs(tdm,"first",corlimit=0.02)

#let's try this with "case"
findAssocs(tdm,"case",corlimit=0.03)
# everything is legal related. looks like bigrams will be needed.

```

# bigram
RWeka package
bigram tokenizer...

```r
#Tokenizer for n-grams and passed on to the term-document matrix constructor
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
txtTdmBi <- TermDocumentMatrix(crude, control = list(tokenize = BigramTokenizer))
```

# Idea
Load the test sentence as a TDM.
Use findAssoc to get words?

```r
#Set working directory
setwd("C:/Users/Michael/SkyDrive/Code/GitHub/DSCapstone/Coursera-SwiftKey/final/en_US")

fileName="testData.txt"
testData <- readLines(fileName)

## REMOVAL OF STRANGE CHARACTERS##
# Replace unicode characters with spaces.
cleanData<-iconv(testData, to='ASCII', sub=' ')
# Replace numbers and ''
cleanData2 <- gsub("'{2}", " ", cleanData)
cleanData3 <- gsub("[0-9]", " ", cleanData2)

## MAKE CORPUS ##
require(tm)
corpus <- Corpus(VectorSource(cleanData3))

## TOKENIZATION ## 

# REMOVE WHITESPACE:
corpus1 <- tm_map(corpus, stripWhitespace)
inspect(corpus1) #don't see a big difference

# LOWERCASE:
corpus2 <- tm_map(corpus1, content_transformer(tolower))
inspect(corpus2) #works

# REMOVE STOPWORDS
corpus3 <- tm_map(corpus2, removeWords, stopwords("english"))
inspect(corpus3) # ok the has been removed...

# STEMMING
corpus4 <- tm_map(corpus3, stemDocument)
inspect(corpus4) # Looks stemmed.

# REMOVE PUNCTUATION
corpus5<- tm_map(corpus4,removePunctuation)

# REMOVE NUMBERS
corpus6<- tm_map(corpus5,removeNumbers)

## END TOKENIZATION ##

## TOKEN ANALYSIS ##

# MAKE TERM DOCUMENT MATRIX (TDM) - a matrix of frequency counts for each word used in the corpus.
tdm<- TermDocumentMatrix(corpus6)
dtm<- DocumentTermMatrix(corpus6)
dtm

```

# Implement "Bag of Words" model

I need to weight the words.


[](http://anythingbutrbitrary.blogspot.com/2013/03/build-search-engine-in-20-minutes-or.html)

# SOME COMMANDS:

```r
findFreqTerms(x = tdm, lowfreq=58, highfreq=Inf)
findAssocs(x=tdm,term="also",corlimit=0.1)

# 1. take a sentence and iterate through the words

# 2. Look up associated words 

# 3. Give a score to each association with higher score to the words near the blank. 

## POSSIBILITIES:

# 1. REMOVE SPARSE TERMS
TDM.common = removeSparseTerms(TDM, 0.1)
dim(TDM)
# [1] 18651   182
dim(TDM.common)
# [1]  71 182
```

# Quiz 2
## Question 1
The guy in front of me just bought a pound of bacon, a bouquet, and a case of
soda
pretzels
cheese
beer

# Vocab
*orthogonal* - right-angle but in statistics means not related.
*dot product* - an algebraic operation that takes two equal-length sequences of numbers (usually coordinate vectors) and returns a single number.
*Euclidean space* - encompasses the two-dimensional Euclidean plane, the three-dimensional space of Euclidean geometry, and certain other spaces
