---
title: "Milestone Report"
subtitle: "Data Science Capstone- Johns Hopkins / Coursera"
author: "Michael Liquori"
date: "Saturday, November 15, 2014"
output: html_document
---
<hr>

### Background

The aim of this project is to develop a model that will allow us to predict the next word a user intends to type to aid in faster typing, similar to the feature currently embedded in most smartphones. We will seek to balance speed and accuracy in developing an algorithm which is competitive with the current word-prediction keyboards available, such as "SwiftKey".

The data provided for NLP (Natural Language Processing) consists of 3 "corpora" of data, one collected from <font color="red" face="bold">**blog posts**</font>, one from <font color="orange" face="bold">**news articles**</font>, and a third from <font color="skyblue" face="bold">**twitter messages**</font> ("tweets"). 

### Exploratory Analysis

We begin by analyzing some basic facts about each corpus such as number of lines, number of words, number of unique words, line length, and average unique word length.

#### Number of Lines, Words, and Words per Line

```{r, echo=FALSE,warning=FALSE,message=FALSE}

# SETUP #
setwd("C:/Users/Michael/SkyDrive/Code/GitHub/DSCapstone/Coursera-SwiftKey/final/en_US")
library(tm)
load("C:/Users/Michael/SkyDrive/Code/GitHub/DSCapstone/Coursera-SwiftKey/final/en_US/RanMdownScriptFullProcessing.RData")

# Line counts

tlCount=2360148
nlCount=1010242
blCount=899288

# Number of Total Words

twCount=30373583
nwCount=34372530
bwCount=37334131

# Number of words per line
tnwl=twCount/tlCount
nnwl=nwCount/nlCount
bnwl=bwCount/blCount

# Get maximum line length

tlmount=173
nlmount=11384
blmount=40833

# Get unique words unstemmed

tuw<-1192198
nuw<-750174
buw<-952939

# Number of Unique Words STEMMED

tuwCount=454293
buwCount=355364
nuwCount=79427

# Plot

# 3 corpora in one plot barplots:

## Shared parameters

# save par defaults first
op<-par()

names=c("Twitter Corpus", "News Corpus", "Blogs Corpus")
colors=c("cadetblue3","chocolate2","firebrick1")
par(mai=c(1,2,1,1))

## Number of Lines

counts=c(tlCount,blCount,nlCount)
bp<-barplot(counts, col=colors, main="Number of Lines", horiz=TRUE, names.arg=names, las=1, axes=FALSE)
axis(1, at = c(0,500000,1000000,1500000,2000000), labels=c("0","500,000","1,000,000","1,500,000","2,000,000"))
text(0,bp,prettyNum(counts,big.mark=",",scientific=F),cex=1.3,pos=4) 
```

We see that the <font color="skyblue" face="bold">**twitter corpus**</font> has many more lines than the others. But how many words are in each line?

```{r, echo=FALSE,warning=FALSE,message=FALSE}

## Average Number of Words per Line
par(mai=c(1,2,1,1))
counts=c(tnwl,bnwl,nnwl)
bp<-barplot(counts, col=colors, main="Average Number of Words per Line", horiz=TRUE, names.arg=names, las=1, axes=FALSE, xlim=c(0,50))
axis(1, at = seq(0,50,by=10))
text(0,bp,prettyNum(counts,big.mark=",",scientific=F),cex=1.3,pos=4) 
```

We see that lines are much shorter in the <font color="skyblue" face="bold">**twitter corpus**</font> and longest in the <font color="orange" face="bold">**news corpus**</font>. 

But, what exactly do lines represent? In the <font color="skyblue" face="bold">**twitter corpus**</font> they seem to represent one tweet, which has a clearly defined limit of 140 characters. In the <font color="red" face="bold">**blogs**</font> and <font color="orange" face="bold">**news corpus**</font>, they are less defined, sometimes representing just one sentence, sometimes an entire paragraph. 

#### Diversity of Vocabulary

Now let's examine the diversity of unique words used in each corpus. There are two different measures we will use: stemmed and unstemmed. Stemming reduces words to their root, so, for example, "walking, walked, walker" would all become simply "walk" and be counted as one unique word. Unstemmed, they would count as 3 unique words.

```{r, echo=FALSE,warning=FALSE,message=FALSE}
## Number of Unique Words Stemmed
par(mai=c(1,2,1,1))
counts=c(tuwCount,buwCount,nuwCount)
bp<-barplot(counts, col=colors, main="Number of Stemmed Unique Words", horiz=TRUE, names.arg=names, las=1, axes=FALSE, xlim=c(0,500000))
axis(1, at = c(0,100000,200000,300000,400000,500000), labels=c(0,"100,000","200,000","300,000","400,000","500,000"))
text(0,bp,prettyNum(counts,big.mark=",",scientific=F),cex=1.3,pos=4) 
## Number of Unique Words Unstemmed
counts=c(tuw,buw,nuw)
bp<-barplot(counts, col=colors, main="Number of Unstemmed Unique Words", horiz=TRUE, names.arg=names, las=1, axes=FALSE, xlim=c(0,1500000))
axis(1, at = seq(0,1500000,by=250000), labels=prettyNum(seq(0,1500000,by=250000),big.mark=",",scientific=F))
text(0,bp,prettyNum(counts,big.mark=",",scientific=F),cex=1.3,pos=4) 

```

The diversity of words in <font color="red" face="bold">**blogs**</font> seems extremely low from our stemmed analysis, but the unstemmed number brings it closer in line with the other corpora. This suggests that used in <font color="red" face="bold">**blogs**</font> are very low, while they are highest in <font color="skyblue" face="bold">**twitter**</font>. What about the length of the average (unstemmed) word used?

```{r, echo=FALSE,warning=FALSE,message=FALSE}
## Average Unique Word Length
par(mai=c(1,2,1,1))
tAvg=4.479
bAvg=4.563
nAvg=4.943
counts=c(tAvg,bAvg,nAvg)
bp<-barplot(counts, col=colors, main="Average Unique Word Length", horiz=TRUE, names.arg=names, las=1, axes=FALSE, xlim=c(0,10))
axis(1, at = c(0,2.5,5,7.5,10), labels=c(0,2.5,5,7.5,10))
text(0,bp,prettyNum(counts,big.mark=",",scientific=F),cex=1.3,pos=4) 
```

As one would probably expect, the <font color="skyblue" face="bold">**twitter corpus**</font> contains the shortest words, due do its character limit and the nature of short-form communication it is intended to be used for, while <font color="red" face="bold">**blogs**</font> have the longest, due probably to a lack of any editor or other constraints on the writer. But, the differences are not all that great.

Now let's look at the distribution of word frequencies, ignoring the higher word counts where there are very few examples:

```{r, echo=FALSE,warning=FALSE,message=FALSE}
# Histogram for each corpus barplots:

## Word length Frequency Histograms for each Corpus
setwd("C:/Users/Michael/SkyDrive/Code/GitHub/DSCapstone/Coursera-SwiftKey/final/en_US")
twLengths=readRDS("twLengths")
bwLengths=readRDS("bwLengths")
nwLengths=readRDS("nwLengths")
par(mfcol=c(3,1))
hist(bwLengths, col ="firebrick1", breaks=seq(0,max(bwLengths),by=1), main="Blogs Corpus Word-Lengths", xlab = "Values", cex.lab = 1.3,xlim=c(0,25))

hist(nwLengths, col ="chocolate2", breaks=seq(0,max(nwLengths),by=1), main="News Corpus Word-Lengths", xlab = "Values", cex.lab = 1.3,xlim=c(0,25))

hist(twLengths, col ="cadetblue3", breaks=seq(0,max(twLengths),by=1), main="Twitter Corpus Word-Lengths", xlab = "Values", cex.lab = 1.3, xlim=c(0,25))
```

We see that the distribution of word lengths are very similar. 

Now, an example of the 10 most common words in each corpora. This is after processing both stemming and eliminating very common words such as "the", which are called "stop words" in language processing.
```{r, echo=FALSE,warning=FALSE,message=FALSE}
setwd("C:/Users/Michael/SkyDrive/Code/GitHub/DSCapstone/Coursera-SwiftKey/final/en_US")
bTop=readRDS("bTop")
tTop=readRDS("tTop")
nTop=readRDS("nTop")

# plot

barplot(bTop, col ="firebrick1", main="Top 10 Words: Blogs Corpus", xlab = "Frequency", cex.lab = 1.3, horiz=TRUE, las=1)

barplot(nTop, col ="chocolate2", main="Top 10 Words: News Corpus", xlab = "Frequency", cex.lab = 1.3,horiz=TRUE, las=1)

barplot(tTop, col ="cadetblue3", main="Top 10 Words: Twitter Corpus", xlab = "Frequency", cex.lab = 1.3, horiz=TRUE, las=1)
```
