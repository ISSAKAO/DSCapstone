---
title: "Milestone Report - Data Science Capstone - Johns Hopkins/Coursera"
author: "Michael Liquori"
date: "Saturday, November 15, 2014"
output: html_document
---

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r, echo=FALSE}

# Start the clock!
ptm <- proc.time()

# SETUP #
gc()
setwd("C:/Users/Michael/SkyDrive/Code/GitHub/DSCapstone/Coursera-SwiftKey/final/en_US")
library(tm)

# FUNCTION DEFINITIONS #

# Make Corpus, Transform, Make Trigram TDM
makeTDM <- function(x) {
corpus<-Corpus(VectorSource(x))
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, content_transformer(tolower))
# corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stemDocument)
corpus<- tm_map(corpus,removePunctuation)
corpus<- tm_map(corpus,removeNumbers)
tdm<- TermDocumentMatrix(corpus)
#tdm<-removeSparseTerms(tdm,0.97)
return(tdm)}

## DATA MUNGING ##

# 1. Corpus, transformations, and TDM Creation
#=============================================#

fileMunge<- function(x) {
text<-readLines(x)
totalLines=length(text)
chunkSize=20000
chunks=totalLines/chunkSize
remainder = chunks %% 1
wholeChunks = chunks-remainder
# initialize list
output=list()
# break file into chunks 
i=1
line=1
while (i<=wholeChunks){
end=line+chunkSize-1
output[[i]]<-text[line:end]
line=end+1
i=i+1
}
output[[i]]<-text[line:totalLines]
# Text Transformations to remove odd characters #
output=lapply(output,FUN=iconv, to='ASCII', sub=' ')
output=lapply(output,FUN= function(x) gsub("'{2}", " ",x))
output=lapply(output,FUN= function(x) gsub("[0-9]", " ",x))
}

# Read, chunk, parse data, then make corpus, do transformations, make TDM of tri-grams:
twit<-fileMunge("en_US.twitter.txt")
tTDM <- makeTDM(twit)
rm(twit)
gc()

news<-fileMunge("en_US.news.txt")
nTDM <- makeTDM(news)
rm(news)
gc()

blog<-fileMunge("en_US.blogs.txt")
bTDM <- makeTDM(blog)
rm(blog)
gc()

## Get line count

tlCount<-system("wc -l en_US.twitter.txt",intern=TRUE)
tlCount<-strsplit(tlCount," ")
tlCount<-as.numeric(tlCount[[1]][1])

nlCount<-system("wc -l en_US.news.txt",intern=TRUE)
nlCount<-strsplit(nlCount," ")
nlCount<-as.numeric(nlCount[[1]][1])

blCount<-system("wc -l en_US.blogs.txt",intern=TRUE)
blCount<-strsplit(blCount," ")
blCount<-as.numeric(blCount[[1]][1])

## Number of Total Words

twCount<-system("wc -w en_US.twitter.txt",intern=TRUE)
twCount<-strsplit(twCount," ")
twCount<-as.numeric(twCount[[1]][1])

nwCount<-system("wc -w en_US.news.txt",intern=TRUE)
nwCount<-strsplit(nwCount," ")
nwCount<-as.numeric(nwCount[[1]][1])

bwCount<-system("wc -w en_US.blogs.txt",intern=TRUE)
bwCount<-strsplit(bwCount," ")
bwCount<-as.numeric(bwCount[[1]][1])

## Get maximum line length

tlmount<-system("wc -L en_US.twitter.txt",intern=TRUE)
tlmount<-strsplit(tlmount," ")
tlmount<-as.numeric(tlmount[[1]][1])

nlmount<-system("wc -L en_US.news.txt",intern=TRUE)
nlmount<-strsplit(nlmount," ")
nlmount<-as.numeric(nlmount[[1]][1])

blmount<-system("wc -L en_US.blogs.txt",intern=TRUE)
blmount<-strsplit(blmount," ")
blmount<-as.numeric(blmount[[1]][1])

## Get unique words

tUgrams<-dimnames(tTDM)$Terms
bUgrams<-dimnames(bTDM)$Terms
nUgrams<-dimnames(nTDM)$Terms

### Number of Unique Words

tuwCount=length(tUgrams)
buwCount=length(bUgrams)
nuwCount=length(nUgrams)




## Get frequency table with word lengths.

charFreqTable<-function(x){
wLengths<-as.numeric((lapply(x,nchar)))
wlFreq=lapply(min(wLengths):max(wLengths),FUN=function(x){length(which(wLengths==x))})
counts=list(min(wLengths):max(wLengths))
return(data.frame(Chars=(min(wLengths):max(wLengths)),Freq=unlist(wlFreq)))
}

twlFreq<-charFreqTable(tUgrams)
bwlFreq<-charFreqTable(bUgrams)
nwlFreq<-charFreqTable(nUgrams)

## AVERAGE word Length
quantity=colsum
product=lapply(1:nrow(twlFreq),FUN=function(x){(twlFreq[x,1]*twlFreq[x,2])})
twlFreq$product=product

# Plot

## Line numbers in each Corpus, horizontal barplot

data<-data.frame(names=c("Twitter Corpus", "News Corpus", "Blogs Corpus"), counts=c(1,2,3))
par(mai=c(1,2,1,1))
barplot(data$count, main="Number of Lines per Corpus", horiz=TRUE, names.arg=data$names, las=1)

## Line length in each Corpus, 3 histograms

## AVERAGE word Length


## Word length Frequency Histograms for each Corpus
twLengths<-as.numeric((lapply(tUgrams,nchar)))
bwLengths<-as.numeric((lapply(bUgrams,nchar)))
nwLengths<-as.numeric((lapply(nUgrams,nchar)))

hist(twLengths, col ="cadetblue3", breaks=seq(0,max(twLengths),by=1), main="Twitter Corpus Word-length frequency", xlab = "Values", cex.lab = 1.3)

hist(nwLengths, col ="chocolate2", breaks=seq(0,max(nwLengths),by=1), main="News Corpus Word-length frequency", xlab = "Values", cex.lab = 1.3)

hist(bwLengths, col ="firebrick1", breaks=seq(0,max(bwLengths),by=1), main="Blogs Corpus Word-length frequency", xlab = "Values", cex.lab = 1.3)

# Stop the clock
proc.time() - ptm # 1255.23

```
