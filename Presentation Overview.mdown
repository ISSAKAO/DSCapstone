Overview of presentation

Key reasons for my decision:

* bigrams would be important in a prediction model for live typing, but not for predicting the end of a sentence.


Entropy / Perplexity measure?

mystring <- c(1,2,3,1,3,5,4,2,1,3,2,4,2,2,3,4,4)
myfreqs <- table(mystring)/length(mystring)
# vectorize
myvec <- as.data.frame(myfreqs)[,2]
# H in bit
-sum(myvec * log2(myvec))
[1] 2.183667

or just feed frequencies to entropy package:
entropy.empirical(mystring, unit="log2")

Perplexity is just 2^Entropy, essentially undoing the log2 part...

Entropy may be regarded as the average number of bits needed to represent a test event xi if one uses an optimal code based on q. Low-perplexity models do a better job of compressing the test sample, requiring few bits per test element on average because q(xi) tends to be high.