DS Phase 2
====
# NEXT

* **Improving lookup model:**
	- right now I have duplicate entries? Yes, i have "zumwalt west in" and "zumwalt west won" which is completely useless!

	BUT, how would i build this table and how long would it take?

	I should instead have a table that only contains the top matching test and prediction. 

* UNSTEM the prediction finally.

X ***KEY*** you must produce a prediction for every word. So maybe if NA, just produce "the" ? Because bigrams can be NA too, ALTHOUGH I have not tried to do bigrams with the ENTIRE data.

- This is true, bigrams CAN be NA, but they could also increase accuracy. Is the speed trade off worth it though? Well, looks like Apple and Swiftkey just put The also. 

Regarding fallback to all corpus, this may not necessarily be the best option, shouldn't i still go with the largest one?
OTOH, a.acc IS the most accurate on average.

* Next I wonder, how much can i shrink the databases and still get decent accuracy, because this size is too slow.

* See Kneser-Key explanation at `https://class.coursera.org/dsscapstone-002/forum/thread?thread_id=86`. Even though bigrams not terribly useful, unigram may be? Hm but that's bigram isn't it? I don'

* Perplexity?

* it seems that a combined trigram thingie works better than classification, so we should use that and then see if we can minimize size to speed up cutting out singletons, 2s, 3s etc.
However, it may be that for reducing size, classification will be better.

# WHAT IS HAPPENING NOW

>How about with classification per line?

OK, server.R already does this. But it does not give us an accuracy measure.

X Complete creating frequency tables for ALL sets.

>NEXT:

Should i do more than trigrams? Read:
`C:\Users\Michael\SkyDrive\Code\GitHub\DSCapstone\up from trigrams`

X This will be too much headache and not necessary (*unless I want to add Quadrigrams etc...*. Get an Amazon EC2 to get below done and get that experience.

> I am now going to make a trigram database of ALL the data since I know now how to do it, breaking each into 3 sets, then each takes about an hour to process so 9 hours.

> I am surprised that combining 2 training sets does WORSE, but I suppose the fact that some outliers  are erroneously pushed up to 2 count may play a role. Is it possible that eliminating the lowest will help? no. It is the randomization process that screwed me here. 
Possibly answering: 
**Question** 
is it bad that i may have duplicates in my training sets? Is this bad statistical practice? If so, I should redo it obvs. 


> More training sets to improve our freqtable? (just change set.seed in `CreateTrainingSet.3.smaller(forquadrigrams).R`)

	

> Create quadrigrams Freq Tables (FIRST must make smaller training sets) 
`CreateTrainingSet.3.smaller(forquadrigrams).R`

-

X Here's an idea, what about if I just merge the trigram sets into 1??
`Merge trigram sets to 1.R`

X now test accuracy of `b4.t6.n4.Tfreq.RDS` on quiz2 and 3

It is looking like bigram backoff is useless. So now, I would like to try a database of 4-grams, which will require smaller training files? Or keep reading for better modeling.

X I am seeing if bigram backoff makes any difference on Quiz 2, since it did NOT on quiz3. `Predictions9.bigramStepback.Quiz2.R`

X I am running process training set.7.BiGrams.R to create backoff bigrams for NA results and try to hit 20% accuracy. I may want to redo the Blog one because I think I ran prediction 6 on it, but is that even any different? not sure. Actually! I don't think so! Did I use correct train4? dunno maybe used train3 by mistake?

X I am running "process training set.6.ProcessFix3.TriGramsOnly.RunningonNewsNow.R" to create n.Tfreq4.RDS. Then I will have three corpus frequency tables approx 30 MB each.

*Next steps*

* Bigram backoff added nothing (although sample size was too small?) So now we move to Quadrigram and see if that helps.

* UNSTEM the prediction finally.

* do nrows instead of is.na in predict function.


# Done?
1. develop algorithm which can classify and test accuracy by going back in sentence and isolating/testing each known trigram.

1.  OK what's my last prediction file? Predictions6.Large.t.Tfreq6..2.functionalize.R
	Now working as *Predictions7.ClassificationTesting.R*

2. Write an algorithm which classifies text by testing all previous trigrams.

So it will load t.TFreq4.RDS first.

3. Add all 3 corpuses to shiny app and classification test (getPred())

4. OK i have a feeling this will be too slow.

* Try reducing size of corpus and doing accuracy measurements again.

	*Predictions8.FunctionalizeForTableOptimization.R*

	OK while it made no diff on T corpus at 16%, it did bring blog corpus down from 17.3% to 15.5%. Since I am aiming towards 20%, this is not good. I need to improve the algorithm first, then reduce.


* *idea* so, the trigram is the trigram and that's that, but if I could come up with a better prediction for the NA cases, THAT could push me over 20% (The 'unseen' trigrams).

This could be done by stepping to bigrams of course.

So let's create bigram database?

First, how many NAs do i get ?

* Now, test accuracy with bigram stepback.
	

----

# A is for ALGORITHM - Now I want to improve my model.

## Measures of accuracy

* cross-entropy

* Perplexity

In 1991, a trigram model was used on a large cor-
pus of one million English words to achieve a perplex-
ity score of 247 per word, corresponding to a cross en-
tropy of 7.95 bits per word or 1.75 bits per letter (Brown,
Della Pietra, Della Pietra, Lai, Mercer, 1992). On this
corpus, ASCII coding has a cross entropy of 8 bits per
character, Huffman coding has 4.46, and the UNIX com-
mand compress has 4.43. On more specialized cor-
pora it is possible to achieve lower perplexity scores than
for more general corpora. Recently, a word perplexity
score of 96.9 was reported on the Associated Press corpus
using a technique called stochastic memoization (Wood,
Archambeau, Gasthaus, James, and Teh, 2009). This is
significantly lower than the perplexity scores reported for
competing approaches.

## Classification

 each training category can be used as a separate
training set for the prediction/classification algorithm. In
the case of prediction, the prediction error for the data
is compared using each category as a training set  The
data can be classified as being in the category which re-
sults in the lowest prediction error.

*SO, what you would do is, run each trigram of the sentence which is KNOWN through the two sets to find the lowest error*

ok cool, that is actionable info.

Steps to build algorithm:

1. Take a test sentence
2. Break into trigrams
3. Run Prediction.
4. Get a measure of accuracy.
So: n.train4.RDS & t.Tfreq6.RDS ?
did i never build the freq table for news perhaps?
nope.




# B. DESIGN
keep the default screen extremely simple Google and Apple but have a button that turns on advanced visualizations where you can put some cool word cloud or something like inverse perfect list of the other options and the probability is even perhaps perhaps and maybe something that tells the identification whether this is from Twitter or news
================
# Initial loading pop-up of some sort

http://stackoverflow.com/questions/18237987/show-that-shiny-is-busy-or-loading-when-changing-tab-panels

# Model Analytics

* Perplexity

* performance vs. accuracy

* Create a test set you can run on different corpus to get automatic accuracies.

* Use 1, 2, and 3 n-grams?

* Markov Chains?

* smooth probabilities

* katz back-off model

* Can you estimate how uncertain you are about the words you are predicting? 

* What are the most commonly missed n-grams? Can you think of a reason why they would be missed and fix that? 

* Other Data Visualizations?
	
	* Word Cloud

	* Graphs of probability?

# *Elements for Presentation*

* Fix for commonly missed

* text cleanup, unstemming

* Different corpuses, classifying

* estimating uncertainty

* How should you document the use of your data product (separately from how you created it) so that others can rapidly deploy your algorithm?

# Improving accuracy


#Unstem:
"dog is my" produces "favorit"

#Shinysky:
----
* Busy indicator for initial loading

```r
# Server.R:
output$plot1 <- renderPlot({
	if (input$busyBtn == 0) 
		return()
	Sys.sleep(3)
	hist(rnorm(10^3))
})
# UI
busyIndicator("Calculation In progress",wait = 0)
	,actionButton("busyBtn","Show busyInidcator")
	,plotOutput("plot1")
```

* Typeahead for word selection?

https://github.com/AnalytixWare/ShinySky

#Examples:
----
https://www.google.com/search?client=opera&q=site%3A+shinyapps.io+prediction&sourceid=opera&ie=UTF-8&oe=UTF-8

# TO DO
----
0. 

* Loading message?

# display something else for NA?


# Classify the text 
to use a particular corpus based on how highly the words score in the unigram frequency database of that corpus (stop words removed)

# X Get the basics of the Shiny App up and running:
	1. A Shiny app that takes as input a phrase (multiple words) in a text box input and outputs a prediction of the next word (SINGLE).

NOTE: top score requires only correct prediction in 20% of cases, 1/5! (Drawn from Twitter and news)

# Ways to make it more awesome:
	
* have text prediction appear inline, letter-by-letter even?

* have a word cloud or top 5 list appear separately?


----
#Shiny issues

setwd("C:/Users/Michael/SkyDrive/Code/GitHub/DSCapstone/Predictor")
shinyapps::configureApp("Predictor", size="xxlarge") 


You can access the error log with ShinyApps::showLogs()

By default, Shiny limits file uploads to 5MB per file. You can modify this limit by using the shiny.maxRequestSize option. For example, adding options(shiny.maxRequestSize=30*1024^2) to the top of server.R would increase the limit to 30MB.

http://shiny.rstudio.com/articles/shinyapps.html