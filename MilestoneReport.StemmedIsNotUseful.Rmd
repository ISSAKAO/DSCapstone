---
title: "Milestone Report"
subtitle: "Data Science Capstone- Johns Hopkins / Coursera"
author: "Michael Liquori"
date: "Saturday, November 15, 2014"
output: html_document
---
<hr>

#### Background

The aim of this project is to develop a model that will allow us to predict the next word a user intends to type to aid in faster typing, similar to the feature currently embedded in most smartphones. We will seek to balance speed and accuracy in developing an algorithm which is competitive with the current word-prediction keyboards available, such as "SwiftKey".

The data provided for NLP (Natural Language Processing) consists of 3 "corpora" of data, one collected from <font color="red" face="bold">**blog posts**</font>, one from <font color="orange" face="bold">**news articles**</font>, and a third from <font color="skyblue" face="bold">**twitter messages**</font> ("tweets"). 

#### Exploratory Analysis

We begin by analyzing some basic facts about each corpus such as number of lines, number of words, number of unique words, line length, and average unique word length.

```{r, echo=FALSE,warning=FALSE,message=FALSE}

# SETUP #
setwd("C:/Users/Michael/SkyDrive/Code/GitHub/DSCapstone/Coursera-SwiftKey/final/en_US")
library(tm)
load("C:/Users/Michael/SkyDrive/Code/GitHub/DSCapstone/Coursera-SwiftKey/final/en_US/RanMdownScriptFullProcessing.RData")

# Line counts

tlCount=2360148
nlCount=1010242
blCount=899288

# Number of Total Words

twCount=30359852
nwCount=34365936
bwCount=37334114

# Number of words per line
tnwl=twCount/tlCount
nnwl=nwCount/nlCount
bnwl=bwCount/blCount

# Get maximum line length

tlmount=173
nlmount=11384
blmount=40833

# Get unique words unstemmed

tuw<-1450780
nuw<-945909
buw<-1216390

## Get unique words Stemmed

tUgrams<-dimnames(tTDM)$Terms
bUgrams<-dimnames(bTDM)$Terms
nUgrams<-dimnames(nTDM)$Terms

### Number of Unique Words

tuwCount=454293
buwCount=355364
nuwCount=79427

## Get frequency table with word lengths.

charFreqTable<-function(x){
wLengths<-as.numeric((lapply(x,nchar)))
wlFreq=lapply(min(wLengths):max(wLengths),FUN=function(x){length(which(wLengths==x))})
counts=list(min(wLengths):max(wLengths))
return(data.frame(Chars=(min(wLengths):max(wLengths)),Freq=unlist(wlFreq)))
}

twlFreq<-charFreqTable(tUgrams)
bwlFreq<-charFreqTable(bUgrams)
nwlFreq<-charFreqTable(nUgrams)

## AVERAGE unique word Length

tproduct=lapply(1:nrow(twlFreq),FUN=function(x){(twlFreq[x,1]*twlFreq[x,2])})
tAvg=(sum(unlist(tproduct))/sum(twlFreq$Freq)) # total chars

bproduct=lapply(1:nrow(bwlFreq),FUN=function(x){(bwlFreq[x,1]*bwlFreq[x,2])})
bAvg=(sum(unlist(bproduct))/sum(bwlFreq$Freq)) # total chars

nproduct=lapply(1:nrow(nwlFreq),FUN=function(x){(nwlFreq[x,1]*nwlFreq[x,2])})
nAvg=(sum(unlist(nproduct))/sum(nwlFreq$Freq)) # total chars

# Plot

# 3 corpora in one plot barplots:

## Shared parameters

# save par defaults first
op<-par()

names=c("Twitter Corpus", "News Corpus", "Blogs Corpus")
colors=c("cadetblue3","chocolate2","firebrick1")
par(mai=c(1,2,1,1))

## Number of Lines

counts=c(tlCount,blCount,nlCount)
bp<-barplot(counts, col=colors, main="Number of Lines", horiz=TRUE, names.arg=names, las=1, axes=FALSE)
axis(1, at = c(0,500000,1000000,1500000,2000000), labels=c("0","500,000","1,000,000","1,500,000","2,000,000"))
text(0,bp,prettyNum(counts,big.mark=",",scientific=F),cex=1.3,pos=4) 
```

We see that the twitter corpus has many more lines than the others. But how many words are in each line?

```{r, echo=FALSE,warning=FALSE,message=FALSE}

## Average Number of Words per Line
par(mai=c(1,2,1,1))
counts=c(tnwl,bnwl,nnwl)
bp<-barplot(counts, col=colors, main="Average Number of Words per Line", horiz=TRUE, names.arg=names, las=1, axes=FALSE, xlim=c(0,50))
axis(1, at = seq(0,50,by=10))
text(0,bp,prettyNum(counts,big.mark=",",scientific=F),cex=1.3,pos=4) 
```

We see that lines are much shorter in the twitter corpus and longest in the news corpus. 

But, what exactly do lines represent? In the twitter corpus they seem to represent one tweet, which has a clearly defined limit of 140 characters. In the blogs and news corpus, they are less defined, sometimes representing just one sentence, sometimes an entire paragraph. 

Now let's examine the diversity of unique words used in each corpus:

```{r, echo=FALSE,warning=FALSE,message=FALSE}
## Number of Unique Words
par(mai=c(1,2,1,1))
counts=c(tuw,buw,nuw)
bp<-barplot(counts, col=colors, main="Number of Unique Words", horiz=TRUE, names.arg=names, las=1, axes=FALSE, xlim=c(0,500000))
axis(1, at = c(0,100000,200000,300000,400000,500000), labels=c(0,"100,000","200,000","300,000","400,000","500,000"))
text(0,bp,prettyNum(counts,big.mark=",",scientific=F),cex=1.3,pos=4) 
```

The diversity of words used in blogs are very low, while they are highest in twitter. What about the length of the words used?

```{r, echo=FALSE,warning=FALSE,message=FALSE}
## Average Unique Word Length
par(mai=c(1,2,1,1))
counts=c(tAvg,bAvg,nAvg)
bp<-barplot(counts, col=colors, main="Average Unique Word Length", horiz=TRUE, names.arg=names, las=1, axes=FALSE, xlim=c(0,10))
axis(1, at = c(0,2.5,5,7.5,10), labels=c(0,2.5,5,7.5,10))
text(0,bp,prettyNum(counts,big.mark=",",scientific=F),cex=1.3,pos=4) 
```

Surprisingly, the twitter corpus contains the longest words. 

# Histogram for each corpus barplots:
par(op)
par(mfcol=c(3,1))

## Line lengths in each Corpus, 3 histograms

## Word length Frequency Histograms for each Corpus

twLengths<-as.numeric((lapply(tUgrams,nchar)))
bwLengths<-as.numeric((lapply(bUgrams,nchar)))
nwLengths<-as.numeric((lapply(nUgrams,nchar)))

hist(twLengths, col ="cadetblue3", breaks=seq(0,max(twLengths),by=1), main="Twitter Corpus Word-Lengths", xlab = "Values", cex.lab = 1.3, xlim=c(0,25))

hist(nwLengths, col ="chocolate2", breaks=seq(0,max(nwLengths),by=1), main="News Corpus Word-Lengths", xlab = "Values", cex.lab = 1.3,xlim=c(0,25))

hist(bwLengths, col ="firebrick1", breaks=seq(0,max(bwLengths),by=1), main="Blogs Corpus Word-Lengths", xlab = "Values", cex.lab = 1.3,xlim=c(0,25))

## AVERAGE word Length


```
