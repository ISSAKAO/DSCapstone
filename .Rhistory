corp[1]
corp[[1]
]
corp[[1]][1]
typeof(corp[[1]][1])
typeof(corp[[1]][1][1])
typeof(corp$content[[1]][1])
corp$content[[1]][1]
corp$content[1]
corp$content[[1]]
findAssocs(triTDM,corp[[1]],0)
as.character(corp[[1]])
findAssocs(triTDM,as.character(corp[[1]]),0)
TrigramTokenizer(corp[[1]])
train<-TrigramTokenizer(corp[[1]])
train
train[1]
findAssocs(triTDM,train,0)
findAssocs(triTDM,train[1],0)
typeof(train)
list(train)
findAssocs(triTDM,list(train),0)
trainGrams<-lapply(train,FUN=function(x){findAssocs(triTDM,train[x],0)}
)
trainGrams
trainGrams<-lapply(train,FUN=function(x){findAssocs(triTDM,x,0)})
trainGrams
triTDM
row_sums(triTDM)
library(slam)
row_sums(triTDM)
row_sums(triTDM)[1]
Matrix(test)
?Matrix
triTDM[1]
triTDM[[1]]
summary(triTDM)
triTDM
triTDM$Terms
triTDM[1]$Terms
attributes(triTDM)
names(triTDM)
names(triTDM$dimnames)
names(triTDM$dimnames$Terms)
names(triTDM$dimnames[1])
names(triTDM$dimnames[1][1])
names(triTDM$dimnames[[1]])
names(triTDM$dimnames[1])
triTDM$dimnames[1]
triTDM$dimnames[1][1]
triTDM$dimnames[[1]]
triTDM$dimnames[[1]][2]
triTDM[1,]
triTDM[2,]
?data.frame
triTDM$dimnames[[1]]
typeof(triTDM$dimnames[[1]])
freq<-data.frame(trigrams=triTDM$dimnames[[1]])
freq
freq[1]
dim(freq)
findFreqTerms(triTDM)
?findFreqTerms
inspect(triTDM)
typeof(inspect(triTDM))
inspect(triTDM)[1]
inspect(triTDM)[2]
inspect(triTDM)[[1]]
inspect(triTDM$terms)
inspect(triTDM$Terms)
inspect(triTDM)
dimnames(triTDM)
dimnames(triTDM)$Docs
tdm["are my favorite"]
tdm["are my favorit"]
triTDM["are my favorit"]
triTDM["are my favorit",]
triTDM["are my favorit",dimnames(triTDM)$Docs]
inspect(triTDM)
z<-inspect(triTDM)
typeof(z)
rowSums(z)
rowSums(z)[1]
rowSums(z)["are my favorit"]
rowSums(z)["band are playing"]
z
rowSums(inspect(triTDM))
freq<-data.frame(trigrams=triTDM$dimnames[[1]], count=rowSums(inspect(triTDM))
)
freq
freq<-data.frame(trigrams=triTDM$dimnames[[1]], count=rowSums(inspect(triTDM))
)
freq
rowSums(inspect(triTDM)
)
nrows(freq)
nrow(freq)
freq$MLE<-(freq$count/nrow(freq))
freq
product=c(1:10)
product
sum(product)
data.frame(Freq=product)
df<-data.frame(Freq=product)
df
sum(df$Freq)
tavg=(1/product)
data<-data.frame(names=c("Twitter Corpus", "News Corpus", "Blogs Corpus"), counts=c(1,2,3))
par(mai=c(1,2,1,1))
barplot(data$count, main="Number of Lines per Corpus", horiz=TRUE, names.arg=data$names, las=1)
data
barplot(data, main="Average Unique Word Length per Corpus", horiz=TRUE, names.arg=data$names, las=1)
barplot(data$count, main="Average Unique Word Length per Corpus", horiz=TRUE, names.arg=data$names, las=1)
barplot(data$count, main="Average Unique Word Length per Corpus", horiz=TRUE, names.arg=data$names, las=1)
data
data<-data.frame(names=c("Twitter Corpus", "News Corpus", "Blogs Corpus"), counts=c(3:1))
par(mai=c(1,2,1,1))
barplot(data$counts, main="Average Unique Word Length per Corpus", horiz=TRUE, names.arg=data$names, las=1)
barplot(data$count, main="Average Unique Word Length per Corpus", horiz=TRUE, names.arg=data$names, las=1)
par()
load("C:/Users/Michael/SkyDrive/Code/GitHub/DSCapstone/Coursera-SwiftKey/final/en_US/triblogs only.TDMloaded.RData")
counts=row_sums(b.tdm)
library(slam)
counts=row_sums(b.tdm)
counts[1]
counts[2]
counts[200]
counts[2001]
triblog<-data.frame(grams=names(counts), counts=counts))
triblog<-data.frame(grams=names(counts), counts=counts)
View(triblog)
triblog[1,]
object.size(triblog)
rm(triblog)
gc()
which(names(counts)=="escort")
names(counts[2])
names(counts[3])
which(names(counts)=="a a a")
?grep
grep("a a",counts)
grep("a a",names(counts))
grep("a a ",names(counts))
a_a<-counts[grep("a a ",names(counts)),]
dim(counts)
rows<-grep("a a ",names(counts))
results<-data.frame(names=names(counts[rows,]),counts=counts[rows,])
results<-data.frame(names=names(counts[rows]),counts=counts[rows])
View(results)
predict<-function(x){
rows<-grep(x,names(counts))
results<-data.frame(names=names(counts[rows]),counts=counts[rows])
}
predict("a a")
predict("at the")
View(results)
results<-predict("at the")
View(results)
results<-predict(" at the ")
View(results)
results<-predict("at the")
View(results)
results<-predict(" at the")
results<-predict("at the ")
View(results)
View(results)
predict<-function(x){
rows<-grep(x,names(counts))
results<-data.frame(names=names(counts[rows]),counts=counts[rows])
results<-results[order(-results$counts),]
}
results<-predict("at the ")
View(results)
results2<-predict("at the")
object.size(results2)
View(results2)
results<-predict("at the beach")
View(results)
results<-predict("at the mall")
View(results)
results<-predict("at the movies")
View(results)
results<-predict("at the movi")
View(results)
results<-predict("on my ")
View(results)
rm(results2)
rm(b.tdm,b.dat)
rm(ptm)
save.image("C:/Users/Michael/SkyDrive/Code/GitHub/DSCapstone/Coursera-SwiftKey/final/en_US/triblogs only.2.countsOnlyandPredictReady..RData")
results<-predict("must be ")
View(results)
results<-predict("must be insane")
View(results)
gc()
saveRDS(counts,"blogTriCounts")
library(ff)
my.obg<-ff(counts)
my.obg
?ff
ffBTcounts<-ff(counts,filename=ffBTcounts)
ffBlogs<-ffdf(tri=counts)
Counts<-ff(counts)
ffBlogs<-ffdf(tri=Counts)
rm(my.obg)
ffBlogs
ffBlogs
dim(ffBlogs)
ffsave(ffBlogs)
ffsave(ffBlogs,file=ffBlogs)
ffsave(ffBlogs,file="ffBlogs")
ffsave(ffBlogs,file="Users/Michael/SkyDrive/Code/GitHub/DSCapstone/Coursera-Swiftkey/final/en_US/ffBlogs")
ffBlogs$tri
ffBlogs$tri[1]
ffBlogs$tri[2]
ffBlogs$tri[200]
ffBlogs[1,]
ffBlogs<-ffdf(names=names(Counts),freq=Counts)
names(Counts)
names(counts)
glob2rx("blue*")
?grob2rx
?glob2rx
shiny::runApp('C:/Users/Michael/SkyDrive/Code/GitHub/Health2')
library(googleVis)
library(shiny)
###Pre-Processing
YTD <- read.csv("1B.csv",stringsAsFactors=FALSE)
YTD <-YTD[-1:-4,]
YTD2<-YTD[,c(1,6:11,13:15)]
YTD2=apply(YTD2, c(1,2), function(x) gsub('\\%', '', x))
YTD2=apply(YTD2, c(1,2), function(x) gsub('\\$', '', x))
YTD2=data.frame(YTD2)
#Row and column names
colnames(YTD2)<-as.character(c("Country/Region","Full Year OP ($$)","YTD OP ($$)","YTD Actuals ($$)","% to OP (%)","Meets OI Requirements", "YTD Incremental over OP ($$)", "% Local Growth OP", "% Local Growth Actual", "LCG % pts. over OP"))
#Filter
YTD2$"sort1" <- as.numeric(sub("\\$","", YTD2$"Full Year OP ($$)"))
YTD3<- subset(YTD2,sort1>=7)
YTD4<-subset(YTD3,YTD3$"Meets OI Requirements"=="YES")
#Cut to YTD columns
YTD5<-YTD4[,c(1:5,7)]
#Make numeric sort column, sort, erase sort column
YTD5$"sort2" <- as.numeric(sub("\\$","", YTD5$"YTD Incremental over OP ($$)"))
YTD6<-YTD5[order(-YTD5$sort2),]
rownames(YTD6)<-c(1:dim(YTD6)[1])
YTD7<-cbind("Rank"=rownames(YTD6),YTD6[,1:6])
#YTD Table ready.
##HTD , starting from YTD4
#Cut to HTD columns
HTD1<-YTD4[,c(1,8:10)]
#Make numeric sort column, sort, erase sort column
HTD1$"sort3" <- as.numeric(sub("\\%","", HTD1$"LCG % pts. over OP"))
HTD2<-HTD1[order(-HTD1$sort3),]
rownames(HTD2)<-c(1:dim(HTD2)[1])
HTD3<-cbind("Rank"=rownames(HTD2),HTD2[,1:4])
#HTD Table ready.
###Fix non-displayed countries
YTD9<-YTD7
HTD4<-HTD3
regions <- function(YTD8) {
#UK Ireland
if (!is.null(which(YTD8[2]=="UK Ireland"))) {
YTD8[,2]<-gsub("UK Ireland", "Great Britain", YTD8[,2])
YTD8<-rbind(YTD8,YTD8[which(YTD8[2]=="Great Britain"),])
YTD8[dim(YTD8)[1],2]<-"Ireland"
}
#Alpine = Swiss, Liechtenstein, Austria.
if (!is.null(which(YTD8[2]=="Alpine"))) {
YTD8[,2]<-gsub("Alpine", "Switzerland", YTD8[,2])
YTD8<-rbind(YTD8,YTD8[which(YTD8[2]=="Switzerland"),])
YTD8[dim(YTD8)[1],2]<-"Liechtenstein"
YTD8<-rbind(YTD8,YTD8[which(YTD8[2]=="Switzerland"),])
YTD8[dim(YTD8)[1],2]<-"Austria"
}
#Benelux = Belgium, Luxembourg, Netherlands
if (!is.null(which(YTD8[2]=="Benelux"))) {
YTD8[,2]<-gsub("Benelux", "Netherlands", YTD8[,2])
YTD8<-rbind(YTD8,YTD8[which(YTD8[2]=="Netherlands"),])
YTD8[dim(YTD8)[1],2]<-"Luxembourg"
YTD8<-rbind(YTD8,YTD8[which(YTD8[2]=="Netherlands"),])
YTD8[dim(YTD8)[1],2]<-"Belgium"
}
#Gulf = Kuwait, Bahrain, Oman, Qatar, UAE
if (!is.null(which(YTD8[2]=="Gulf"))) {
YTD8[,2]<-gsub("Gulf", "United Arab Emirates", YTD8[,2])
YTD8<-rbind(YTD8,YTD8[which(YTD8[2]=="United Arab Emirates"),])
YTD8[dim(YTD8)[1],2]<-"Oman"
YTD8<-rbind(YTD8,YTD8[which(YTD8[2]=="United Arab Emirates"),])
YTD8[dim(YTD8)[1],2]<-"Qatar"
YTD8<-rbind(YTD8,YTD8[which(YTD8[2]=="United Arab Emirates"),])
YTD8[dim(YTD8)[1],2]<-"Bahrain"
YTD8<-rbind(YTD8,YTD8[which(YTD8[2]=="United Arab Emirates"),])
YTD8[dim(YTD8)[1],2]<-"Kuwait"
}
#Iberia = Port, spain
if (!is.null(which(YTD8[2]=="Iberia"))) {
YTD8[,2]<-gsub("Iberia", "Spain", YTD8[,2])
YTD8<-rbind(YTD8,YTD8[which(YTD8[2]=="Spain"),])
YTD8[dim(YTD8)[1],2]<-"Portugal"
YTD8<-rbind(YTD8,YTD8[which(YTD8[2]=="Spain"),])
YTD8[dim(YTD8)[1],2]<-"Andorra"
}
#Central America & Caribbean Region = DR, Panama, Guatemala, CR, Honduras, Nicaragua, El Salvador
if (!is.null(which(YTD8[2]=="Central America & Caribbean Region"))) {
YTD8[,2]<-gsub("Central America & Caribbean Region", "Dominican Republic", YTD8[,2])
YTD8<-rbind(YTD8,YTD8[which(YTD8[2]=="Dominican Republic"),])
YTD8[dim(YTD8)[1],2]<-"Panama"
YTD8<-rbind(YTD8,YTD8[which(YTD8[2]=="Dominican Republic"),])
YTD8[dim(YTD8)[1],2]<-"Guatemala"
YTD8<-rbind(YTD8,YTD8[which(YTD8[2]=="Dominican Republic"),])
YTD8[dim(YTD8)[1],2]<-"Costa Rica"
YTD8<-rbind(YTD8,YTD8[which(YTD8[2]=="Dominican Republic"),])
YTD8[dim(YTD8)[1],2]<-"Honduras"
YTD8<-rbind(YTD8,YTD8[which(YTD8[2]=="Dominican Republic"),])
YTD8[dim(YTD8)[1],2]<-"Nicaragua"
YTD8<-rbind(YTD8,YTD8[which(YTD8[2]=="Dominican Republic"),])
YTD8[dim(YTD8)[1],2]<-"El Salvador"
YTD8<-rbind(YTD8,YTD8[which(YTD8[2]=="Dominican Republic"),])
YTD8[dim(YTD8)[1],2]<-"Jamaica"
YTD8<-rbind(YTD8,YTD8[which(YTD8[2]=="Dominican Republic"),])
YTD8[dim(YTD8)[1],2]<-"Trinidad and Tobago"
}
#Andean Region = Ecuador, Peru, Bolivia, Paraguay
if (!is.null(which(YTD8[2]=="Andean Region"))) {
YTD8[,2]<-gsub("Andean Region", "Ecuador", YTD8[,2])
YTD8<-rbind(YTD8,YTD8[which(YTD8[2]=="Ecuador"),])
YTD8[dim(YTD8)[1],2]<-"Peru"
YTD8<-rbind(YTD8,YTD8[which(YTD8[2]=="Ecuador"),])
YTD8[dim(YTD8)[1],2]<-"Bolivia"
YTD8<-rbind(YTD8,YTD8[which(YTD8[2]=="Ecuador"),])
YTD8[dim(YTD8)[1],2]<-"Paraguay"
}
#ANZ = NA for now
if (!is.null(which(YTD8[2]=="ANZ"))) {
YTD8[,2]<-gsub("ANZ", "Australia", YTD8[,2])
YTD8<-rbind(YTD8,YTD8[which(YTD8[2]=="Australia"),])
YTD8[dim(YTD8)[1],2]<-"New Zealand"
}
#Nordic = NA for now
#Argentina Uruguay
#Cesko
#Singapore Region
return(YTD8)
}
YTD9<-regions(YTD9)
HTD4<-regions(HTD4)
###MAP
output$myMap <- renderGvis({
if (input$adjust==1){
data<-cbind(YTD9,"YTD Inc. USD over OP"=as.numeric(sub("\\$", "", YTD9$"YTD Incremental over OP ($$)")))
#color="{values:[0,.5,1,1.5],colors:['#FF0000', '#FFC0CB', '#FFA500','#008000']}"
color="{colors:['#FF0000', '#FFC0CB', '#FFA500','#008000']}"
var="YTD Inc. USD over OP"
}
if (input$adjust==2){
data<-cbind(HTD4,"LCG Perc. over OP"=as.numeric(sub('\\%', '', HTD4$"LCG % pts. over OP")))
#color="{values:[-50,0,10,30],colors:['#FF0000', '#FFC0CB', '#FFA500','#008000']}"
color="{colors:['#FF0000', '#FFC0CB', '#FFA500','#008000']}"
var="LCG Perc. over OP"
}
gvisGeoChart(data, locationvar="Country/Region", colorvar=var, options=list(
colorAxis=color,height=400,width=600,keepAspectRatio='false'))
})
data<-cbind(YTD9,"YTD Inc. USD over OP"=as.numeric(sub("\\$", "", YTD9$"YTD Incremental over OP ($$)")))
#color="{values:[0,.5,1,1.5],colors:['#FF0000', '#FFC0CB', '#FFA500','#008000']}"
color="{colors:['#FF0000', '#FFC0CB', '#FFA500','#008000']}"
var="YTD Inc. USD over OP"
gvisGeoChart(data, locationvar="Country/Region", colorvar=var, options=list(
colorAxis=color,height=400,width=600,keepAspectRatio='false'))
load("C:/Users/Michael/SkyDrive/Code/GitHub/DSCapstone/Coursera-SwiftKey/final/en_US/twitLoadedAndProcessed.RData")
library(tm)
makeTDM <- function(x) {
corpus<-Corpus(VectorSource(x))
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, stemDocument)
tdm<- TermDocumentMatrix(corpus)
#tdm<-removeSparseTerms(tdm,0.97)
return(tdm)}
library("filehash")
filehashOption("DB1")
dbCreate("t.ass")
db <- dbInit("t.ass", type="DB1")
# ok I need to go through this step-by-step where x is a manageable size still.
x=assocsToDB(twit[[1]][1:500])
x=twit[[1]][1:500]
grep("/",x)
x[64]
gsub("\\\\", " ", x[64])
gsub("\\\/", " ", x[64])
gsub("\\/", " ", x[64])
grep("\",x)
grep("\\",x)
grep("\\\",x)
grep("\\\\",x)
?grep
grep("\",x)
grep("\\",x)
grep("\",x,fixed=true)
grep("\\",x,fixed=true)
grep("\\",x,fixed=TRUE)
grep("\",x,fixed=TRUE)
grep("\\\\\",x)
grep("\\\\",x)
grep("\\\\\\",x)
grep("\\\\",x)
grep("\\\\","he\lo")
="he\lo"
a="he\lo"
a="he\\lo"
a
grep("\\\\","he\\lo")
rm(a)
grep("\\\\\\","he\\lo")
grep("\\\\\","he\\lo")
grep("\\\\\",z)
grep("\\\\\",x)
grep("\\\\\\\",x)
grep("\\\\\\\\",x)
grep("\\\\*",x)
grep("\\\\+",x)
lapply(x,FUN=function(x) gsub("\\/", " ", x))
tdm <- makeTDM(x)
tdm
inspect(tdm)
ass<-lapply(dimnames(tdm)$Terms,FUN=function(x){findAssocs(tdm,x,0)})
ass[which(lapply(1:length(ass),FUN=function(x){is.null(dimnames(ass[[x]]))==1})==TRUE)]=NULL
ass
# Start the clock!
ptm <- proc.time()
lapply(1:length(ass),FUN=function(x){
key=dimnames(ass[[x]])[[2]]
new=ass[[x]][1:length(ass[[x]])]
names(new)=dimnames(ass[[x]])[[1]]
if(dbExists(db,key)==TRUE)
{
existing=db[[key]]
db[[key]]=c(new,existing)
}
else
{
db[[key]]=new
}
})
# Stop the clock
proc.time() - ptm
dbList(db)
# Start the clock!
ptm <- proc.time()
fileMunge<- function(x) {
text<-readLines(x)
totalLines=length(text)
chunkSize=10000
chunks=totalLines/chunkSize
remainder = chunks %% 1
wholeChunks = chunks-remainder
# initialize list
output=list()
# break file into chunks
i=1
line=1
while (i<=wholeChunks){
end=line+chunkSize-1
output[[i]]<-text[line:end]
line=end+1
i=i+1
}
output[[i]]<-text[line:totalLines]
# CONVERT TO ASCII
output=lapply(output,FUN=iconv, to='ASCII', sub=' ')
}
twit<-fileMunge("en_US.twitter.txt")
# Process each element of list into Sentences
process<- function(output) {
# Text Transformations to remove odd characters #
# replace APOSTROPHES OF 2 OR MORE with space - WHY??? that never happens..
# output=lapply(output,FUN= function(x) gsub("'{2}", " ",x))
# Replace numbers with spaces... not sure why to do that yet either.
# output=lapply(output,FUN= function(x) gsub("[0-9]", " ",x))
# Erase commas.
output=lapply(output,FUN=function(x) gsub(",?", "", x))
# Erase ellipsis
output=lapply(output,FUN=function(x) gsub("\\.{3,}", "", x))
# Erase colon
output=lapply(output,FUN=function(x) gsub("\\:", "", x))
##### SENTENCE SPLITTING AND CLEANUP
# Split into sentences only on single periods or any amount of question marks or exclamation marks and -
output<-strsplit(output[[1]],"[\\.]{1}")
output<-strsplit(unlist(output),"\\?+")
output<-strsplit(unlist(output),"\\!+")
output<-strsplit(unlist(output),"\\-+")
# Split also on parentheses
output<-strsplit(unlist(output),"\\(+")
output<-strsplit(unlist(output),"\\)+")
# split also on quotation marks
output<-strsplit(unlist(output),"\\\"")
# remove spaces at start and end of sentences:
output<-lapply(output,FUN=function(x) gsub("^\\s+", "", x))
output<-lapply(output,FUN=function(x) gsub("\\s+$", "", x))
# Replace ~ and any whitespace around with just one space
output<-lapply(output,FUN=function(x) gsub("\\s*~\\s*", " ", x))
# Replace forward slash with space
output<-lapply(output,FUN=function(x) gsub("\\/", " ", x))
# Eliminate empty and single letter values (more?)
output[which(nchar(unlist(unlist(output)))==1)]=NULL
output[which(nchar(unlist(unlist(output)))==0)]=NULL
output=unlist(output)
}
twit<-lapply(twit,FUN=function(x){lapply(x,process)})
# OK i think that worked, taking 514MB.
# so.. twit2[[x]][[y]][z] where x=chunk, y=line, z=sentence
# Stop the clock
proc.time() - ptm
