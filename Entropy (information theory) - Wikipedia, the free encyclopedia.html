<!DOCTYPE html>
<!-- saved from url=(0061)http://en.wikipedia.org/wiki/Entropy_%28information_theory%29 -->
<html lang="en" dir="ltr" class="client-js ve-not-available" hc="a3" style=""><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="UTF-8">
<title>Entropy (information theory) - Wikipedia, the free encyclopedia</title>
<meta name="generator" content="MediaWiki 1.25wmf8">
<link rel="alternate" href="android-app://org.wikipedia/http/en.m.wikipedia.org/wiki/Entropy_(information_theory)">
<link rel="alternate" type="application/x-wiki" title="Edit this page" href="http://en.wikipedia.org/w/index.php?title=Entropy_(information_theory)&action=edit">
<link rel="edit" title="Edit this page" href="http://en.wikipedia.org/w/index.php?title=Entropy_(information_theory)&action=edit">
<link rel="apple-touch-icon" href="http://bits.wikimedia.org/apple-touch/wikipedia.png">
<link rel="shortcut icon" href="http://bits.wikimedia.org/favicon/wikipedia.ico">
<link rel="search" type="application/opensearchdescription+xml" href="http://en.wikipedia.org/w/opensearch_desc.php" title="Wikipedia (en)">
<link rel="EditURI" type="application/rsd+xml" href="http://en.wikipedia.org/w/api.php?action=rsd">
<link rel="alternate" hreflang="x-default" href="http://en.wikipedia.org/wiki/Entropy_(information_theory)">
<link rel="copyright" href="http://creativecommons.org/licenses/by-sa/3.0/">
<link rel="alternate" type="application/atom+xml" title="Wikipedia Atom feed" href="http://en.wikipedia.org/w/index.php?title=Special:RecentChanges&feed=atom">
<link rel="canonical" href="http://en.wikipedia.org/wiki/Entropy_(information_theory)">
<link rel="stylesheet" href="http://bits.wikimedia.org/en.wikipedia.org/load.php?debug=false&lang=en&modules=ext.gadget.DRN-wizard%2CReferenceTooltips%2Ccharinsert%2Cfeatured-articles-links%2CrefToolbar%2Cteahouse%7Cext.math.styles%7Cext.rtlcite%2Cwikihiero%2CwikimediaBadges%7Cext.uls.nojs%7Cext.visualEditor.viewPageTarget.noscript%7Cmediawiki.legacy.commonPrint%2Cshared%7Cmediawiki.skinning.interface%7Cmediawiki.ui.button%7Cskins.vector.styles%7Cwikibase.client.init&only=styles&skin=vector&*">
<style>
@-webkit-keyframes centralAuthPPersonalAnimation{0%{opacity:0;-webkit-transform:translateY(-20px)}100%{opacity:1;-webkit-transform:translateY(0)}}@-moz-keyframes centralAuthPPersonalAnimation{0%{opacity:0;-moz-transform:translateY(-20px)}100%{opacity:1;-moz-transform:translateY(0)}}@-o-keyframes centralAuthPPersonalAnimation{0%{opacity:0;-o-transform:translateY(-20px)}100%{opacity:1;-o-transform:translateY(0)}}@keyframes centralAuthPPersonalAnimation{0%{opacity:0;transform:translateY(-20px)}100%{opacity:1;transform:translateY(0)}}.centralAuthPPersonalAnimation{-webkit-animation-duration:1s;-moz-animation-duration:1s;-o-animation-duration:1s;animation-duration:1s;-webkit-animation-fill-mode:both;-moz-animation-fill-mode:both;-o-animation-fill-mode:both;animation-fill-mode:both;-webkit-animation-name:centralAuthPPersonalAnimation;-moz-animation-name:centralAuthPPersonalAnimation;-o-animation-name:centralAuthPPersonalAnimation;animation-name:centralAuthPPersonalAnimation}
/* cache key: enwiki:resourceloader:filter:minify-css:7:0dcacc990dd02e7db9669ab3090b80f1 */
.mw-viewPageTarget-loading{width:128px;height:15px;float:right} .mw-editsection{white-space:nowrap; unicode-bidi:-moz-isolate;unicode-bidi:-webkit-isolate;unicode-bidi:isolate}.mw-editsection-divider{color:#555}.ve-tabmessage-appendix{font-size:0.7em;vertical-align:top;line-height:1.43em;padding-left:0.5em; background-image:none !important;display:inline !important} .mw-viewPageTarget-loading{background-image:url(data:image/gif;base64,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);background-image:url(//bits.wikimedia.org/static-1.25wmf8/extensions/VisualEditor/modules/ve-mw/init/styles/images/loading-ltr.gif?2014-11-12T19:15:00Z)!ie}
/* cache key: enwiki:resourceloader:filter:minify-css:7:a562270fb658a4e1b39618719a620fdf */
.uls-menu a{cursor:pointer}.uls-menu.callout .caret-before{border-top:20px solid transparent;border-right:20px solid #C9C9C9;border-bottom:20px solid transparent;display:inline-block;left:-21px;top:30px;position:absolute}.uls-menu.callout .caret-after{border-top:20px solid transparent;border-right:20px solid #FCFCFC;border-bottom:20px solid transparent;display:inline-block;left:-20px;top:30px;position:absolute}.uls-ui-languages button{width:23%;text-overflow:ellipsis;margin-right:4%}button.uls-more-languages{width:auto}.settings-title{font-size:11pt}.settings-text{color:#555555;font-size:9pt}div.display-settings-block:hover .settings-text{color:#252525}
/* cache key: enwiki:resourceloader:filter:minify-css:7:22d1681fa868b4ff4fbcb1ec1e58a9ea */
.tipsy{padding:5px;position:absolute;z-index:100000;cursor:default}.tipsy-inner{padding:5px 8px 4px 8px; background-color:#ffffff;border:solid 1px #a7d7f9;color:black;max-width:15em;border-radius:4px; }.tipsy-arrow{position:absolute;background:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAALAgMAAADUwp+1AAAACVBMVEX5+fmn1/n///9s6BFKAAAAAXRSTlMAQObYZgAAACpJREFUCB1jZBD4wMiQMoeRcUU4I9uSaYxSE54xZjn8AtMgPkgcJA9UBwAeDw1Qrb3pVAAAAABJRU5ErkJggg==) no-repeat top left;background:url(//bits.wikimedia.org/static-1.25wmf8/resources/src/jquery.tipsy/images/tipsy.png?2014-11-12T19:10:00Z) no-repeat top left!ie;width:11px;height:6px} .tipsy-n .tipsy-arrow{top:0px;left:50%;margin-left:-5px} .tipsy-nw .tipsy-arrow{top:1px;left:10px} .tipsy-ne .tipsy-arrow{top:1px;right:10px} .tipsy-s .tipsy-arrow{bottom:0px;left:50%;margin-left:-5px;background-position:bottom left} .tipsy-sw .tipsy-arrow{bottom:0px;left:10px;background-position:bottom left} .tipsy-se .tipsy-arrow{bottom:0px;right:10px;background-position:bottom left} .tipsy-e .tipsy-arrow{top:50%;margin-top:-5px;right:1px;width:5px;height:11px;background-position:top right} .tipsy-w .tipsy-arrow{top:50%;margin-top:-5px;left:0px;width:6px;height:11px} .tipsy{font-size:0.8em}
/* cache key: enwiki:resourceloader:filter:minify-css:7:4a40c672dc9b9b3f595411bf8bf5c2cb */
@media print{#centralNotice{display:none}}
/* cache key: enwiki:resourceloader:filter:minify-css:7:ddb0c98a055632ae8e349c9cf48ac703 */</style><style>
.cite-accessibility-label{position:absolute !important; top:-99999px;clip:rect(1px 1px 1px 1px); clip:rect(1px,1px,1px,1px);padding:0 !important;border:0 !important;height:1px !important;width:1px !important;overflow:hidden}.mw-cite-backlink,.cite-accessibility-label{-moz-user-select:none;-webkit-user-select:none;-ms-user-select:none;user-select:none}
/* cache key: enwiki:resourceloader:filter:minify-css:7:a59b70abe6ecaa39388bb79ebc509cb0 */
.postedit-container{margin:0 auto;position:fixed;top:0;height:0;left:50%;z-index:1000;font-size:13px}.postedit-container:hover{cursor:pointer}.postedit{position:relative;top:0.6em;left:-50%;padding:.6em 3.6em .6em 1.1em;line-height:1.5625em;color:#626465;background-color:#f4f4f4;border:1px solid #dcd9d9;text-shadow:0 0.0625em 0 rgba(255,255,255,0.5);border-radius:5px;box-shadow:0 2px 5px 0 #ccc;-webkit-transition:all 0.25s ease-in-out;-moz-transition:all 0.25s ease-in-out;-ms-transition:all 0.25s ease-in-out;-o-transition:all 0.25s ease-in-out;transition:all 0.25s ease-in-out}.skin-monobook .postedit{top:6em !important}.postedit-faded{opacity:0}.postedit-icon{padding-left:41px;  line-height:25px;background-repeat:no-repeat;background-position:8px 50%}.postedit-icon-checkmark{background-image:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAMAAAAoLQ9TAAABblBMVEUAAAD///////9PfTf///80aRdTgjn///9Feij///////////9Rfzf///////////9PfjZRgDh1o1xOfTb///////+bwYqLtnj///////9PfTa82K////9WhT6YxIL///9QgDdTgzr////////j7uDl7eLq8efi693k7OH///////9UhjuBr2rp9uRUhjr///9YljVKgir///9WiTlYjT3////9/v57vFlbkT5PjC9dlD/5/fhuq09stUTs9uhxuElctCpfnT1huDFloEZloUZmpENmvDZpvDxpvTxqvjxrvT5rvT9rwTxsqktswD5uwkBvuUdxw0NztFBztU9ztVBzwkp0tlJ1xkd2t1R3uVR4w1F4xk54x014yE15uVZ5v1R5xVB6v1R7yFJ8wVh9xVl9yFR9yVd9ylN+xVh+yFd/x1l/yFeAylmEx1+Ny2uY0Hqe04Wj1Ymv3Ze33qLD47TJ5L3O6cPU7Mrq9eb2+/Q4j37OAAAAQHRSTlMAAQIEBAUFBQwPFB4fJCUoKiosQEhJS01RUlZZXmdydXaChYuSlJSWmJmoq6uur8LExcvM19fg5ejt8fX2+Pr7SljgewAAAKpJREFUGBkFwQNCAwAAAMDLtl3LtrG4rWXbtvX77gAgZ6grFwC0bhwNVgKgdPZx8b0dgLi+s7Wn0VoAqpfOI9+BNADZI7fLrz2pSEwGHZuH+78lSK8ZLkLezF3ooyUG3VPXq2USei9WngeyoG195yBYWDF3E/2pAhl1e9Gr8bGT+bfOFCC2fnvh4X7rcqIAQNNu+HT6sxkAjceTL/2ZAIhv+PorBwBJxfkA//dFHSCBy/UTAAAAAElFTkSuQmCC);background-image:url(//bits.wikimedia.org/static-1.25wmf8/resources/src/mediawiki.action/images/green-checkmark.png?2014-11-12T19:10:00Z)!ie;background-position:left}.postedit-close{position:absolute;padding:0 .8em;right:0;top:0;font-size:1.25em;font-weight:bold;line-height:2.3em;color:black;text-shadow:0 0.0625em 0 white;text-decoration:none;opacity:0.2;filter:alpha(opacity=20)}.postedit-close:hover{color:black;text-decoration:none;opacity:0.4;filter:alpha(opacity=40)}
/* cache key: enwiki:resourceloader:filter:minify-css:7:acf12498d8f5cdc12763a7b02264ecc6 */
.mw-collapsible-toggle{float:right;-moz-user-select:none;-webkit-user-select:none;-ms-user-select:none;user-select:none}.mw-customtoggle,.mw-collapsible-toggle{cursor:pointer} caption .mw-collapsible-toggle{float:none} li .mw-collapsible-toggle{float:none} .mw-collapsible-toggle-li{list-style:none}
/* cache key: enwiki:resourceloader:filter:minify-css:7:869aa9133c31e6040d4830b259da96a8 */
.suggestions{overflow:hidden;position:absolute;top:0;left:0;width:0;border:none;z-index:1099;padding:0;margin:-1px 0 0 0}.suggestions-special{position:relative;background-color:white;cursor:pointer;border:solid 1px #aaaaaa;padding:0;margin:0;margin-top:-2px;display:none;padding:0.25em 0.25em;line-height:1.25em}.suggestions-results{background-color:white;cursor:pointer;border:solid 1px #aaaaaa;padding:0;margin:0}.suggestions-result{color:black;margin:0;line-height:1.5em;padding:0.01em 0.25em;text-align:left; overflow:hidden;-o-text-overflow:ellipsis; text-overflow:ellipsis;white-space:nowrap}.suggestions-result-current{background-color:#4C59A6;color:white}.suggestions-special .special-label{color:gray;text-align:left}.suggestions-special .special-query{color:black;font-style:italic;text-align:left}.suggestions-special .special-hover{background-color:silver}.suggestions-result-current .special-label,.suggestions-result-current .special-query{color:white}.highlight{font-weight:bold}
/* cache key: enwiki:resourceloader:filter:minify-css:7:f8d0c6895ce3ae14434c16b8fca59432 */
.wp-teahouse-question-form{position:absolute;margin-left:auto;margin-right:auto;background-color:#f4f3f0;border:1px solid #a7d7f9;padding:1em}#wp-th-question-ask{float:right}.wp-teahouse-ask a.external{background-image:none !important}.wp-teahouse-respond-form{position:absolute;margin-left:auto;margin-right:auto;background-color:#f4f3f0;border:1px solid #a7d7f9;padding:1em}.wp-th-respond{float:right}.wp-teahouse-respond a.external{background-image:none !important}
/* cache key: enwiki:resourceloader:filter:minify-css:7:8614dfa7ef847fc9bfcc2d1c9f720197 */
.referencetooltip{position:absolute;list-style:none;list-style-image:none;opacity:0;font-size:10px;margin:0;z-index:5;padding:0}.referencetooltip li{border:#080086 2px solid;max-width:260px;padding:10px 8px 13px 8px;margin:0px;background-color:#F7F7F7;box-shadow:2px 4px 2px rgba(0,0,0,0.3);-moz-box-shadow:2px 4px 2px rgba(0,0,0,0.3);-webkit-box-shadow:2px 4px 2px rgba(0,0,0,0.3)}.referencetooltip li+li{margin-left:7px;margin-top:-2px;border:0;padding:0;height:3px;width:0px;background-color:transparent;box-shadow:none;-moz-box-shadow:none;-webkit-box-shadow:none;border-top:12px #080086 solid;border-right:7px transparent solid;border-left:7px transparent solid}.referencetooltip>li+li::after{content:'';border-top:8px #F7F7F7 solid;border-right:5px transparent solid;border-left:5px transparent solid;margin-top:-12px;margin-left:-5px;z-index:1;height:0px;width:0px;display:block}.client-js body .referencetooltip li li{border:none;box-shadow:none;-moz-box-shadow:none;-webkit-box-shadow:none;height:auto;width:auto;margin:auto;padding:0;position:static}.RTflipped{padding-top:13px}.referencetooltip.RTflipped li+li{position:absolute;top:2px;border-top:0;border-bottom:12px #080086 solid}.referencetooltip.RTflipped li+li::after{border-top:0;border-bottom:8px #F7F7F7 solid;position:absolute;margin-top:7px}.RTsettings{float:right;height:16px;width:16px;cursor:pointer;background-image:url(//upload.wikimedia.org/wikipedia/commons/e/ed/Cog.png);margin-top:-9px;margin-right:-7px;-webkit-transition:opacity 0.15s;-moz-transition:opacity 0.15s;-o-transition:opacity 0.15s;-ms-transition:opacity 0.15s;transition:opacity 0.15s;opacity:0.6;filter:alpha(opacity=60)}.RTsettings:hover{opacity:1;filter:alpha(opacity=100)}.RTTarget{border:#080086 2px solid}
/* cache key: enwiki:resourceloader:filter:minify-css:7:7efa21f6db726653194905048a6b006a */
.skin-vector li.GA,.skin-monobook li.GA,.skin-modern li.GA{list-style-image:url(//upload.wikimedia.org/wikipedia/commons/4/42/Monobook-bullet-ga.png)} .skin-vector li.FA,.skin-monobook li.FA{list-style-image:url(//upload.wikimedia.org/wikipedia/commons/d/d4/Monobook-bullet-star.png)}.skin-modern li.FA{list-style-image:url(//upload.wikimedia.org/wikipedia/commons/thumb/2/2c/Modern-bullet-star.svg/9px-Modern-bullet-star.svg.png)}
/* cache key: enwiki:resourceloader:filter:minify-css:7:22ca6c0fc9d0923a196f8a3aec796994 */
.mw-ui-icon{position:relative;min-height:1.4em;min-width:1.4em}.mw-ui-icon.mw-ui-icon-element{text-indent:-999px;overflow:hidden;width:3.4em;min-width:3.4em;max-width:3.4em}.mw-ui-icon.mw-ui-icon-element:before{left:0;right:0;position:absolute;margin:0 1em}.mw-ui-icon.mw-ui-icon-after:after,.mw-ui-icon.mw-ui-icon-before:before,.mw-ui-icon.mw-ui-icon-element:before{background-position:50% 50%;float:left;display:block;background-repeat:no-repeat;background-size:100% auto;min-height:1.4em;content:''}.mw-ui-icon.mw-ui-icon-before:before{position:relative;width:1.4em;margin-right:1em}.mw-ui-icon.mw-ui-icon-after:after{position:relative;float:right;width:1.4em;margin-left:1em}.mw-ui-icon-ok.mw-ui-icon:after,.mw-ui-icon-ok.mw-ui-icon:before{background-image:url(//bits.wikimedia.org/static-1.25wmf8/resources/src/mediawiki.ui/components/images/ok.png?2014-11-12T19:10:00Z);background-image:-webkit-linear-gradient(transparent,transparent),url(data:image/svg+xml,%3C%3Fxml%20version%3D%221.0%22%20encoding%3D%22utf-8%22%3F%3E%0A%3C%21--%20Generator%3A%20Adobe%20Illustrator%2015.0.0%2C%20SVG%20Export%20Plug-In%20.%20SVG%20Version%3A%206.00%20Build%200%29%20%20--%3E%0A%3C%21DOCTYPE%20svg%20PUBLIC%20%22-%2F%2FW3C%2F%2FDTD%20SVG%201.1%2F%2FEN%22%20%22http%3A%2F%2Fwww.w3.org%2FGraphics%2FSVG%2F1.1%2FDTD%2Fsvg11.dtd%22%3E%0A%3Csvg%20version%3D%221.1%22%20id%3D%22Layer_1%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%20xmlns%3Axlink%3D%22http%3A%2F%2Fwww.w3.org%2F1999%2Fxlink%22%20x%3D%220px%22%20y%3D%220px%22%0A%09%20width%3D%22142.282px%22%20height%3D%22142.28px%22%20viewBox%3D%220%20-11.785%20142.282%20142.28%22%20enable-background%3D%22new%200%20-11.785%20142.282%20142.28%22%0A%09%20xml%3Aspace%3D%22preserve%22%3E%0A%3Cg%3E%0A%0A%09%09%3Crect%20x%3D%2218.012%22%20y%3D%2241.792%22%20transform%3D%22matrix%280.6983%20-0.7158%200.7158%200.6983%20-17.1914%2077.8785%29%22%20fill%3D%22%23F0F0F0%22%20width%3D%22131.56%22%20height%3D%2235.083%22%2F%3E%0A%0A%09%09%3Crect%20x%3D%222.416%22%20y%3D%2264.455%22%20transform%3D%22matrix%280.7158%200.6983%20-0.6983%200.7158%2067.7777%20-2.5416%29%22%20fill%3D%22%23F0F0F0%22%20width%3D%2269.191%22%20height%3D%2235.082%22%2F%3E%0A%3C%2Fg%3E%0A%3C%2Fsvg%3E%0A);background-image:-webkit-linear-gradient(transparent,transparent),url(//bits.wikimedia.org/static-1.25wmf8/resources/src/mediawiki.ui/components/images/ok.svg?2014-11-12T19:10:00Z)!ie;background-image:linear-gradient(transparent,transparent),url(data:image/svg+xml,%3C%3Fxml%20version%3D%221.0%22%20encoding%3D%22utf-8%22%3F%3E%0A%3C%21--%20Generator%3A%20Adobe%20Illustrator%2015.0.0%2C%20SVG%20Export%20Plug-In%20.%20SVG%20Version%3A%206.00%20Build%200%29%20%20--%3E%0A%3C%21DOCTYPE%20svg%20PUBLIC%20%22-%2F%2FW3C%2F%2FDTD%20SVG%201.1%2F%2FEN%22%20%22http%3A%2F%2Fwww.w3.org%2FGraphics%2FSVG%2F1.1%2FDTD%2Fsvg11.dtd%22%3E%0A%3Csvg%20version%3D%221.1%22%20id%3D%22Layer_1%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%20xmlns%3Axlink%3D%22http%3A%2F%2Fwww.w3.org%2F1999%2Fxlink%22%20x%3D%220px%22%20y%3D%220px%22%0A%09%20width%3D%22142.282px%22%20height%3D%22142.28px%22%20viewBox%3D%220%20-11.785%20142.282%20142.28%22%20enable-background%3D%22new%200%20-11.785%20142.282%20142.28%22%0A%09%20xml%3Aspace%3D%22preserve%22%3E%0A%3Cg%3E%0A%0A%09%09%3Crect%20x%3D%2218.012%22%20y%3D%2241.792%22%20transform%3D%22matrix%280.6983%20-0.7158%200.7158%200.6983%20-17.1914%2077.8785%29%22%20fill%3D%22%23F0F0F0%22%20width%3D%22131.56%22%20height%3D%2235.083%22%2F%3E%0A%0A%09%09%3Crect%20x%3D%222.416%22%20y%3D%2264.455%22%20transform%3D%22matrix%280.7158%200.6983%20-0.6983%200.7158%2067.7777%20-2.5416%29%22%20fill%3D%22%23F0F0F0%22%20width%3D%2269.191%22%20height%3D%2235.082%22%2F%3E%0A%3C%2Fg%3E%0A%3C%2Fsvg%3E%0A);background-image:linear-gradient(transparent,transparent),url(//bits.wikimedia.org/static-1.25wmf8/resources/src/mediawiki.ui/components/images/ok.svg?2014-11-12T19:10:00Z)!ie}
/* cache key: enwiki:resourceloader:filter:minify-css:7:1570fe1fb6e757bdf4785ae8cb83f801 */
#p-lang .uls-settings-trigger{background:transparent url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAA4AAAAgCAMAAAAVMLmlAAAAA3NCSVQICAjb4U/gAAAACXBIWXMAAA3XAAAN1wFCKJt4AAAAGXRFWHRTb2Z0d2FyZQB3d3cuaW5rc2NhcGUub3Jnm+48GgAAALpQTFRF////W1tbgICAWVlZgICAUVFRgICAWVlZhYWFUlJShYWFVFRUgYGBVVVVgYGBVVVVgICAVFRUVVVVVlZWgYGBgICAVFRUgYGBVFRUgICAgYGBVVVVVVVVgYGBVlZWgYGBVFRUgICAVFRUgICAVVVVgICAVlZWgICAVFRUgICAVVVVgICAVFRUgICAVlZWgICAVVVVgICAVVVVgICAVVVVgICAVVVVgICAVVVVgICAVVVVgICAVVVVgICA9Fa9bgAAADx0Uk5TAA4OFBQWFhcXGRlJSUtLWlpbXV9fYGFhZGRlZmlpa2twcIiIioqPj5GRn5+goKGh6+vv7/Dw+fn8/P7+YuRUVgAAAOlJREFUGBmNwcdWAgEURMFLFCUaAcWAEhQYHBRmCN3//1u+BYcVC6s4p2U3OLrrVxN7Xn7uELoH5w65Dx1g6JNnoLazfz++VnZehkZir4pQ3NjzBg5TwtgBhzFh6kDr294UobiykxZc7ezVdLyx8yrw6pM+8GjnDjsf7gj3/WpiJ7Vhl6OW3eC/mlKdo5teZSHNSoM24XqvTCHTvg0MdDIASpn08/6ZSlkJ6jMpLUBhLc3qKEwIIwUURoSJAs2ltC5AIZUWTbjcSulktJayCvCikx7wJG0VttrfEp5eLpfS4uLtgaOmVOeMP8BAM9RClMO9AAAAAElFTkSuQmCC) no-repeat right top;background:transparent url(//bits.wikimedia.org/static-1.25wmf8/extensions/UniversalLanguageSelector/resources/css/../images/cog-sprite.png?2014-11-12T19:15:00Z) no-repeat right top!ie;background-image:-webkit-linear-gradient(transparent,transparent),url(data:image/svg+xml,%3C%3Fxml%20version%3D%221.0%22%20encoding%3D%22UTF-8%22%3F%3E%3Csvg%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%20xmlns%3Axlink%3D%22http%3A%2F%2Fwww.w3.org%2F1999%2Fxlink%22%20width%3D%2214%22%20height%3D%2232%22%3E%3Cdefs%3E%3Cpath%20d%3D%22M13.582%206.632h-1.064c-.133-.538-.354-1.065-.645-1.552l.75-.75c.164-.164.164-.429%200-.593l-1.359-1.36c-.164-.164-.43-.164-.594%200l-.75.75c-.49-.294-1.012-.512-1.551-.645v-1.064c0-.231-.187-.418-.419-.418h-1.918c-.231%200-.418.187-.418.418v1.063c-.541.135-1.062.352-1.552.646l-.75-.75c-.164-.164-.429-.164-.593%200l-1.36%201.36c-.164.164-.164.429%200%20.593l.75.75c-.292.488-.494%201.015-.627%201.551h-1.064c-.231%200-.418.187-.418.419v1.918c0%20.231.187.418.418.418h1.046c.134.542.351%201.062.645%201.551l-.75.75c-.164.164-.164.429%200%20.593l1.36%201.36c.164.164.429.164.593%200l.75-.75c.491.296%201.01.493%201.551.627v1.063c.001.233.188.42.419.42h1.918c.231%200%20.419-.187.419-.418v-1.063c.542-.134%201.061-.333%201.551-.627l.75.75c.164.164.43.164.594%200l1.359-1.36c.164-.164.164-.429%200-.593l-.75-.75c.295-.489.51-1.013.645-1.551h1.064c.23-.002.418-.189.418-.42v-1.918c0-.232-.188-.418-.418-.418zm-6.582%204.012c-1.461%200-2.645-1.184-2.645-2.644s1.184-2.644%202.645-2.644%202.645%201.184%202.645%202.644-1.184%202.644-2.645%202.644z%22%20id%3D%22a%22%2F%3E%3C%2Fdefs%3E%3Cuse%20xlink%3Ahref%3D%22%23a%22%20fill%3D%22%23808080%22%2F%3E%3Cuse%20xlink%3Ahref%3D%22%23a%22%20transform%3D%22translate%280%2016%29%22%20fill%3D%22%23555%22%2F%3E%3C%2Fsvg%3E);background-image:-webkit-linear-gradient(transparent,transparent),url(//bits.wikimedia.org/static-1.25wmf8/extensions/UniversalLanguageSelector/resources/css/../images/cog-sprite.svg?2014-11-12T19:15:00Z)!ie;background-image:linear-gradient(transparent,transparent),url(data:image/svg+xml,%3C%3Fxml%20version%3D%221.0%22%20encoding%3D%22UTF-8%22%3F%3E%3Csvg%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%20xmlns%3Axlink%3D%22http%3A%2F%2Fwww.w3.org%2F1999%2Fxlink%22%20width%3D%2214%22%20height%3D%2232%22%3E%3Cdefs%3E%3Cpath%20d%3D%22M13.582%206.632h-1.064c-.133-.538-.354-1.065-.645-1.552l.75-.75c.164-.164.164-.429%200-.593l-1.359-1.36c-.164-.164-.43-.164-.594%200l-.75.75c-.49-.294-1.012-.512-1.551-.645v-1.064c0-.231-.187-.418-.419-.418h-1.918c-.231%200-.418.187-.418.418v1.063c-.541.135-1.062.352-1.552.646l-.75-.75c-.164-.164-.429-.164-.593%200l-1.36%201.36c-.164.164-.164.429%200%20.593l.75.75c-.292.488-.494%201.015-.627%201.551h-1.064c-.231%200-.418.187-.418.419v1.918c0%20.231.187.418.418.418h1.046c.134.542.351%201.062.645%201.551l-.75.75c-.164.164-.164.429%200%20.593l1.36%201.36c.164.164.429.164.593%200l.75-.75c.491.296%201.01.493%201.551.627v1.063c.001.233.188.42.419.42h1.918c.231%200%20.419-.187.419-.418v-1.063c.542-.134%201.061-.333%201.551-.627l.75.75c.164.164.43.164.594%200l1.359-1.36c.164-.164.164-.429%200-.593l-.75-.75c.295-.489.51-1.013.645-1.551h1.064c.23-.002.418-.189.418-.42v-1.918c0-.232-.188-.418-.418-.418zm-6.582%204.012c-1.461%200-2.645-1.184-2.645-2.644s1.184-2.644%202.645-2.644%202.645%201.184%202.645%202.644-1.184%202.644-2.645%202.644z%22%20id%3D%22a%22%2F%3E%3C%2Fdefs%3E%3Cuse%20xlink%3Ahref%3D%22%23a%22%20fill%3D%22%23808080%22%2F%3E%3Cuse%20xlink%3Ahref%3D%22%23a%22%20transform%3D%22translate%280%2016%29%22%20fill%3D%22%23555%22%2F%3E%3C%2Fsvg%3E);background-image:linear-gradient(transparent,transparent),url(//bits.wikimedia.org/static-1.25wmf8/extensions/UniversalLanguageSelector/resources/css/../images/cog-sprite.svg?2014-11-12T19:15:00Z)!ie;height:16px;width:14px;float:right;cursor:pointer}.skin-vector #p-lang .uls-settings-trigger{ margin-top:3px}#p-lang .uls-settings-trigger:hover{background-position:right -16px}
/* cache key: enwiki:resourceloader:filter:minify-css:7:4d772cfe6d8171eec9d8f907cfb0a7b1 */</style><style>
.suggestions a.mw-searchSuggest-link,.suggestions a.mw-searchSuggest-link:hover,.suggestions a.mw-searchSuggest-link:active,.suggestions a.mw-searchSuggest-link:focus{color:black;text-decoration:none}.suggestions-result-current a.mw-searchSuggest-link,.suggestions-result-current a.mw-searchSuggest-link:hover,.suggestions-result-current a.mw-searchSuggest-link:active,.suggestions-result-current a.mw-searchSuggest-link:focus{color:white}.suggestions a.mw-searchSuggest-link .special-query{ overflow:hidden;-o-text-overflow:ellipsis; text-overflow:ellipsis;white-space:nowrap}
/* cache key: enwiki:resourceloader:filter:minify-css:7:ae3fa4570b5ac0c6cf7b3776c8ae4d6f */
.mw-mmv-overlay{position:fixed;top:0px;left:0px;right:0px;bottom:0px;z-index:1000;background-color:#000000}body.mw-mmv-lightbox-open{overflow-y:auto}body.mw-mmv-lightbox-open #mw-page-base,body.mw-mmv-lightbox-open #mw-head-base,body.mw-mmv-lightbox-open #mw-navigation,body.mw-mmv-lightbox-open #content,body.mw-mmv-lightbox-open #footer,body.mw-mmv-lightbox-open #globalWrapper // monobook{ display:none}body.mw-mmv-lightbox-open > *{ display:none}body.mw-mmv-lightbox-open > .mw-mmv-overlay,body.mw-mmv-lightbox-open > .mw-mmv-wrapper{display:block}.mw-mmv-filepage-buttons{margin-top:5px}.mw-mmv-filepage-buttons .mw-mmv-view-expanded,.mw-mmv-filepage-buttons .mw-mmv-view-config{display:block}.mw-mmv-filepage-buttons .mw-mmv-view-expanded.mw-ui-icon:before{background-image:url(data:image/svg+xml,%3C%3Fxml%20version%3D%221.0%22%20encoding%3D%22UTF-8%22%20standalone%3D%22no%22%3F%3E%0A%3C%21--%20Created%20with%20Inkscape%20%28http%3A%2F%2Fwww.inkscape.org%2F%29%20--%3E%0A%0A%3Csvg%0A%20%20%20xmlns%3Adc%3D%22http%3A%2F%2Fpurl.org%2Fdc%2Felements%2F1.1%2F%22%0A%20%20%20xmlns%3Acc%3D%22http%3A%2F%2Fcreativecommons.org%2Fns%23%22%0A%20%20%20xmlns%3Ardf%3D%22http%3A%2F%2Fwww.w3.org%2F1999%2F02%2F22-rdf-syntax-ns%23%22%0A%20%20%20xmlns%3Asvg%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%0A%20%20%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%0A%20%20%20version%3D%221.1%22%0A%20%20%20width%3D%22100%25%22%0A%20%20%20height%3D%22100%25%22%0A%20%20%20viewBox%3D%220%200%201024%20768%22%0A%20%20%20id%3D%22Layer_1%22%0A%20%20%20xml%3Aspace%3D%22preserve%22%3E%3Cmetadata%0A%20%20%20%20%20id%3D%22metadata17%22%3E%3Crdf%3ARDF%3E%3Ccc%3AWork%0A%20%20%20%20%20%20%20%20%20rdf%3Aabout%3D%22%22%3E%3Cdc%3Aformat%3Eimage%2Fsvg%2Bxml%3C%2Fdc%3Aformat%3E%3Cdc%3Atype%0A%20%20%20%20%20%20%20%20%20%20%20rdf%3Aresource%3D%22http%3A%2F%2Fpurl.org%2Fdc%2Fdcmitype%2FStillImage%22%20%2F%3E%3Cdc%3Atitle%3E%3C%2Fdc%3Atitle%3E%3C%2Fcc%3AWork%3E%3C%2Frdf%3ARDF%3E%3C%2Fmetadata%3E%3Cdefs%0A%20%20%20%20%20id%3D%22defs15%22%20%2F%3E%3Cg%0A%20%20%20%20%20id%3D%22g3%22%3E%3Cpolygon%0A%20%20%20%20%20%20%20points%3D%22851.2%2C71.6%20690.7%2C232.1%20650.6%2C191.8%20641%2C356.6%20805.8%2C347.3%20765.5%2C306.9%20926%2C146.4%20984.5%2C204.9%20997.6%2C0%20792.7%2C13.1%20%22%0A%20%20%20%20%20%20%20id%3D%22polygon5%22%0A%20%20%20%20%20%20%20style%3D%22fill%3A%23777777%22%20%2F%3E%3Cpolygon%0A%20%20%20%20%20%20%20points%3D%22769.6%2C89.3%20611.9%2C89.3%20682.8%2C160.1%20690.7%2C167.6%20%22%0A%20%20%20%20%20%20%20id%3D%22polygon7%22%0A%20%20%20%20%20%20%20style%3D%22fill%3A%23777777%22%20%2F%3E%3Cpath%0A%20%20%20%20%20%20%20d%3D%22m%20643.6%2C402.2%20-51.2%2C3%203%2C-51.2%209.4%2C-164.4%205.8%2C-100.3%20H%2026.4%20V%20768%20H%20909.5%20V%20387%20l%20-100.9%2C5.8%20-165%2C9.4%20z%20M%20813.9%2C678%20H%20113.6%20l%20207.2%2C-270.2%2031.5%2C-12.9%20195.7%2C204.9%20105.9%2C-63.2%20159.8%2C140.8%200.2%2C0.6%200%2C0%20z%22%0A%20%20%20%20%20%20%20id%3D%22path9%22%0A%20%20%20%20%20%20%20style%3D%22fill%3A%23777777%22%20%2F%3E%3Cpolygon%0A%20%20%20%20%20%20%20points%3D%22909.5%2C386.1%20909.5%2C228%20830.4%2C306.9%20838.2%2C314.8%20%22%0A%20%20%20%20%20%20%20id%3D%22polygon11%22%0A%20%20%20%20%20%20%20style%3D%22fill%3A%23777777%22%20%2F%3E%3C%2Fg%3E%3C%2Fsvg%3E);background-image:url(//bits.wikimedia.org/static-1.25wmf8/extensions/MultimediaViewer/resources/mmv/img/expand.svg?2014-11-12T19:13:20Z)!ie}.mw-mmv-filepage-buttons .mw-mmv-view-config.mw-ui-icon:before{background-image:url(data:image/svg+xml,%3C%3Fxml%20version%3D%221.0%22%20encoding%3D%22UTF-8%22%20standalone%3D%22no%22%3F%3E%0A%3C%21--%20Created%20with%20Inkscape%20%28http%3A%2F%2Fwww.inkscape.org%2F%29%20--%3E%0A%0A%3Csvg%0A%20%20%20xmlns%3Adc%3D%22http%3A%2F%2Fpurl.org%2Fdc%2Felements%2F1.1%2F%22%0A%20%20%20xmlns%3Acc%3D%22http%3A%2F%2Fcreativecommons.org%2Fns%23%22%0A%20%20%20xmlns%3Ardf%3D%22http%3A%2F%2Fwww.w3.org%2F1999%2F02%2F22-rdf-syntax-ns%23%22%0A%20%20%20xmlns%3Asvg%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%0A%20%20%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%0A%20%20%20version%3D%221.1%22%0A%20%20%20width%3D%22100%25%22%0A%20%20%20height%3D%22100%25%22%0A%20%20%20viewBox%3D%220%200%201024%20768%22%0A%20%20%20id%3D%22Layer_1%22%0A%20%20%20xml%3Aspace%3D%22preserve%22%3E%3Cmetadata%0A%20%20%20%20%20id%3D%22metadata9%22%3E%3Crdf%3ARDF%3E%3Ccc%3AWork%0A%20%20%20%20%20%20%20%20%20rdf%3Aabout%3D%22%22%3E%3Cdc%3Aformat%3Eimage%2Fsvg%2Bxml%3C%2Fdc%3Aformat%3E%3Cdc%3Atype%0A%20%20%20%20%20%20%20%20%20%20%20rdf%3Aresource%3D%22http%3A%2F%2Fpurl.org%2Fdc%2Fdcmitype%2FStillImage%22%20%2F%3E%3Cdc%3Atitle%3E%3C%2Fdc%3Atitle%3E%3C%2Fcc%3AWork%3E%3C%2Frdf%3ARDF%3E%3C%2Fmetadata%3E%3Cdefs%0A%20%20%20%20%20id%3D%22defs7%22%20%2F%3E%3Cpath%0A%20%20%20%20%20d%3D%22M%20897%2C454.6%20V%20313.4%20L%20810.4%2C299%20c%20-6.4%2C-23.3%20-16%2C-45.7%20-27.3%2C-65.8%20L%20833.6%2C161.8%20734.2%2C61.6%20662.8%2C112.1%20C%20641.9%2C100.9%20620.3%2C91.2%20597%2C84.8%20L%20582.6%2C-1%20H%20441.4%20L%20427%2C85.6%20c%20-23.3%2C6.4%20-45.7%2C16%20-65.8%2C27.3%20L%20289.8%2C62.4%20189.5%2C161.9%20240%2C233.3%20c%20-11.2%2C20.9%20-20.9%2C42.5%20-27.3%2C66.6%20L%20127%2C313.4%20v%20141.2%20l%2085.8%2C14.4%20c%206.4%2C23.3%2016%2C45.7%2027.3%2C66.6%20l%20-50.5%2C71.4%2099.5%2C99.5%2071.4%2C-50.5%20c%2020.9%2C11.2%2042.5%2C20.9%2066.6%2C27.3%20l%2014.4%2C85.8%20h%20141.2%20l%2014.4%2C-86.6%20c%2023.3%2C-6.4%2045.7%2C-16%2065.8%2C-27.3%20l%2071.4%2C50.5%2099.5%2C-99.5%20-50.5%2C-71.4%20c%2011.2%2C-20.9%2020.9%2C-42.5%2027.3%2C-66.6%20L%20897%2C454.6%20z%20m%20-385%2C77%20C%20430.2%2C531.6%20364.4%2C465%20364.4%2C384%20364.4%2C302.2%20431%2C236.4%20512%2C236.4%20c%2081%2C0%20147.6%2C65.8%20147.6%2C147.6%200%2C81.8%20-65.8%2C147.6%20-147.6%2C147.6%20z%22%0A%20%20%20%20%20id%3D%22path3%22%0A%20%20%20%20%20style%3D%22fill%3A%23777777%22%20%2F%3E%3C%2Fsvg%3E);background-image:url(//bits.wikimedia.org/static-1.25wmf8/extensions/MultimediaViewer/resources/mmv/img/gear_gray.svg?2014-11-12T19:13:20Z)!ie;opacity:0.75}.mw-mmv-filepage-buttons .mw-mmv-view-config.mw-ui-icon:before:hover{opacity:1}
/* cache key: enwiki:resourceloader:filter:minify-css:7:f3fa0fcfa2703e0d3a5dc0c40fbc0934 */</style><meta name="ResourceLoaderDynamicStyles" content="">
<link rel="stylesheet" href="http://bits.wikimedia.org/en.wikipedia.org/load.php?debug=false&lang=en&modules=site&only=styles&skin=vector&*">
<style>a:lang(ar),a:lang(kk-arab),a:lang(mzn),a:lang(ps),a:lang(ur){text-decoration:none}
/* cache key: enwiki:resourceloader:filter:minify-css:7:3904d24a08aa08f6a68dc338f9be277e */</style>
<script src="./Entropy (information theory) - Wikipedia, the free encyclopedia_files/load.php"></script><script src="./Entropy (information theory) - Wikipedia, the free encyclopedia_files/load(1).php"></script>
<script>if(window.mw){
mw.config.set({"wgCanonicalNamespace":"","wgCanonicalSpecialPageName":false,"wgNamespaceNumber":0,"wgPageName":"Entropy_(information_theory)","wgTitle":"Entropy (information theory)","wgCurRevisionId":633857614,"wgRevisionId":633857614,"wgArticleId":15445,"wgIsArticle":true,"wgIsRedirect":false,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":["Use dmy dates from July 2013","Articles needing additional references from April 2012","All articles needing additional references","All articles with unsourced statements","Articles with unsourced statements from April 2013","Articles with unsourced quotes","Wikipedia articles needing clarification from July 2014","Wikipedia articles incorporating text from PlanetMath","Entropy and information","Information theory","Statistical theory","Randomness"],"wgBreakFrames":false,"wgPageContentLanguage":"en","wgPageContentModel":"wikitext","wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgMonthNamesShort":["","Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"wgRelevantPageName":"Entropy_(information_theory)","wgIsProbablyEditable":true,"wgRestrictionEdit":[],"wgRestrictionMove":[],"wgWikiEditorEnabledModules":{"toolbar":true,"dialogs":true,"hidesig":true,"preview":false,"previewDialog":false,"publish":false},"wgBetaFeaturesFeatures":[],"wgMediaViewerOnClick":true,"wgMediaViewerEnabledByDefault":true,"wgVisualEditor":{"isPageWatched":false,"pageLanguageCode":"en","pageLanguageDir":"ltr","svgMaxSize":4096,"namespacesWithSubpages":{"6":0,"8":0,"1":true,"2":true,"3":true,"4":true,"5":true,"7":true,"9":true,"10":true,"11":true,"12":true,"13":true,"14":true,"15":true,"100":true,"101":true,"102":true,"103":true,"104":true,"105":true,"106":true,"107":true,"108":true,"109":true,"110":true,"111":true,"830":true,"831":true,"447":true,"2600":false,"828":true,"829":true}},"wikilove-recipient":"","wikilove-anon":0,"wgHHVMStart":1412726400000,"wgULSAcceptLanguageList":["pt-br","pt","en-us","en"],"wgULSCurrentAutonym":"English","wgFlaggedRevsParams":{"tags":{"status":{"levels":1,"quality":2,"pristine":3}}},"wgStableRevisionId":null,"wgCategoryTreePageCategoryOptions":"{\"mode\":0,\"hideprefix\":20,\"showcount\":true,\"namespaces\":false}","wgNoticeProject":"wikipedia","wgWikibaseItemId":"Q204570"});
}</script><script>if(window.mw){
mw.loader.implement("user.options",function($,jQuery){mw.user.options.set({"ccmeonemails":0,"cols":80,"date":"default","diffonly":0,"disablemail":0,"editfont":"default","editondblclick":0,"editsectiononrightclick":0,"enotifminoredits":0,"enotifrevealaddr":0,"enotifusertalkpages":1,"enotifwatchlistpages":0,"extendwatchlist":0,"fancysig":0,"forceeditsummary":0,"gender":"unknown","hideminor":0,"hidepatrolled":0,"imagesize":2,"math":0,"minordefault":0,"newpageshidepatrolled":0,"nickname":"","norollbackdiff":0,"numberheadings":0,"previewonfirst":0,"previewontop":1,"rcdays":7,"rclimit":50,"rows":25,"showhiddencats":false,"shownumberswatching":1,"showtoolbar":1,"skin":"vector","stubthreshold":0,"thumbsize":4,"underline":2,"uselivepreview":0,"usenewrc":0,"watchcreations":1,"watchdefault":0,"watchdeletion":0,"watchlistdays":3,"watchlisthideanons":0,"watchlisthidebots":0,"watchlisthideliu":0,"watchlisthideminor":0,"watchlisthideown":0,"watchlisthidepatrolled":0,"watchmoves":0,"watchrollback":0,
"wllimit":250,"useeditwarning":1,"prefershttps":1,"flaggedrevssimpleui":1,"flaggedrevsstable":0,"flaggedrevseditdiffs":true,"flaggedrevsviewdiffs":false,"usebetatoolbar":1,"usebetatoolbar-cgd":1,"visualeditor-enable":0,"visualeditor-betatempdisable":0,"visualeditor-enable-experimental":0,"visualeditor-enable-language":0,"visualeditor-hidebetawelcome":0,"wikilove-enabled":1,"echo-subscriptions-web-page-review":true,"echo-subscriptions-email-page-review":false,"ep_showtoplink":false,"ep_bulkdelorgs":false,"ep_bulkdelcourses":true,"ep_showdyk":true,"echo-subscriptions-web-education-program":true,"echo-subscriptions-email-education-program":false,"echo-notify-show-link":true,"echo-show-alert":true,"echo-email-frequency":0,"echo-email-format":"html","echo-subscriptions-email-system":true,"echo-subscriptions-web-system":true,"echo-subscriptions-email-user-rights":true,"echo-subscriptions-web-user-rights":true,"echo-subscriptions-email-other":false,"echo-subscriptions-web-other":true,
"echo-subscriptions-email-edit-user-talk":false,"echo-subscriptions-web-edit-user-talk":true,"echo-subscriptions-email-reverted":false,"echo-subscriptions-web-reverted":true,"echo-subscriptions-email-article-linked":false,"echo-subscriptions-web-article-linked":false,"echo-subscriptions-email-mention":false,"echo-subscriptions-web-mention":true,"echo-subscriptions-web-edit-thank":true,"echo-subscriptions-email-edit-thank":false,"echo-subscriptions-web-flow-discussion":true,"echo-subscriptions-email-flow-discussion":false,"gettingstarted-task-toolbar-show-intro":true,"uls-preferences":"","multimediaviewer-enable":true,"language":"en","variant-gan":"gan","variant-iu":"iu","variant-kk":"kk","variant-ku":"ku","variant-shi":"shi","variant-sr":"sr","variant-tg":"tg","variant-uz":"uz","variant-zh":"zh","searchNs0":true,"searchNs1":false,"searchNs2":false,"searchNs3":false,"searchNs4":false,"searchNs5":false,"searchNs6":false,"searchNs7":false,"searchNs8":false,"searchNs9":false,"searchNs10":
false,"searchNs11":false,"searchNs12":false,"searchNs13":false,"searchNs14":false,"searchNs15":false,"searchNs100":false,"searchNs101":false,"searchNs108":false,"searchNs109":false,"searchNs118":false,"searchNs119":false,"searchNs446":false,"searchNs447":false,"searchNs710":false,"searchNs711":false,"searchNs828":false,"searchNs829":false,"searchNs2600":false,"gadget-teahouse":1,"gadget-ReferenceTooltips":1,"gadget-geonotice":1,"gadget-DRN-wizard":1,"gadget-charinsert":1,"gadget-refToolbar":1,"gadget-mySandbox":1,"gadget-featured-articles-links":1,"variant":"en"});},{},{},{});mw.loader.implement("user.tokens",function($,jQuery){mw.user.tokens.set({"editToken":"+\\","patrolToken":"+\\","watchToken":"+\\"});},{},{},{});
/* cache key: enwiki:resourceloader:filter:minify-js:7:1d7d25c363537d562f59445bb7aee239 */
}</script>
<script>if(window.mw){
mw.loader.load(["mediawiki.page.startup","mediawiki.legacy.wikibits","mediawiki.legacy.ajax","ext.centralauth.centralautologin","mmv.head","ext.visualEditor.viewPageTarget.init","ext.uls.init","ext.uls.interface","ext.centralNotice.bannerController","skins.vector.js"]);
}</script>
<link rel="dns-prefetch" href="http://meta.wikimedia.org/">
<!--[if lt IE 7]><style type="text/css">body{behavior:url("/w/static-1.25wmf8/skins/Vector/csshover.min.htc")}</style><![endif]-->
<style class="winnerfm-style">.freefl-overlay{
    cursor: pointer;
    background-color: #3c3c3c;
    text-align: left;
    margin: 0px;
    padding: 0px;
    background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAGQAAABkCAIAAAD/gAIDAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA3FpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuNS1jMDE0IDc5LjE1MTQ4MSwgMjAxMy8wMy8xMy0xMjowOToxNSAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo5Mjc5OWJhMS1jNzIwLTE5NGEtYTc1Ny1jNWIxODYyNTE2ZTciIHhtcE1NOkRvY3VtZW50SUQ9InhtcC5kaWQ6MTEyQjEzQkZENTAyMTFFM0I2ODg5NjQ1MjUxOERCMjIiIHhtcE1NOkluc3RhbmNlSUQ9InhtcC5paWQ6MTEyQjEzQkVENTAyMTFFM0I2ODg5NjQ1MjUxOERCMjIiIHhtcDpDcmVhdG9yVG9vbD0iQWRvYmUgUGhvdG9zaG9wIENDIChXaW5kb3dzKSI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOjkyNzk5YmExLWM3MjAtMTk0YS1hNzU3LWM1YjE4NjI1MTZlNyIgc3RSZWY6ZG9jdW1lbnRJRD0ieG1wLmRpZDo5Mjc5OWJhMS1jNzIwLTE5NGEtYTc1Ny1jNWIxODYyNTE2ZTciLz4gPC9yZGY6RGVzY3JpcHRpb24+IDwvcmRmOlJERj4gPC94OnhtcG1ldGE+IDw/eHBhY2tldCBlbmQ9InIiPz5GhQdBAAAFFklEQVR42uyc3VKzQAyGt5FWbXX0wBnv/7r0Bjzx31Zb234vBCNdYMlXgULJHnSg0JB9Jhv27+3o/v5+u926YDk7Ozs5OQnfAyOLxaIuU5+fn5vNJnzb6elpFEW1mJpMJuPx2FUVMlJKUjBFRkpJar1ek5FSkoqboZFSkiqFZaTypIphGalCUgWwjFQZKR+WkQqQ2oFlpMKkfmEZqUpSKSwjpSEVwzJSSlIxLCOlJAVT0RGT0pjSkwJ0MlJKUqGxoZHKN2QyUvqUR0ZK/3KIhkYKpr6+vvYg5UeWkQp3OH6twxv4ZKQCXbOoaVK3t7fZq+/v7x8fH3uQms1mFxcX2asPDw9iHHd+f3+DRXOkUlhNxxROxUt+yt4xBSKenzjFPXAMlcTBarVqiFQMq4XWBy8fHx9raX0ITC98cA/yLAzCw9Fo1BypGFb7eQpuERG8h038iquBY1R1kxQ2xTfjs6xx8dWm89QOrBZIgQJXDA7hTkQBnGNY+MQPs49A3V5fX/HN5eWlNL2np6fU3Z+QpKTIl4VVrZcU/IxaiClU5vr6WtqREFwul8wOj+C8BqfhOo6n02nqX1LOz8/5V5Ljl0nhkGQPOcc3RwqmohZaHx6DvCuBILXlusFdPuXczK7jubj68vKCkBTQ7DSHlTwRB2wHn/JlE6QKevBN5CmuSfgGbpJZoAg9dBe8VwHuyb8NOcQajani4U4TpDRlnBQPn9exkohrLaN7Q8io6XefEhZymUcBp5LgcJwH1zIpt/cia41DEE5VXE88HU1PmiF3GmZJ8VJe+6T8yDrIuI8TPDoKeANy5uJQws9vbm7Yn8rHtUDK7bHIWvsIGZ/z+ZyHLPwGxFV0tXDMvdZVUg5OKg72u7u7vs8ltLbMEzlbZFVPn9oi639MNNsiq5ZUxbqhkcqSiidLjJSSlDvWRdYmSJWODY1UoSkyUnpTZKT0pshI6U2RkdKbIiOlN0VGSm+KjJTeFBkp176S9ehJtbTIepBJwdpJ1aNkHQgp93cl63BIuT8qWQdFyv1FyTo0Uq4Li6x9IeX2U7IOk9TO5N+QtyEroZOR0ocnGSl9QyYjpU95ZKRcjYusRsopF1mNlFMushopV7nIOkmKS3bj8TdZSRFvP3t7e+s1Kd6hKvtURX4VIOUCW7vzkiLZ1z+dTtdJ6W9MuSL5VZiUK1SyrlYr3JSXFLFeCV2Nq6srGCpUFfWl9SGmPPlVJakUFqrNHQhggiHRfpT57RKNw5HlKRCAJyAQdxF+hEFZ+VUMC9dktzqOWe/hMpIitEe2znol2Xrdd1Iiv3KJugotBuHGu6QL5VfgmOqpWFuDa3lJkSQ/0St5kqKexlRWfoU3WNpHJyqTX8WwWPr3/PyMrzxJkQiIJCPmJUX9bX04XSwW3m15+RVOOYx+EzwCyvMe1r0En5cU9TpPwc9sd6Fw2Ac/QUpeZZErkRR5k4It56n2t/eVwfLfhp6kyEMzqI2Q+VYpniDSye1Kijwd15BJyauM1bTxgScpkq7DwEmxG3Cbu13pIqtIinB5npRwMz4aUmhPgZgCE/QV4BVIifwqFTrlM/qQW58UZKdsjzIaICmlKY9UASwjFTBFRkpvioyU3hQZKb0pMlJ6U2Sk9KbISLn+LrJ2llTnFlm7TMp1apG146Rc4O+CjVR+CElGSj/Yjg5OqjtbRiunJchI6SdwyEjpp7rISOknBclIKUmlCX6blE6ROuymtbLyT4ABAI0sAxAZwQU9AAAAAElFTkSuQmCC);
    background-position: 0 0;
    background-repeat: repeat;
    z-index: 999999;
    position: relative;
}
</style></head>
<body class="mediawiki ltr sitedir-ltr ns-0 ns-subject page-Entropy_information_theory skin-vector action-view vector-animateLayout">
		<div id="mw-page-base" class="noprint"></div>
		<div id="mw-head-base" class="noprint"></div>
		<div id="content" class="mw-body" role="main">
			<a id="top"></a>

							<div id="siteNotice"><div id="centralNotice" class="cn-fundraising"><script>
  /* MediaWiki:CentralNotice/Resources/BannerShowHideCountDate.js
   * Determine if a banner should be hidden or displayed based on the
   * contents of the cookie named `hide-cookie-name` and that + `-wait`.
   *
   * Showing a banner entails that a certain number of impressions have
   * already occured in a time period.
   * 
   * hide-cookie-show-count - Number of impressions to show per period
   * hide-cookie-wait-count - Number of impressions to wait per period
   * hide-cookie-wait-delay - Number of second in period
   *
   * Provides CentralNotice alterImpressionData hook
   * Banner may be forced if URL parameter force = 1
   * Counters may be reset if URL parameter reset = 1
   */
  (function(mw) {
    var
      /** Total number of impressions seen by this user */
      cookieCount = parseInt($.cookie('centralnotice_bannercount_fr12')) || 0,

      waitData = ($.cookie('centralnotice_bannercount_fr12-wait') || '').split(/[|]/),
      /** This cycle's count of how many impressions we've waited for */
      waitCount = parseInt(waitData[0]) || 0,
      /** Timestamp (ms) until we can show another banner */
      waitUntil = parseInt(waitData[1]) || 0,
      /** Number of impressions seen this cycle */
      waitSeenCount = parseInt(waitData[2]) || 0,

      showLimit = parseInt('1') || 0,
      waitLimit = parseInt('5') || 0,
      waitDelay = parseInt('2592000') || 0,

      hideBanner = true,
      hideReason = null;

    if (location.search.match(/\breset=1/)) {
      // Reset counters on demand
      cookieCount = 0;
      waitCount = 0;
      waitUntil = 0;
      waitSeenCount = 0;
    }

    var
      pastDate = waitUntil < new Date().getTime(),
      waitForHideImps = waitCount < waitLimit,
      waitForShowImps = waitSeenCount < showLimit;
 
    if (location.search.match(/\bforce=1/)) {
      hideBanner = false;
    } else if (!pastDate) {
      hideReason = 'waitdate';
      waitCount += 1;
    } else if (pastDate && waitForHideImps) {
      hideReason = 'waitimps';
      waitCount += 1;
    } else if (pastDate && !waitForHideImps && waitForShowImps) {
      hideBanner = false;
      waitSeenCount += 1;
      cookieCount += 1;
      
      if (waitSeenCount >= showLimit) {
        waitCount = 0;
        waitSeenCount = 0;
        waitUntil = new Date().getTime() + (waitDelay * 1000);
      }
    } else {
      hideReason = 'waiterr';
      waitCount = 0;
      waitSeenCount = 0;
      waitUntil = new Date().getTime() + (waitDelay * 1000);
    }
    waitData = waitCount + '|' + waitUntil + '|' + waitSeenCount;
 
    // Finish up and store results
    $.cookie('centralnotice_bannercount_fr12', cookieCount, { expires: 365, path: '/' });
    $.cookie('centralnotice_bannercount_fr12-wait', waitData, { expires: 365, path: '/' });
    mw.centralNotice.bannerData.hideResult = hideBanner;
    mw.centralNotice.bannerData.hideReason = hideReason;
    mw.centralNotice.bannerData.cookieCount = cookieCount;
  })(mediaWiki);
 
  mediaWiki.centralNotice.bannerData.alterImpressionData = function( impressionData ) {
    // Returning true from this function indicates the banner was shown
    if (mediaWiki.centralNotice.bannerData.hideReason) {
      impressionData.reason = mediaWiki.centralNotice.bannerData.hideReason;
    }
    return !mediaWiki.centralNotice.bannerData.hideResult;
  };
</script>

<style>
  #centralNotice.collapsed #B14_1122_enUS_dsk_hlcf_mr{
    display: none;
  }
  div#B14_1122_enUS_dsk_hlcf_mr {
    display: none;
    position: absolute;
    top: 0;
    left: 0;
    right: 0;
    z-index: 9999;
    background: transparent;
    font-family: Arial, Helvetica, Verdana, sans-serif;
    cursor: pointer;
  }

  div#B14_1122_enUS_dsk_hlcf_mr #B14_1122_enUS_dsk_hlcf_mr-main {
    background: #cedff1;
    padding: 0;
    width: 100%;
    border: none;
    border-spacing: 0; /* because border-collapse hides shadow in IE */
  }
  body.rtl div#B14_1122_enUS_dsk_hlcf_mr #B14_1122_enUS_dsk_hlcf_mr-main {
    box-shadow: -5px 5px 5px #aaa;
  }

  div#B14_1122_enUS_dsk_hlcf_mr #B14_1122_enUS_dsk_hlcf_mr-facts p {
    margin: 0;
    padding: 10px 10px 10px 40px;
    background: url(//upload.wikimedia.org/wikipedia/donate/thumb/9/96/I.svg/20px-I.svg.png) no-repeat 10px 8px;
    color: #000;
    font-size: 17px;
    line-height: 1.2;
    font-weight: bold;
    text-align: justify;
  }
  body.rtl div#B14_1122_enUS_dsk_hlcf_mr #B14_1122_enUS_dsk_hlcf_mr-facts p {
    padding-left: 10px;
    padding-right: 40px;
    background-position: top 10px right 8px;
  }
  @media (max-width: 1024px) {
    div#B14_1122_enUS_dsk_hlcf_mr #B14_1122_enUS_dsk_hlcf_mr-facts p {
      font-size: 15px;
    }
  }

  .B14_1122_enUS_dsk_hlcf_mr-highlight {
    background: #fff64c;
    color: black;
    padding: 1px 0;
  }

  #B14_1122_enUS_dsk_hlcf_mr-form-wrapper {
    width: 330px;
    background: #f5faff;
    padding: 0;
    border-left: 1px solid #a8bedf;
  }
  #B14_1122_enUS_dsk_hlcf_mr-form {
    height: 100%;
    font-size: 14px;
    font-weight: bold;
    padding: 4px 8px;
  }
  #B14_1122_enUS_dsk_hlcf_mr-form table {
    border: none;
    border-collapse: collapse;
    width: 100%;
    height: 100%;
  }
  #B14_1122_enUS_dsk_hlcf_mr-form td {
    min-width: 55px;
    white-space: nowrap;
    margin: 0;
    vertical-align: middle;
    padding: 2px;
  }
  #B14_1122_enUS_dsk_hlcf_mr-form span label {
    margin-left: 2px;
  }
  #B14_1122_enUS_dsk_hlcf_mr-form .amount-options {
    height: 20%;
    color: #091d41;
  }
  #B14_1122_enUS_dsk_hlcf_mr-form .paymentmethod-options td {
    text-align: center;
    padding: 12px 4px 8px 4px ;
  }
  #B14_1122_enUS_dsk_hlcf_mr-form button,
  #B14_1122_enUS_dsk_hlcf_mr-nag button {
    display: inline-block;
    min-width: 110px;
    min-height: 26px;
    margin: 0 1px;
    border: 1px solid #626f90;
    border-radius: 5px;
    background: #ffffff;
    background: -moz-linear-gradient(top, #ffffff 0%, #e5e5e5 100%);
    background: -webkit-gradient(linear, left top, left bottom, color-stop(0%,#ffffff), color-stop(100%,#e5e5e5));
    background: -webkit-linear-gradient(top, #ffffff 0%,#e5e5e5 100%);
    background: -o-linear-gradient(top, #ffffff 0%,#e5e5e5 100%);
    background: -ms-linear-gradient(top, #ffffff 0%,#e5e5e5 100%);
    background: linear-gradient(to bottom, #ffffff 0%,rgb(240, 240, 240) 100%);
    box-shadow: 0 1px 1px #eee;
    color: black;
    font-size: 12px;
    font-weight: bold;
    cursor: pointer;
  }
  #B14_1122_enUS_dsk_hlcf_mr-form button:hover,
  #B14_1122_enUS_dsk_hlcf_mr-nag button:hover {
    color: #0645ad;
  }

  div#B14_1122_enUS_dsk_hlcf_mr span#B14_1122_enUS_dsk_hlcf_mr-close {
    display: block;
    position: absolute;
    right: 0px;
    top: 0px;
    width: 21px;
    height: 21px;
    background: url(//upload.wikimedia.org/wikipedia/donate/2/27/Small-white-close.png) no-repeat center center black;
    cursor: pointer;
  }
  body.rtl div#B14_1122_enUS_dsk_hlcf_mr span#B14_1122_enUS_dsk_hlcf_mr-close {
    right: auto;
    left: 0;
  }

  div#B14_1122_enUS_dsk_hlcf_mr p#B14_1122_enUS_dsk_hlcf_mr-legal {
    display: none;
    position: relative;
    margin: 0;
    padding: 10px 40px 5px 40px;
    background: #cedff1;
    font-size: 11px;
    line-height: 1;
    font-weight: normal;
  }

  #B14_1122_enUS_dsk_hlcf_mr-nag {
    display: none;
    position: fixed;
    top: 0;
    left: 0;
    background-color: #ffcc00;
    padding: 6px;
    width: 100%;
    text-align: center;
    font-size: 17px;
    font-weight: bold;
  }
  .B14_1122_enUS_dsk_hlcf_mr-nag-link {
    vertical-align: middle;
  }
  .B14_1122_enUS_dsk_hlcf_mr-nag-link:hover {
    text-decoration: underline;
  }
  .B14_1122_enUS_dsk_hlcf_mr-nag-close {
    position: absolute;
    top: 0px;
    right: 11px;
    z-index: 100;
    background: url(//upload.wikimedia.org/wikipedia/donate/2/27/Small-white-close.png) no-repeat center center black;
    width: 21px;
    height: 21px;
    display: block;
  }
</style>

<div id="B14_1122_enUS_dsk_hlcf_mr">
  <table id="B14_1122_enUS_dsk_hlcf_mr-main" style="height: 117px;">
    <tbody><tr>
      <td id="B14_1122_enUS_dsk_hlcf_mr-facts">
        <p>
          <i>DEAR WIKIPEDIA READERS:</i> Advertising is not evil. But it doesn't belong here. We survive on donations averaging about <span style="white-space: nowrap;">$15</span>. <span class="B14_1122_enUS_dsk_hlcf_mr-highlight"> Now is the time we ask. If everyone reading this right now gave <span style="white-space: nowrap;">$3</span>, our fundraiser would be done within an hour. Yep, that’s about the price of buying a programmer a coffee.</span> We’re a small non-profit with costs of a top 5 website: servers, staff and programs. Wikipedia is something special. It is like a library or a public park where we can all go to think and learn. If Wikipedia is useful to you, take one minute to make a tax-deductible donation to keep it online and ad-free another year. <i>Thank you.</i>
        </p>
      </td>

      <td id="B14_1122_enUS_dsk_hlcf_mr-form-wrapper">
<style>
/* hide methods which aren't monthly capable when monthly option is selected */
.form-monthly button.no-monthly {
  display: none !important;
}
</style>
<form id="B14_1122_enUS_dsk_hlcf_mr-form" method="post" name="paypalcontribution">
  <input type="hidden" name="utm_medium" value="sitenotice">
  <input type="hidden" name="utm_campaign" value="C14_enUS_dsk_lw_FR">
  <input type="hidden" name="utm_source" value="B14_1122_enUS_dsk_hlcf_mr">
  <input type="hidden" name="language" value="en">
  <input type="hidden" name="country" value="US">
  <input type="hidden" name="currency_code" id="input_currency_code" value="USD">
  <input type="hidden" name="payment_method" value="">
  <input type="hidden" name="referrer" value="">
  <input type="hidden" name="returnto" value="">
  <table>
    <tbody><tr class="frequency-options">
      <td colspan="2"><input type="radio" name="frequency" id="frequency_onetime" onclick="toggleMonthly(false);" value="onetime" checked="checked"><label for="frequency_onetime">One-time</label></td>
      <td colspan="2"><input type="radio" name="frequency" id="frequency_monthly" onclick="toggleMonthly(true);" value="monthly"><label for="frequency_monthly">Monthly*</label></td>
    </tr>
    <tr class="amount-options">
      <td>
        <input type="radio" name="amount" id="input_amount_0" onclick="document.paypalcontribution.amountGiven.value=&#39;&#39;" value="3">
        <label for="input_amount_0">$3</label>
      </td>
      <td>
        <input type="radio" name="amount" id="input_amount_1" onclick="document.paypalcontribution.amountGiven.value=&#39;&#39;" value="5">
        <label for="input_amount_1">$5</label>
      </td>
      <td>
        <input type="radio" name="amount" id="input_amount_2" onclick="document.paypalcontribution.amountGiven.value=&#39;&#39;" value="10">
        <label for="input_amount_2">$10</label>
      </td>
      <td>
        <input type="radio" name="amount" id="input_amount_3" onclick="document.paypalcontribution.amountGiven.value=&#39;&#39;" value="20">
        <label for="input_amount_3">$20</label>
      </td>
    </tr>
    <tr class="amount-options">
      <td>
        <input type="radio" name="amount" id="input_amount_4" onclick="document.paypalcontribution.amountGiven.value=&#39;&#39;" value="30">
        <label for="input_amount_4">$30</label>
      </td>
      <td>
        <input type="radio" name="amount" id="input_amount_5" onclick="document.paypalcontribution.amountGiven.value=&#39;&#39;" value="50">
        <label for="input_amount_5">$50</label>
      </td>
      <td>
        <input type="radio" name="amount" id="input_amount_6" onclick="document.paypalcontribution.amountGiven.value=&#39;&#39;" value="100">
        <label for="input_amount_6">$100</label>
      </td>
      <td>
        <input type="radio" name="amount" id="input_amount_other" value="0">
        <span id="input_label_other" onclick="$(&#39;#input_amount_other&#39;).prop(&#39;checked&#39;, true);">$
          <input style="height: 1em;" type="text" name="amountGiven" size="3" autocomplete="off" onfocus="$(&#39;#input_amount_other&#39;).prop(&#39;checked&#39;, true);">
        </span>
      </td>
    </tr>
    <tr class="paymentmethod-options">
      <td colspan="4">
        
        
        <button class="paymentmethod-cc" onclick="redirectPayment(&#39;cc&#39;); return false;">
          Credit Card</button>
        
        <button class="paymentmethod-pp" onclick="redirectPayment(&#39;paypal&#39;); return false;">
          PayPal</button>
        <button class="paymentmethod-amazon no-monthly" onclick="redirectPayment(&#39;amazon&#39;); return false;" style="">
          Amazon</button>
        
        <button class="paymentmethod-boletos no-monthly" onclick="redirectPayment(&#39;cash&#39;, &#39;boleto&#39;); return false;" style="display:none;">
          Boleto</button>
        
        <button class="paymentmethod-pp-usd" onclick="redirectPayment(&#39;paypal&#39;, undefined, true); return false;" style="display: none;">
          PayPal (USD)</button>
      </td>
    </tr>
  </tbody></table>
<input type="hidden" name="utm_key" value="2"></form>
        <span id="B14_1122_enUS_dsk_hlcf_mr-close" onclick="fundraisingBanner.hide(); return false;">&nbsp;</span>
      </td>

    </tr>
  </tbody></table>

  <p id="B14_1122_enUS_dsk_hlcf_mr-legal">
    
<a href="https://wikimediafoundation.org/wiki/Special:LandingCheck?basic=true&landing_page=Problems_donating&country=US&language=en&uselang=en" target="_blank">Problems donating?</a> |
<a href="https://wikimediafoundation.org/wiki/Special:LandingCheck?basic=true&landing_page=Ways_to_Give&country=US&language=en&uselang=en" target="_blank">Other ways to give</a> |
<a href="https://wikimediafoundation.org/wiki/Special:LandingCheck?landing_page=FAQ&basic=true&country=US&language=en&uselang=en" target="_blank">Frequently asked questions</a> |
<span class="informationsharing-US" style="">By donating, you are agreeing to our <a href="https://wikimediafoundation.org/wiki/Special:LandingCheck?basic=true&landing_page=Donor_policy&country=US&language=en&uselang=en" target="_blank">donor privacy policy</a>. The Wikimedia Foundation is a nonprofit, <a href="https://wikimediafoundation.org/wiki/Special:LandingCheck?basic=true&landing_page=Tax_Deductibility&country=US&language=en&uselang=en" target="_blank">tax-exempt organization</a>.</span>
<span class="informationsharing-NL" style="display: none;">By donating, you are agreeing to our <a href="https://wikimediafoundation.org/wiki/Special:LandingCheck?basic=true&landing_page=Donor_policy&country=US&language=en&uselang=en" target="_blank">donor privacy policy</a> and to sharing your information with the Wikimedia Foundation and its service providers in the U.S. and elsewhere. The Wikimedia Foundation is a nonprofit, <a href="https://wikimediafoundation.org/wiki/Special:LandingCheck?basic=true&landing_page=Tax_Deductibility&country=US&language=en&uselang=en" target="_blank">tax-exempt organization</a>.</span>
<span class="informationsharing-nonUS" style="display: none;">By donating, you are agreeing to our <a href="https://wikimediafoundation.org/wiki/Special:LandingCheck?basic=true&landing_page=Donor_policy&country=US&language=en&uselang=en" target="_blank">donor privacy policy</a> and to sharing your information with the <a href="https://wikimediafoundation.org/wiki/Special:LandingCheck?basic=true&landing_page=Tax_Deductibility&country=US&language=en&uselang=en" target="_blank">Wikimedia Foundation</a> and its service providers in the U.S. and elsewhere.</span>
<span class="recurring-details">*Recurring payments will be debited by the Wikimedia Foundation until you notify us to stop. We'll send you an email receipt for each payment, which will include a link to <a href="https://wikimediafoundation.org/wiki/Special:LandingCheck?basic=true&landing_page=Cancel_or_change_recurring_payments&country=US&language=en&uselang=en" target="_blank">easy cancellation instructions.</a></span>
  </p>

  <div id="B14_1122_enUS_dsk_hlcf_mr-nag" style="display: block;">
    <img src="./Entropy (information theory) - Wikipedia, the free encyclopedia_files/20px-I-blue.svg.png">
    <span class="B14_1122_enUS_dsk_hlcf_mr-nag-link">If we all gave <span style="white-space: nowrap;">$3</span>, the fundraiser would be over in an hour.</span>
    <button class="B14_1122_enUS_dsk_hlcf_mr-nag-donate">Please Donate Now</button>
    <span class="B14_1122_enUS_dsk_hlcf_mr-nag-close"></span>
  </div>
</div>

<script>
var fundraisingBanner = {};

fundraisingBanner.addSpace = function() {
  /* add space for the banner, and extra px if given below. called on load and window resize */
  var extra = 0;
  var bannerHeight = $('div#B14_1122_enUS_dsk_hlcf_mr').height();
  $('#mw-panel').css('top', bannerHeight+160+extra);
  $('#mw-head').css('top', bannerHeight+extra);
  $('#mw-page-base').css('padding-top', bannerHeight+extra);
};

fundraisingBanner.show = function() {
  $('body').prepend($('#centralNotice'));
  $('#B14_1122_enUS_dsk_hlcf_mr').css('display', 'block');
  fundraisingBanner.addSpace();

  $(window).resize(function() {
    fundraisingBanner.addSpace();
  });
}

fundraisingBanner.hide = function() {
  /* hide the banner, and remove the extra space which was added for it */
  mw.centralNotice.hideBanner();
  $('#B14_1122_enUS_dsk_hlcf_mr').hide();
  $('#mw-panel').css('top', '160px');
  $('#mw-head').css('top', '0px');
  $('#mw-page-base').css('padding-top', '0');
  return false;
};

fundraisingBanner.showLegal = function() {
  $('#B14_1122_enUS_dsk_hlcf_mr-legal').show();
}
fundraisingBanner.hideLegal = function() {
  $('#B14_1122_enUS_dsk_hlcf_mr-legal').hide();
}
</script>

<script>
/**
 * NOTE: The following currency mapping is WMF-specific based on payment
 * provider availability, NOT necessarily the official currency of the country
 */
function getCurrency() {
    switch(Geo.country) {
        // Big 5 at the top for speed
        case "US": return "USD";
        case "CA": return "CAD";
        case "GB": return "GBP";
        case "AU": return "AUD";
        case "NZ": return "NZD";
        // Euro countries
        case "AD":
        case "AL":
        case "AM":
        case "AT":
        case "AW":
        case "AZ":
        case "BE":
        case "BY":
        case "CI":
        case "CY":
        case "DE":
        case "EE":
        case "ES":
        case "FI":
        case "FR":
        case "GF":
        case "GR":
        case "IE":
        case "IT":
        case "LU":
        case "LV":
        case "LY":
        case "MC":
        case "ME":
        case "MG":
        case "MT":
        case "NL":
        case "PT":
        case "RE":
        case "RS":
        case "SI":
        case "SK":
        case "SM":
        case "SR":
        case "VA":
            return "EUR";
        // The rest
        case "AE": return "AED";
        case "AR": return "ARS";
        case "BA": return "BAM";
        case "BB": return "BBD";
        case "BD": return "BDT";
        case "BG": return "BGN";
        case "BH": return "BHD";
        case "BM": return "BMD";
        case "BO": return "BOB";
        case "BR": return "USD";
        case "BZ": return "BZD";
        case "CH": return "CHF";
        case "CK": return "NZD";
        case "CL": return "CLP";
        case "CN": return "CNY";
        case "CO": return "COP";
        case "CR": return "CRC";
        case "CZ": return "CZK";
        case "DK": return "DKK";
        case "DO": return "DOP";
        case "DZ": return "DZD";
        case "EG": return "EGP";
        case "FJ": return "FJD";
        case "FO": return "DKK";
        case "GL": return "DKK";
        case "GT": return "GTQ";
        case "HK": return "HKD";
        case "HN": return "HNL";
        case "HR": return "HRK";
        case "HU": return "HUF";
        case "ID": return "IDR";
        case "IL": return "ILS";
        case "IN": return "INR";
        case "IS": return "ISK";
        case "JM": return "JMD";
        case "JO": return "JOD";
        case "JP": return "JPY";
        case "KE": return "KES";
        case "KI": return "AUD";
        case "KR": return "KRW";
        case "KZ": return "KZT";
        case "LB": return "LBP";
        case "LI": return "CHF";
        case "LK": return "LKR";
        case "LT": return "LTL";
        case "MA": return "MAD";
        case "MK": return "MKD";
        case "MV": return "MVR";
        case "MW": return "GBP";
        case "MX": return "MXN";
        case "MY": return "MYR";
        case "NI": return "NIO";
        case "NO": return "NOK";
        case "NP": return "INR";
        case "NR": return "AUD";
        case "OM": return "OMR";
        case "PA": return "PAB";
        case "PE": return "PEN";
        case "PG": return "AUD";
        case "PH": return "PHP";
        case "PK": return "PKR";
        case "PL": return "PLN";
        case "PY": return "PYG";
        case "QA": return "QAR";
        case "RO": return "RON";
        case "RU": return "RUB";
        case "SA": return "SAR";
        case "SD": return "GBP";
        case "SE": return "SEK";
        case "SG": return "SGD";
        case "TH": return "THB";
        case "TM": return "RUB";
        case "TN": return "TND";
        case "TR": return "TRY";
        case "TT": return "TTD";
        case "TW": return "TWD";
        case "UA": return "UAH";
        case "UY": return "UYU";
        case "UZ": return "RUB";
        case "VE": return "VEF";
        case "VN": return "VND";
        case "VU": return "AUD";
        case "ZA": return "ZAR";
        // small multi-country currencies
        case "CW":
        case "SX":
            return "ANG";
        case "AG":
        case "DM":
        case "GD":
        case "KN":
        case "LC":
            return "XCD";
        case "BJ":
        case "BF":
        case "CI":
        case "GW":
        case "ML":
        case "NE":
        case "SN":
        case "TG":
            return "XOF";
        case "PF":
        case "NC":
        case "WF":
            return "XPF";
        // fall back to USD
        default:
            return "USD";
    }
}
</script>
<script>
function currencyLocalize(currency, amount, language){
    // Do some basic number formatting - digit separators etc
    if (window.Intl && typeof window.Intl === "object") {
        var locale = language + '-' + Geo.country;
        var formatter = new Intl.NumberFormat(locale);
    } else {
        // boo, bad browser! let's just have a thing that throws it back unformatted for now
        var formatter = {};
        formatter.format = function(number) {
            return number.toString();
        };
    }
    if (isNaN(amount) || amount == "") {
        // it's probably the "other" string or box
        var fmAmount = amount;
    } else {
        var fmAmount = formatter.format(amount);
    }
    // End number formatting

    var currencies = {
        "USD" : "$\t",
        "EUR" : {
            "en" : "€\t",
            "cy" : "€\t",
            "ga" : "€\t",
            "mt" : "€\t",
            "nl" : "€ \t",
            "lv" : "€ \t",
            "tr" : "€ \t",
            "default" : "\t €"
        },
        "AED" : " د.إ \t",
        "ANG" : "ƒ\t",
        "ARS" : "$\t",
        "AUD" : "$\t",
        "BAM" : "\t KM",
        "BBD" : "Bcs$\t",
        "BDT" : "৳\t",
        "BGN" : "лв\t",
        "BHD" : "د.ب\t",
        "BMD" : "BD$\t",
        "BOB" : "$b\t",
        "BRL" : "R$\t",
        "BZD" : "BZ$\t",
        "CAD" : {
            "fr" : "$\t",
            "default" : "$\t"
        },
        "CHF" : "Fr. \t",
        "CLP" : "$\t",
        "CNY" : "\t ¥",
        "COP" : "$\t",
        "CRC" : "\t ₡",
        "CZK" : "\t Kč",
        "DKK" : "\t kr.",
        "DOP" : "RD$\t",
        "DZD" : "د.ج\t",
        "EEK" : "\t kr",
        "EGP" : {
            "en" : "E£\t",
            "default" : "\t جنيه"
        },
        "FJD" : "FJ$\t",
        "GBP" : "£\t",
        "GTQ" : "Q\t",
        "HKD" : "$\t",
        "HNL" : "L\t",
        "HRK" : "\t kn",
        "HUF" : "\t Ft",
        "IDR" : "Rp \t",
        "ILS" : {
            "he" : "\t ₪",
            "yi" : "\t ₪",
            "ar" : "\t ₪",
            "default" : "₪ \t"
        },
        "INR" : "Rs. \t",
        "ISK" : "\t kr",
        "JMD" : "J$ \t",
        "JOD" : "دينار\t",
        "JPY" : "¥\t",
        "KES" : "\t KSh",
        "KRW" : "₩\t",
        "KWD" : "د.ك \t",
        "KZT" : "〒 \t", // TODO: don't use the JP postal code symbol once KZT works in unicode
        "LBP" : "LBP \t",
        "LKR" : "\t Rs.",
        "LTL" : "\t Lt",
        "LVL" : "\t Ls",
        "MAD" : "د.م.\t",
        "MKD" : "\t ден",
        "MOP" : "MOP$\t",
        "MUR" : "\t Rs",
        "MXN" : "$\t",
        "MVR" : "Rf. \t",
        "MYR" : "RM\t",
        "NIO" : "C$\t",
        "NOK" : "\t kr",
        "NZD" : "$\t",
        "OMR" : "ر.ع\t",
        "PAB" : "\t B/.",
        "PEN" : "\t S/.",
        "PHP" : "₱\t",
        "PKR" : "Rs \t",
        "PLN" : "\t zł",
        "PYG" : "\t ₲",
        "QAR" : "ر.ق\t",
        "RON" : "\t lei",
        "RUB" : "\t руб",
        "SAR" : "﷼\t",
        "SCR" : "SR \t",
        "SEK" : "\t kr",
        "SGD" : "S$ \t",
        "SVC" : "\t ₡",
        "THB" : {
            "th" : "\t บาท",
            "default" : "\t ฿"
        },
        "TND" : "\t د.ت",
        "TRY" : "₺\t",
        "TTD" : "TT$\t",
        "TWD" : "NT$\t",
        "TZS" : "\t/=",
        "UAH" : "₴\t",
        "UYU" : "$U \t",
        "UZS" : "\t сўм",
        "VEF" : "Bs.F. \t",
        "VND" : "\t₫",
        "VUV" : "VT\t",
        "XAF" : "FCFA\t",
        "XCD" : "EC$\t",
        "XOF" : "CFA \t",
        "XPF" : "\t F",
        "ZAR" : "R \t"
    };
    if(currencies[currency] == null){
        return currency + " " + fmAmount;
    }
    if( currencies[currency] instanceof Object ){ // not a string
        if(currencies[currency][language] != null){
            return currencies[currency][language].replace("\t", fmAmount);
        }
        return currencies[currency]["default"].replace("\t", fmAmount);
    }
    return currencies[currency].replace("\t", fmAmount);
}
</script>
<script>
function getMinimum(currency){
    var minimums = {
        'ADF' : 4.8,
        'ADP' : 122.2,
        'AED' : 3.6,
        'AFA' : 58,
        'AFN' : 58,
        'ALL' : 104.8,
        'AMD' : 410,
        'ANG' : 1.8,
        'AOA' : 97.9,
        'AON' : 97.9,
        'ARS' : 8.1,
        'ATS' : 10.1,
        'AUD' : 1,
        'AWG' : 1.7,
        'AZM' : 3926.1,
        'AZN' : 0.78,
        'BAM' : 1.4,
        'BBD' : 2,
        'BDT' : 78.9,
        'BEF' : 29.6,
        'BGL' : 1.4,
        'BGN' : 1.4,
        'BHD' : 0.37,
        'BIF' : 1566.9,
        'BMD' : 1,
        'BND' : 1.2,
        'BOB' : 7,
        'BRL' : 2.2,
        'BSD' : 1,
        'BTN' : 60,
        'BWP' : 8.9,
        'BYR' : 10242.7,
        'BZD' : 2,
        'CAD' : 1,
        'CDF' : 936.3,
        'CHF' : 0.89,
        'CLP' : 557.4,
        'CNY' : 6.1,
        'COP' : 1880,
        'CRC' : 564.6,
        'CUC' : 1,
        'CUP' : 23.1,
        'CVE' : 81.1,
        'CYP' : 0.43,
        'CZK' : 20.1,
        'DEM' : 1.4,
        'DJF' : 186.1,
        'DKK' : 5.4,
        'DOP' : 43.7,
        'DZD' : 79.6,
        'ECS' : 25588.5,
        'EEK' : 11.7,
        'EGP' : 7.1,
        'ESP' : 122.2,
        'ETB' : 19.7,
        'EUR' : 0.73,
        'FIM' : 4.3,
        'FJD' : 1.8,
        'FKP' : 0.58,
        'FRF' : 4.8,
        'GBP' : 0.58,
        'GEL' : 1.7,
        'GHC' : 31867.4,
        'GHS' : 3.1,
        'GIP' : 0.58,
        'GMD' : 40,
        'GNF' : 7147.9,
        'GRD' : 250.3,
        'GTQ' : 7.9,
        'GYD' : 200,
        'HKD' : 7.7,
        'HNL' : 21.1,
        'HRK' : 5.5,
        'HTG' : 45.7,
        'HUF' : 224.9,
        'IDR' : 11947.4,
        'IEP' : 0.57,
        'ILS' : 3.4,
        'INR' : 60,
        'IQD' : 1177.9,
        'IRR' : 25660.7,
        'ISK' : 113.8,
        'ITL' : 1422.8,
        'JMD' : 112.6,
        'JOD' : 0.71,
        'JPY' : 101.8,
        'KES' : 88.8,
        'KGS' : 52.4,
        'KHR' : 4096.6,
        'KMF' : 362.4,
        'KPW' : 135,
        'KRW' : 1020.3,
        'KWD' : 0.28,
        'KYD' : 0.84,
        'KZT' : 185.5,
        'LAK' : 8176.6,
        'LBP' : 1528.8,
        'LKR' : 130.4,
        'LRD' : 90,
        'LSL' : 10.6,
        'LTL' : 2.5,
        'LUF' : 29.6,
        'LVL' : 0.51,
        'LYD' : 1.2,
        'MAD' : 8.2,
        'MDL' : 14.1,
        'MGA' : 2461.2,
        'MGF' : 9149.1,
        'MKD' : 45.6,
        'MMK' : 993,
        'MNT' : 1829.1,
        'MOP' : 8.1,
        'MRO' : 294.8,
        'MTL' : 0.31,
        'MUR' : 31.3,
        'MVR' : 15.6,
        'MWK' : 397.6,
        'MXN' : 12.9,
        'MYR' : 3.2,
        'MZM' : 31645.5,
        'MZN' : 31.6,
        'NAD' : 10.6,
        'NGN' : 164.6,
        'NIO' : 26.3,
        'NLG' : 1.6,
        'NOK' : 6,
        'NPR' : 97.4,
        'NZD' : 1.1,
        'OMR' : 0.38,
        'PAB' : 1,
        'PEN' : 2.8,
        'PGK' : 2.4,
        'PHP' : 43.8,
        'PKR' : 99,
        'PLN' : 3,
        'PTE' : 147.3,
        'PYG' : 4494.3,
        'QAR' : 3.6,
        'ROL' : 32320.6,
        'RON' : 3.2,
        'RSD' : 85.1,
        'RUB' : 34.3,
        'RWF' : 690.1,
        'SAR' : 3.7,
        'SBD' : 7.3,
        'SCR' : 13,
        'SDD' : 572.7,
        'SDG' : 5.7,
        'SDP' : 2272.2,
        'SEK' : 6.6,
        'SGD' : 1.2,
        'SHP' : 0.58,
        'SIT' : 176,
        'SKK' : 22.1,
        'SLL' : 4450.3,
        'SOS' : 970.8,
        'SRD' : 3.3,
        'SRG' : 3300.3,
        'STD' : 18185.1,
        'SVC' : 8.9,
        'SYP' : 149.2,
        'SZL' : 10.6,
        'THB' : 32.4,
        'TJS' : 4.9,
        'TMM' : 14285.7,
        'TMT' : 2.8,
        'TND' : 1.6,
        'TOP' : 1.8,
        'TRL' : 2131287.2,
        'TRY' : 2.1,
        'TTD' : 6.5,
        'TWD' : 30,
        'TZS' : 1717.9,
        'UAH' : 12.1,
        'UGX' : 2608.9,
        'USD' : 1,
        'UYU' : 23.4,
        'UZS' : 2367.9,
        'VEB' : 6301.1,
        'VEF' : 6.3,
        'VND' : 21431.6,
        'VUV' : 94.6,
        'WST' : 2.3,
        'XAF' : 481.9,
        'XAG' : 0.04,
        'XAU' : 0,
        'XCD' : 2.7,
        'XEU' : 0.73,
        'XOF' : 481.9,
        'XPD' : 0,
        'XPF' : 88,
        'XPT' : 0,
        'YER' : 215.2,
        'YUN' : 85.1,
        'ZAR' : 10.6,
        'ZMK' : 5327.6,
        'ZWD' : 376.3
    };

    if($.inArray(currency, minimums)){
        return minimums[currency];
    }

    return 1;
}
</script>
<script>
var fundraisingAmounts = fundraisingAmounts || {};
 
fundraisingAmounts.asks = {
    "USD" : {
        "default" : [3, 5, 10, 20, 30, 50, 100]
    },
    "EUR" : {
        "ES"      : [2, 5, 10, 20, 30, 50, 100],
        "NL"      : [2, 5, 10, 20, 30, 50, 100],
        "BE"      : [2, 5, 10, 20, 30, 50, 100],
        "FR"      : [2, 5, 10, 20, 30, 50, 100],
        "default" : [3, 5, 10, 20, 30, 50, 100]
    },
    "GBP" : [3, 5, 10, 20, 30, 50, 100],
    "CAD" : [3, 5, 10, 20, 30, 50, 100],
    "AUD" : [3, 5, 10, 20, 30, 50, 100],
    "NZD" : [3, 5, 10, 20, 30, 50, 100],
    "AED" : [25, 50, 100, 200, 300, 500, 1000],
    "ANG" : [5, 10, 20, 35, 50, 100, 200],
    "ARS" : [20, 50, 100, 200, 500, 750, 1000],
    "BAM" : [3, 5, 10, 25, 40, 70, 140],
    "BBD" : [5, 10, 20, 40, 60, 100, 200],
    "BDT" : [200, 400, 800, 1500, 2000, 4000, 7500],
    "BGN" : [10, 25, 50, 75, 100, 150, 200],
    "BHD" : [1, 2, 4, 8, 10, 20, 40],
    "BMD" : [3, 5, 10, 20, 30, 50, 100],
    "BOB" : [20, 30, 60, 140, 200, 300, 600],
    "BRL" : [10, 20, 30, 50, 100, 250, 500],
    "BZD" : [5, 10, 20, 40, 60, 100, 200],
    "CLP" : [1500, 2500, 5000, 10000, 15000, 25000, 50000],
    "CNY" : [50, 75, 100, 300, 500, 1000, 1500],
    "COP" : [5000, 10000, 20000, 50000, 100000, 150000, 200000],
    "CRC" : [2500, 5000, 10000, 20000, 50000, 75000, 100000],
    "CZK" : [50, 100, 200, 400, 600, 1000, 2000],
    "DKK" : [20, 100, 150, 200, 300, 500, 1000],
    "DOP" : [200, 500, 1000, 2000, 5000, 7500, 10000],
    "DZD" : [200, 400, 750, 1500, 2000, 4000, 7500],
    "FJD" : [5, 10, 20, 40, 50, 100, 200],
    "GTQ" : [20, 40, 80, 150, 200, 350, 750],
    "HKD" : [20, 50, 100, 150, 200, 500, 1000],
    "HNL" : [50, 100, 200, 400, 600, 1000, 1800],
    "HRK" : [35, 50, 100, 250, 500, 1000, 1500],
    "HUF" : [500, 1000, 2000, 4000, 5000, 7000, 10000],
    "IDR" : [35000, 50000, 75000, 100000, 150000, 250000, 500000],
    "ISK" : [300, 500, 1000, 2000, 3000, 5000, 10000],
    "ILS" : [15, 50, 100, 200, 300, 500, 1000],
    "INR" : [100, 200, 300, 500, 1000, 2000, 3000],
    "JMD" : [300, 500, 1000, 2000, 3000, 5000, 10000],
    "JPY" : [700, 1000, 1500, 2000, 3000, 5000, 10000],
    "KRW" : [5000, 10000, 20000, 30000, 50000, 75000, 100000],
    "KWD" : [2, 5, 10,15, 25, 30, 75],
    "KZT" : [750, 1500, 3000, 7500, 12000, 15000, 35000],
    "LBP" : [7500, 15000, 30000, 75000, 120000, 175000, 350000],
    "LTL" : [15, 25, 50, 100, 200, 250, 600],
    "MKD" : [100, 200, 400, 800, 1200, 2000, 4000],
    "MVR" : [40, 75, 150, 300, 450, 750, 1500],
    "MXN" : [50, 100, 200, 500, 750, 1000, 2000],
    "MYR" : [10, 30, 50, 100, 200, 300, 500],
    "NIO" : [100, 250, 500, 1000, 1500, 2500, 5500],
    "NOK" : [20, 100, 150, 200, 500, 750, 1000],
    "OMR" : [1, 2, 4, 8, 10, 20, 40],
    "PAB" : [3, 5, 10, 20, 30, 50, 100],
    "PEN" : [15, 30, 50, 150, 200, 275, 700],
    "PHP" : [200, 500, 750, 1000, 2000, 3000, 5000],
    "PLN" : [10, 20, 50, 100, 200, 300, 500],
    "PYG" : [10000, 20000, 40000, 80000, 120000, 200000, 400000],
    "QAR" : [20, 50, 75, 185, 250, 350, 1000],
    "RON" : [25, 50, 75, 100, 200, 300, 500],
    "RUB" : [100, 150, 200, 500, 1000, 2000, 3000],
    "SAR" : [20, 50, 100, 200, 500, 1000, 1500],
    "SEK" : [30, 50, 100, 200, 300, 500, 1000],
    "SGD" : [3, 5, 10, 20, 30, 50, 100],
    "THB" : [50, 100, 250, 500, 750, 1000, 2000],
    "TND" : [5, 10, 15, 30, 50, 75, 150],
    "TRY" : [5, 10, 20, 40, 50, 100, 200],
    "TTD" : [20, 30, 60, 120, 200, 300, 600],
    "TWD" : [150, 300, 500, 1000, 1500, 2000, 5000],
    "UAH" : [50, 75, 150, 300, 500, 750, 1000],
    "UYU" : [100, 200, 400, 1000, 1500, 1950, 5000],
    "VEF" : [20, 30, 50, 100, 200, 300, 600],
    "VND" : [60000, 100000, 200000, 400000, 600000, 1000000, 2000000],
    "XCD" : [5, 10, 20, 50, 80, 120, 250],
    "XOF" : [1000, 2000, 5000, 10000, 150000, 200000, 400000],
    "XPF" : [250, 400, 800, 1600, 2500, 4000, 8000],
    "ZAR" : [20, 50, 100, 200, 300, 500, 1000]
};
 
fundraisingAmounts.averages = {
    "USD" : 15,
    "EUR" : 10,
    "GBP" : 10,
    "CAD" : 15,
    "AUD" : 15,
    "NZD" : 15,
    "AED" : 50,
    "ALL" : 1500,
    "AMD" : 5000,
    "ANG" : 25,
    "ARS" : 100,
    "AZN" : 10,
    "BAM" : 20,
    "BBD" : 30,
    "BDT" : 1000,
    "BGN" : 20,
    "BHD" : 6,
    "BMD" : 15,
    "BND" : 20,
    "BOB" : 100,
    "BRL" : 25,
    "BSD" : 15,
    "BZD" : 30,
    "CHF" : 15,
    "CLP" : 8000,
    "CNY" : 100,
    "COP" : 30000,
    "CRC" : 7500,
    "CZK" : 250,
    "DKK" : 100,
    "DOP" : 600,
    "DZD" : 1000,
    "EGP" : 100,
    "ETB" : 250,
    "FJD" : 25,
    "GEL" : 25,
    "GNF" : 100000,
    "GTQ" : 100,
    "HKD" : 100,
    "HNL" : 250,
    "HRK" : 80,
    "HUF" : 2500,
    "IDR" : 150000,
    "ILS" : 50,
    "INR" : 750,
    "IQD" : 15000,
    "ISK" : 1500,
    "JMD" : 1500,
    "JOD" : 10,
    "JPY" : 1500,
    "KES" : 1000,
    "KRW" : 15000,
    "KWD" : 5,
    "KZT" : 2000,
    "LBP" : 20000,
    "LKR" : 2000,
    "LTL" : 40,
    "LVL" : 10,
    "MAD" : 120,
    "MKD" : 600,
    "MOP" : 100,
    "MUR" : 500,
    "MVR" : 200,
    "MXN" : 150,
    "MYR" : 50,
    "MZN" : 50,
    "NGN" : 2500,
    "NIO" : 350,
    "NOK" : 100,
    "OMR" : 5,
    "PAB" : 15,
    "PEN" : 40,
    "PHP" : 600,
    "PKR" : 1500,
    "PLN" : 50,
    "PYG" : 60000,
    "QAR" : 50,
    "RON" : 50,
    "RSD" : 1200,
    "RUB" : 500,
    "SAR" : 50,
    "SBD" : 100,
    "SEK" : 100,
    "SGD" : 20,
    "THB" : 500,
    "TND" : 25,
    "TRY" : 25,
    "TTD" : 100,
    "TWD" : 500,
    "UAH" : 150,
    "UYU" : 300,
    "VEF" : 100,
    "VND" : 300000,
    "XCD" : 40,
    "XOF" : 7000,
    "XPF" : 1000,
    "ZAR" : 150
};

// Mostly the same as first asks option.
// For backwards compatibility, these need to be given as an array of one number
fundraisingAmounts.ifEveryone = {
    "USD" : {
        "default" : [3]
    },
    "EUR" : {
        "ES"      : [2],
        "NL"      : [2],
        "BE"      : [2],
        "FR"      : [2],
        "default" : [3]
    },
    "GBP" : [3],
    "CAD" : [3],
    "AUD" : [3],
    "NZD" : [3],
    "AED" : [25],
    "ANG" : [5],
    "ARS" : [20],
    "BAM" : [3],
    "BBD" : [5],
    "BDT" : [200],
    "BGN" : [10],
    "BHD" : [1],
    "BMD" : [3],
    "BOB" : [20],
    "BRL" : [10],
    "BZD" : [5],
    "CLP" : [1500],
    "CNY" : [50],
    "COP" : [5000],
    "CRC" : [2500],
    "CZK" : [50],
    "DKK" : [20],
    "DOP" : [200],
    "DZD" : [200],
    "FJD" : [5],
    "GTQ" : [20],
    "HKD" : [20],
    "HNL" : [50],
    "HRK" : [35],
    "HUF" : [500],
    "IDR" : [35000],
    "ILS" : [15],
    "INR" : [100],
    "ISK" : [300],
    "JMD" : [300],
    "JPY" : [700],
    "KRW" : [5000],
    "KWD" : [2],
    "KZT" : [750],
    "LBP" : [7500],
    "LTL" : [15],
    "MKD" : [100],
    "MVR" : [40],
    "MXN" : [50],
    "MYR" : [10],
    "NIO" : [100],
    "NOK" : [20],
    "OMR" : [1],
    "PAB" : [3],
    "PEN" : [15],
    "PHP" : [200],
    "PLN" : [10],
    "PYG" : [10000],
    "QAR" : [20],
    "RON" : [25],
    "RUB" : [100],
    "SAR" : [20],
    "SEK" : [30],
    "SGD" : [3],
    "THB" : [50],
    "TND" : [5],
    "TRY" : [5],
    "TTD" : [20],
    "TWD" : [150],
    "UAH" : [50],
    "UYU" : [100],
    "VEF" : [20],
    "VND" : [60000],
    "XCD" : [5],
    "XOF" : [1000],
    "XPF" : [250],
    "ZAR" : [20]
};
</script>
<script>
var fundraisingAmounts = fundraisingAmounts || {};

fundraisingAmounts.pick = function(source, currency, country) {
    /*
     * Select the correct amount or array of amounts from object in "source"
     * In future may extend to allow a language parameter too
     * 
     * @param {object} source   - the amounts object e.g. fundraisingAmounts.asks, fundraisingAmounts.averages
     * @param {string} currency - ISO code of currency
     * @param {string} country  - ISO code of country (optional)
     * @return {array/number}   - depending on source
     */

    if ( source[currency]["default"] ) { // we need to go deeper
        if ( source[currency][country] !== undefined ) {
            return source[currency][country];
        } else {
            return source[currency]["default"];
        }
    } else {
        return source[currency];
    }
};

function convertAsk(amount, currency, country) {
    /*
     * Given an amount in USD, find an "equivalent" local ask amount for the given currency and country
     * @TODO: namespace this
     * 
     * @param {number} amount   - USD amount
     * @param {string} currency - ISO code of currency
     * @param {string} country  - ISO code of country
     * @return {number}         - local amount
     */
    var usdbase = [3, 5, 10, 20, 30, 50, 100];
    var usdamount = parseInt(amount, 10);
    if(isNaN(usdamount)){
        return 0;
    }

    if(currency === 'USD'){
        if(country === 'US'){
            // easy case, skip the complicated stuff
            return usdamount;
        }
    }
 
    var index = $.inArray(usdamount, usdbase);
    if (index == -1) {
        // the amount is not in the USD ask array, find a near neighbor
        index = 0;
        while (usdbase[index+1] < usdamount && index < usdbase.length + 1) {
            index++;
        }
    }

    var array = fundraisingAmounts.pick(fundraisingAmounts.asks, currency, country);
    if (array === undefined) {
        // in case we don't have amounts for this currency
        return usdamount;
    } else {
        return array[index];
    }
}

function getAverage(currency, country, language) {
    /*
     * Return a formatted string with the "average" donation amount in the given currency, country and language
     * @TODO: just move this into banner code
     * 
     * @param {string} currency - ISO code of currency
     * @param {string} country  - ISO code of country
     * @param {string} country  - ISO code of language     
     * @return {string}         - formatted amount 
     */
    var amount = fundraisingAmounts.pick(fundraisingAmounts.averages, currency, country);
    return currencyLocalize(currency, amount, language);
}

function getMinimumString(currency, country, language) {
    /*
     * Return a formatted string with the "if everyone" amount in the given currency, country and language
     * @TODO: just move this into banner code
     * 
     * @param {string} currency - ISO code of currency
     * @param {string} country  - ISO code of country
     * @param {string} country  - ISO code of language     
     * @return {string}         - formatted amount 
     */
    var amount = fundraisingAmounts.pick(fundraisingAmounts.ifEveryone, currency, country)[0];
    return currencyLocalize(currency, amount, language);
}
</script>
<script>
// Various common actions for banner setup.

var getQuerystring = function(key) {
    // Should not be used any more, use mw.util.getParamValue() instead
    // Kept for now though because this returns '' instead of null if key is not present
    key = key.replace( /[\[]/, '\\\[' ).replace( /[\]]/, '\\\]' );
    var regex = new RegExp( '[\\?&]' + key + '=([a-zA-Z0-9\_\-]*)' );
    var qs = regex.exec( window.location.search );
    return qs == null ? '' : qs[1];
};

$(document).ready( function () {

    // Allow overriding the geolocation for testing different countries
    if(mw.util.getParamValue('country')) {
        Geo.country = mw.util.getParamValue('country');
    }

    var language = mw.config.get('wgUserLanguage'),
        currency = getCurrency(Geo.country);

    // Localize links in the smallprint
    $('#B14_1122_enUS_dsk_hlcf_mr-legal a').each(function(index) {
        // Add language and country
        var url = $(this).attr("href");
        if( url.indexOf("?") === -1 ) {
            url = url + "?";
        } else {
            url = url + "&";
        }
        $(this).attr("href", url + "country=" + Geo.country + "&language=" + language + "&uselang=" + language);

        // Make links open in new tab
        $(this).attr("target", "_blank");
    });

    // Show the correct legal text variant
    if (Geo.country === 'US') {
        $('.informationsharing-US').show();
        $('.informationsharing-NL').hide();
        $('.informationsharing-nonUS').hide();
    }
    if (Geo.country === 'NL') {
        $('.informationsharing-US').hide();
        $('.informationsharing-NL').show();
        $('.informationsharing-nonUS').hide();
    }

    // Find instances of %AVERAGE% in banner text, and replace with correct average for currency & country
    $("div#B14_1122_enUS_dsk_hlcf_mr").each(function(index){
        var avgString = getAverage(currency, Geo.country, language).replace(/\.$/, ''); // strip any period from end for use in running text
        var newHtml = $(this).html().replace(/%AVERAGE%/g, '<span style="white-space: nowrap;">' + avgString + '</span>');
        $(this).html(newHtml);
    });

    // Find instances of %MINIMUM% in banner text, and replace with correct minimum for currency & country
    $("div#B14_1122_enUS_dsk_hlcf_mr").each(function(index){
        var ifString = getMinimumString(currency, Geo.country, language).replace(/\.$/, ''); // strip any period from end for use in running text
        var newHtml = $(this).html().replace(/%MINIMUM%/g, '<span style="white-space: nowrap;">' + ifString + '</span>');
        $(this).html(newHtml);
    });

});
</script>
<script>
/**
 * Core code for forms in banners.
 * Handles showing the correct payment methods, validating input, and sending on to payments wiki.
 * (Note that localizing amount options is done elsewhere, in BannerFormAmountOptions.js)
 *
 * Dependencies: FR2013/Resources/CurrencyMinimums.js
 *               FR2013/Resources/Country2Currency.js
 */
 
function checkPaymentOutages() {
    /*  Check against the scheduled payment method outages below
     *  and hide the relevant button for any which are ongoing.
     */
    /* This file can be used to schedule hiding of individual payment methods from
 * banners and donatewiki e.g. if they have scheduled downtime.
 * Valid methods are:
 * ideal, yandex, cc, dd, sofort, pp, amazon, bpay, webmoney, boletos, enets, pp-usd
 * (most of the time it's 'ideal'...)
 * Can also limit outage to a specific country with country: "XX" (where XX is an ISO code)
 *
 * Note that in JavaScript dates the months (and only the months) start at 0.
 * Jan=0, Feb=1, Mar=2, Apr=3 etc. How hateful 
 */
var outages = [
{
    start:      new Date(Date.UTC(2014, 9, 14)), 
    end:        new Date(Date.UTC(2015, 0, 1)),
    method:     "ideal"
},
{
    start:      new Date(Date.UTC(2014, 9, 14)), 
    end:        new Date(Date.UTC(2015, 0, 1)),
    method:     "bpay"
},
{
    start:      new Date(Date.UTC(2014, 9, 14)), 
    end:        new Date(Date.UTC(2015, 0, 1)),
    method:     "yandex"
}
];
    var now = new Date();

    for (var i = outages.length - 1; i >= 0; i--) {
        if ( now > outages[i]['start'] && now < outages[i]['end'] ) {
            if (outages[i]['country'] === undefined || outages[i]['country'] == Geo.country) {
                $('.paymentmethod-' + outages[i]['method']).hide();
            }
        }
    };
}

function validateForm(form) {
    var error = true;
 
    // Get amount selection
    var amount = null;
    for (var i = 0; i < form.amount.length; i++) {
        if (form.amount[i].checked) {
            amount = form.amount[i].value;
        }
    }
    if (form.amountGiven.value != '') {
        var otherAmount = form.amountGiven.value;
        otherAmount = otherAmount.replace(/[,.](\d)$/, '\:$10');
        otherAmount = otherAmount.replace(/[,.](\d)(\d)$/, '\:$1$2');
        otherAmount = otherAmount.replace(/[\$,.]/g, '');
        otherAmount = otherAmount.replace(/:/, '.');
        form.amountGiven.value = otherAmount;
        amount = otherAmount;
    }
    // Check amount is a real number
    error = ( amount == null || isNaN(amount) || amount.value <= 0 );
    // Check amount is at least the minimum
    var currency = form.currency_code.value;
    if (amount < getMinimum(currency) || error) {
        alert('You must contribute at least $1'.replace('$1', getMinimum(currency) + ' ' + currency));
        error = true;
    }
    return !error;
}
 
function redirectPayment(paymentMethod, paymentSubMethod, skipValidation) {
    if (typeof paymentSubMethod == 'undefined'){
        paymentSubMethod = '';
    }
    var form = document.paypalcontribution; // we should really change this some day
    var language = $("input[name='language']").val();
 
    var paymentsURL = 'https://payments.wikimedia.org/index.php/Special:GatewayFormChooser';
 
    var params = {
        'uselang' : language,
        'language' : language,
        'currency_code' : $("input[name='currency_code']").val(),
        'country' : $("input[name='country']").val(),
        'paymentmethod' : paymentMethod
    };
    if( paymentSubMethod != '' ){
        params['submethod'] = paymentSubMethod;
    }
 
    // Testing for Adyen
    if( paymentMethod === 'adyen-cc' ) {
      paymentMethod = 'cc';
      params.paymentmethod = 'cc';
      params.gateway = 'adyen';
    }
 
    var frequency = $("input[name='frequency']:checked").val();
    if( frequency !== 'monthly' ){
        frequency = 'onetime';
    } else {
        params['recurring'] = 'true';
        // the following line is both obsolete, and is causing errors.
        // paymentMethod = 'r' + paymentMethod;
    }
 
    form.action = paymentsURL + '?' + $.param(params);
    form.payment_method.value = paymentMethod;
    if( paymentSubMethod != '' ) {
        form.payment_method.value = form.payment_method.value + '.' + paymentSubMethod;
    }
    form.utm_source.value = 'B14_1122_enUS_dsk_hlcf_mr.no-LP' + '.' + form.payment_method.value;
 
    if (skipValidation || validateForm(document.paypalcontribution)) {
        form.submit();
    }
}
 
function toggleMonthly( monthly ){
    if( monthly.type === 'checkbox' ){
        monthly = monthly.checked;
    } 
    if (monthly) {
        $('#B14_1122_enUS_dsk_hlcf_mr-form').addClass('form-monthly');
    } else {
        $('#B14_1122_enUS_dsk_hlcf_mr-form').removeClass('form-monthly');
    }
}
 
$(document).ready( function () {
    if ( wgCanonicalSpecialPageName != "CentralNotice" && wgCanonicalSpecialPageName != "NoticeTemplate" ) {
        mw.centralNotice.events.bannerLoaded.done(function(e){
            $('[name="paypalcontribution"]').append($('<input>', {
                'type': 'hidden', 'name': 'utm_key', 'value': (mw.centralNotice.bannerData.cookieCount || 0)
            }));
        });
 
        var currency = getCurrency(Geo.country);
        var language = mw.config.get('wgUserLanguage');
 
        // hide CC or PP buttons anywhere we need to
        var noCC = [];
        if ($.inArray(Geo.country, noCC) != -1) {
            $(".paymentmethod-cc").remove();
        }
        var noPP = ['IN', 'RU', 'SG', 'AE', 'QA', 'JP', 'OM', 'BD', 'BO',
                    'PA', 'PE', 'PY', 'GT', 'JM', 'TT', 'DZ'];
        if ($.inArray(Geo.country, noPP) != -1){
            $(".paymentmethod-pp").remove();
        }

        // countries where PP must be in USD
        var ppUSD = ['BG', 'HR', 'LT', 'MK', 'RO', 'UA', 'SA', 'CN', 'ID', 'KR', 
                     'KZ', 'MY', 'VN', 'AR', 'CL', 'DO', 'CO', 'NI', 'UY', 'ZA',
                     'BH', 'LB', 'VE', 'TR', 'IS', 'BA', 'MV', 'BB', 'BM', 'BZ',
                     'CR', 'CW', 'SX', 'HN', 'KN', 'DM', 'AG', 'LC', 'GD', 'FJ',
                     'TN', 'BJ', 'BF', 'CI', 'GW', 'ML', 'NE', 'SN', 'TG'];
        if ($.inArray(Geo.country, ppUSD) != -1){
            $(".paymentmethod-pp").remove();
            $(".paymentmethod-pp-usd").show();
        }
 
        // can't do monthly credit card in India
        if (Geo.country === 'IN') {
            $(".paymentmethod-cc").addClass("no-monthly");
        }
 
        // show any extra local payment methods, or remove them if not needed
        var extrapaymentmethods = {
            'amazon' : ['US'],
            'bpay' : ['AU'],
            'ideal' : ['NL'],
            'yandex' : ['RU'],
            'webmoney' : ['RU'],
            'dd' : ['AT', 'DE', 'ES', 'NL'],
            // 'boletos' : ['BR'],
            'enets' : ['SG'],
            'sofort' : []
        };
 
        for (var method in extrapaymentmethods) {
            var $methodbutton = $('.paymentmethod-' + method);
 
            if ($.inArray(Geo.country, extrapaymentmethods[method]) != -1) { // country is in the list
                $methodbutton.show();
            } else {
                $methodbutton.remove();
            }
        }
        
        checkPaymentOutages();
 
        // set the form fields
        $("input[name='country']").val(Geo.country);
        $("input[name='currency_code']").val(currency);
        $("input[name='language']").val(mw.config.get('wgUserLanguage'));
        $("input[name='return_to']").val("Thank_You/" + mw.config.get('wgUserLanguage'));
 
        // handle pressing Enter on "Other" field
        $('input[name="amountGiven"]').keydown(function(e){
            if (e.keyCode == 13) {
                e.preventDefault();
                redirectPayment('cc'); // use credit card by default. Might be nice to have different defaults for some countries, but this will do for now.
                return false;
            }
        });
 
        // if there are no recurring payment methods available, disable the "monthly" radio button.
        if ( !$('#B14_1122_enUS_dsk_hlcf_mr-form *[class^="paymentmethod-"]:not(.no-monthly)').length ) {
            $('#frequency_monthly').prop('disabled', 'disabled');
        }
    }
});
</script>
<script>
/**
 * Localize amount options in banner forms.
 *
 * Dependencies: FR2013/Resources/Country2Currency.js
 *               FR2013/Resources/CurrencyLocalize.js
 */

$(document).ready( function () {

    var currency = getCurrency(Geo.country);
    var language = mw.config.get('wgUserLanguage');

    // do fun things to localize currency in the banner and form
    $("input[name='amount']").each(function(index){
        var id = $(this).attr("id");
        var $label = $("label[for='" + id + "']");
        var amount = convertAsk($(this).val(), currency, Geo.country);
        $(this).val(amount);
        $label.html(currencyLocalize(currency, amount, language));
    });

    $otherLabel = $("span#input_label_other");
    $otherLabel.html( 
        currencyLocalize(
            currency,
            $otherLabel.html().replace('$',''), // strip the $ sign already on there
            language
        )
    );

});
</script>

<script>
$(document).ready(function() {
  /* code for "nag" box */
  $(window).scroll(function() {
    if ($(window).scrollTop() <= 160) {
      $('#B14_1122_enUS_dsk_hlcf_mr-nag').css('display', 'none');
    }
    window.setTimeout(function() {
      if ($(window).scrollTop() > 160) {      
        $('#B14_1122_enUS_dsk_hlcf_mr-nag').slideDown();
      }
    }, 1500 );
  });

  $('.B14_1122_enUS_dsk_hlcf_mr-nag-close').click(function(event) {
    $('#B14_1122_enUS_dsk_hlcf_mr-nag').remove();
    event.stopPropagation();
  });

  $('#B14_1122_enUS_dsk_hlcf_mr-nag').click(function() {
    window.scrollTo(0,0);
  });
  /* end code for nag box */

  $('#B14_1122_enUS_dsk_hlcf_mr').mouseenter(fundraisingBanner.showLegal);
  $('#B14_1122_enUS_dsk_hlcf_mr').mouseleave(fundraisingBanner.hideLegal);

  if (!mw.centralNotice.bannerData.hideResult) {
    fundraisingBanner.show();
  }

});
</script></div><!-- CentralNotice --></div>
						<div class="mw-indicators">
</div>
			<h1 id="firstHeading" class="firstHeading" lang="en"><span dir="auto">Entropy (information theory)</span></h1>
						<div id="bodyContent" class="mw-body-content">
									<div id="siteSub">From Wikipedia, the free encyclopedia</div>
								<div id="contentSub"></div>
												<div id="jump-to-nav" class="mw-jump">
					Jump to:					<a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29#mw-navigation">navigation</a>, 					<a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29#p-search">search</a>
				</div>
				<div id="mw-content-text" lang="en" dir="ltr" class="mw-content-ltr"><table class="metadata plainlinks ambox ambox-content ambox-Refimprove" role="presentation">
<tbody><tr>
<td class="mbox-image">
<div style="width:52px;"><a href="http://en.wikipedia.org/wiki/File:Question_book-new.svg" class="image"><img alt="" src="./Entropy (information theory) - Wikipedia, the free encyclopedia_files/50px-Question_book-new.svg.png" width="50" height="39" srcset="//upload.wikimedia.org/wikipedia/en/thumb/9/99/Question_book-new.svg/75px-Question_book-new.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/9/99/Question_book-new.svg/100px-Question_book-new.svg.png 2x" data-file-width="262" data-file-height="204"></a></div>
</td>
<td class="mbox-text"><span class="mbox-text-span">This article <b>needs additional citations for <a href="http://en.wikipedia.org/wiki/Wikipedia:Verifiability" title="Wikipedia:Verifiability">verification</a></b>. <span class="hide-when-compact">Please help <a class="external text" href="http://en.wikipedia.org/w/index.php?title=Entropy_(information_theory)&action=edit">improve this article</a> by <a href="http://en.wikipedia.org/wiki/Help:Introduction_to_referencing/1" title="Help:Introduction to referencing/1">adding citations to reliable sources</a>. Unsourced material may be challenged and removed.</span> <small><i>(April 2012)</i></small></span></td>
</tr>
</tbody></table>
<div class="thumb tright">
<div class="thumbinner" style="width:222px;"><a href="http://en.wikipedia.org/wiki/File:Entropy_flip_2_coins.jpg" class="image"><img alt="" src="./Entropy (information theory) - Wikipedia, the free encyclopedia_files/220px-Entropy_flip_2_coins.jpg" width="220" height="129" class="thumbimage" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/d/d4/Entropy_flip_2_coins.jpg/330px-Entropy_flip_2_coins.jpg 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/d/d4/Entropy_flip_2_coins.jpg/440px-Entropy_flip_2_coins.jpg 2x" data-file-width="1312" data-file-height="768"></a>
<div class="thumbcaption">
<div class="magnify"><a href="http://en.wikipedia.org/wiki/File:Entropy_flip_2_coins.jpg" class="internal" title="Enlarge"></a></div>
2 bits of entropy.</div>
</div>
</div>
<p>In <a href="http://en.wikipedia.org/wiki/Information_theory" title="Information theory">information theory</a>, <b>entropy</b> is the average amount of information contained in each message received. Here, <i>message</i> stands for an event, sample or character drawn from a distribution or data stream. Entropy thus characterizes our uncertainty about our source of information. (Entropy is best understood as a measure of uncertainty rather than certainty as entropy is larger for more random sources.) The source is also characterized by the probability distribution of the samples drawn from it. The idea here is that the less likely an event is, the more <a href="http://en.wikipedia.org/wiki/Self-information" title="Self-information">information</a> it provides when it occurs. For some other reasons (explained below) it makes sense to define information as the negative of the logarithm of the probability distribution. The probability distribution of the events, coupled with the information amount of every event, forms a random variable whose average (a.k.a. <a href="http://en.wikipedia.org/wiki/Expected_value" title="Expected value">expected</a>) value is the average amount of information, a.k.a. entropy, generated by this distribution. Because entropy is average information, it is also measured in <a href="http://en.wikipedia.org/wiki/Shannon_(unit)" title="Shannon (unit)">shannons</a>, <a href="http://en.wikipedia.org/wiki/Nat_(unit)" title="Nat (unit)">nats</a>, or <a href="http://en.wikipedia.org/wiki/Hartley_(unit)" title="Hartley (unit)">hartleys</a>, depending on the base of the logarithm used to define it.</p>
<p>The logarithm of the probability distribution is useful as measure of information because of its additivity. For instance, flipping a coin provides 1 shannon of information whereas <i>m</i> tosses gather <i>m</i> bits. Generally, you need log<sub>2</sub>(<i>n</i>) bits to represent a variable that can take one of <i>n</i> values. Since 1 of <i>n</i> outcomes is possible when you apply a scale graduated with <i>n</i> marks, you receive log<sub>2</sub>(<i>n</i>) bits of information with every such measurement. The log<sub>2</sub>(n) rule holds only until all outcomes are equally probable. If one of the events occurs more often than others, observation of that event is less informative. Conversely, observing rarer events compensate by providing more information when observed. Since observation of less probable events occurs more rarely, the net effect is that the entropy (thought of as the average information) received from non-uniformly distributed data is less than log<sub>2</sub>(<i>n</i>). Entropy is zero when only one certain outcome is expected. Shannon entropy quantifies all these considerations exactly when a probability distribution of the source is provided. It is important to note that the meaning of the events observed (a.k.a. the meaning of <i>messages</i>) do not matter in the definition of entropy. Entropy only takes into account the probability of observing a specific event, so the information it encapsulates is information about the underlying probability distribution, not the meaning of the events themselves.</p>
<p>Generally, "entropy" stands for "disorder" or uncertainty. The entropy we talk about here was introduced by <a href="http://en.wikipedia.org/wiki/Claude_E._Shannon" title="Claude E. Shannon" class="mw-redirect">Claude E. Shannon</a> in his 1948 paper "<a href="http://en.wikipedia.org/wiki/A_Mathematical_Theory_of_Communication" title="A Mathematical Theory of Communication">A Mathematical Theory of Communication</a>".<sup id="cite_ref-shannonPaper_1-0" class="reference"><a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29#cite_note-shannonPaper-1"><span>[</span>1<span>]</span></a></sup> We also call it <b>Shannon entropy</b> to distinguish from other occurrences of the term, which appears in various parts of physics in different forms. Shannon entropy provides an absolute limit on the best possible average length of <a href="http://en.wikipedia.org/wiki/Lossless" title="Lossless" class="mw-redirect">lossless</a> encoding or <a href="http://en.wikipedia.org/wiki/Data_compression" title="Data compression">compression</a> of any communication, assuming that<sup id="cite_ref-2" class="reference"><a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29#cite_note-2"><span>[</span>2<span>]</span></a></sup> the communication may be represented as a sequence of <a href="http://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables" title="Independent and identically distributed random variables">independent and identically distributed random variables</a>.</p>
<p></p>
<div id="toc" class="toc">
<div id="toctitle">
<h2>Contents</h2>
<span class="toctoggle">&nbsp;[<a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29#" id="togglelink">hide</a>]&nbsp;</span></div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29#Introduction"><span class="tocnumber">1</span> <span class="toctext">Introduction</span></a></li>
<li class="toclevel-1 tocsection-2"><a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29#Definition"><span class="tocnumber">2</span> <span class="toctext">Definition</span></a></li>
<li class="toclevel-1 tocsection-3"><a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29#Example"><span class="tocnumber">3</span> <span class="toctext">Example</span></a></li>
<li class="toclevel-1 tocsection-4"><a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29#Rationale"><span class="tocnumber">4</span> <span class="toctext">Rationale</span></a></li>
<li class="toclevel-1 tocsection-5"><a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29#Aspects"><span class="tocnumber">5</span> <span class="toctext">Aspects</span></a>
<ul>
<li class="toclevel-2 tocsection-6"><a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29#Relationship_to_thermodynamic_entropy"><span class="tocnumber">5.1</span> <span class="toctext">Relationship to thermodynamic entropy</span></a></li>
<li class="toclevel-2 tocsection-7"><a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29#Entropy_as_information_content"><span class="tocnumber">5.2</span> <span class="toctext">Entropy as information content</span></a></li>
<li class="toclevel-2 tocsection-8"><a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29#Data_compression"><span class="tocnumber">5.3</span> <span class="toctext">Data compression</span></a></li>
<li class="toclevel-2 tocsection-9"><a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29#World.27s_technological_capacity_to_store_and_communicate_entropic_information"><span class="tocnumber">5.4</span> <span class="toctext">World's technological capacity to store and communicate entropic information</span></a></li>
<li class="toclevel-2 tocsection-10"><a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29#Limitations_of_entropy_as_information_content"><span class="tocnumber">5.5</span> <span class="toctext">Limitations of entropy as information content</span></a></li>
<li class="toclevel-2 tocsection-11"><a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29#Limitations_of_entropy_as_a_measure_of_unpredictability"><span class="tocnumber">5.6</span> <span class="toctext">Limitations of entropy as a measure of unpredictability</span></a></li>
<li class="toclevel-2 tocsection-12"><a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29#Data_as_a_Markov_process"><span class="tocnumber">5.7</span> <span class="toctext">Data as a Markov process</span></a></li>
<li class="toclevel-2 tocsection-13"><a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29#b-ary_entropy"><span class="tocnumber">5.8</span> <span class="toctext"><i>b</i>-ary entropy</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-14"><a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29#Efficiency"><span class="tocnumber">6</span> <span class="toctext">Efficiency</span></a></li>
<li class="toclevel-1 tocsection-15"><a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29#Characterization"><span class="tocnumber">7</span> <span class="toctext">Characterization</span></a>
<ul>
<li class="toclevel-2 tocsection-16"><a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29#Continuity"><span class="tocnumber">7.1</span> <span class="toctext">Continuity</span></a></li>
<li class="toclevel-2 tocsection-17"><a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29#Symmetry"><span class="tocnumber">7.2</span> <span class="toctext">Symmetry</span></a></li>
<li class="toclevel-2 tocsection-18"><a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29#Maximum"><span class="tocnumber">7.3</span> <span class="toctext">Maximum</span></a></li>
<li class="toclevel-2 tocsection-19"><a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29#Additivity"><span class="tocnumber">7.4</span> <span class="toctext">Additivity</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-20"><a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29#Further_properties"><span class="tocnumber">8</span> <span class="toctext">Further properties</span></a></li>
<li class="toclevel-1 tocsection-21"><a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29#Extending_discrete_entropy_to_the_continuous_case"><span class="tocnumber">9</span> <span class="toctext">Extending discrete entropy to the continuous case</span></a>
<ul>
<li class="toclevel-2 tocsection-22"><a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29#Differential_entropy"><span class="tocnumber">9.1</span> <span class="toctext">Differential entropy</span></a></li>
<li class="toclevel-2 tocsection-23"><a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29#Relative_entropy"><span class="tocnumber">9.2</span> <span class="toctext">Relative entropy</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-24"><a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29#Use_in_combinatorics"><span class="tocnumber">10</span> <span class="toctext">Use in combinatorics</span></a>
<ul>
<li class="toclevel-2 tocsection-25"><a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29#Loomis-Whitney_inequality"><span class="tocnumber">10.1</span> <span class="toctext">Loomis-Whitney inequality</span></a></li>
<li class="toclevel-2 tocsection-26"><a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29#Approximation_to_binomial_coefficient"><span class="tocnumber">10.2</span> <span class="toctext">Approximation to binomial coefficient</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-27"><a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29#See_also"><span class="tocnumber">11</span> <span class="toctext">See also</span></a></li>
<li class="toclevel-1 tocsection-28"><a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29#References"><span class="tocnumber">12</span> <span class="toctext">References</span></a></li>
<li class="toclevel-1 tocsection-29"><a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29#Further_reading"><span class="tocnumber">13</span> <span class="toctext">Further reading</span></a>
<ul>
<li class="toclevel-2 tocsection-30"><a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29#Textbooks_on_information_theory"><span class="tocnumber">13.1</span> <span class="toctext">Textbooks on information theory</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-31"><a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29#External_links"><span class="tocnumber">14</span> <span class="toctext">External links</span></a></li>
</ul>
</div>
<p></p>
<h2><span class="mw-headline" id="Introduction">Introduction</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="http://en.wikipedia.org/w/index.php?title=Entropy_(information_theory)&action=edit&section=1" title="Edit section: Introduction">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Entropy is a measure of <i>unpredictability</i> of <i>information content</i>. To get an informal, intuitive understanding of the connection between these three English terms, consider the example of a poll on some political issue. Usually, such polls happen because the outcome of the poll isn't already known. In other words, the outcome of the poll is relatively <i>unpredictable</i>, and actually performing the poll and learning the results gives some new <i>information</i>; these are just different ways of saying that the <i>entropy</i> of the poll results is large. Now, consider the case that the same poll is performed a second time shortly after the first poll. Since the result of the first poll is already known, the outcome of the second poll can be predicted well and the results should not contain much new information; in this case the entropy of the second poll results is small.</p>
<p>Now consider the example of a coin toss. When the coin is fair, that is, when the probability of heads is the same as the probability of tails, then the entropy of the coin toss is as high as it could be. This is because there is no way to predict the outcome of the coin toss ahead of time—the best we can do is predict that the coin will come up heads, and our prediction will be correct with probability 1/2. Such a coin toss has one bit of entropy since there are two possible outcomes that occur with equal probability, and learning the actual outcome contains one bit of information. Contrarily, a coin toss with a coin that has two heads and no tails has zero entropy since the coin will always come up heads, and the outcome can be predicted perfectly.</p>
<p>English text has fairly low entropy. In other words, it is fairly predictable. Even if we don't know exactly what is going to come next, we can be fairly certain that, for example, there will be many more e's than z's, that the combination 'qu' will be much more common than any other combination with a 'q' in it, and that the combination 'th' will be more common than 'z', 'q', or 'qu'. After the first few letters one can often guess the rest of the word. Uncompressed, English text has between 0.6 and 1.3 <a href="http://en.wikipedia.org/wiki/Bit" title="Bit">bits</a> of entropy for each character of message.<sup id="cite_ref-Schneier.2C_B_page_234_3-0" class="reference"><a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29#cite_note-Schneier.2C_B_page_234-3"><span>[</span>3<span>]</span></a></sup><sup id="cite_ref-Shannon.2C_Claude_E._1951_4-0" class="reference"><a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29#cite_note-Shannon.2C_Claude_E._1951-4"><span>[</span>4<span>]</span></a></sup></p>
<p>If a <a href="http://en.wikipedia.org/wiki/Data_compression" title="Data compression">compression</a> scheme is lossless—that is, you can always recover the entire original message by decompressing—then a compressed message has the same quantity of information as the original, but communicated in fewer characters. That is, it has more information, or a higher entropy, per character. This means a compressed message has less redundancy. Roughly speaking, <a href="http://en.wikipedia.org/wiki/Shannon%27s_source_coding_theorem" title="Shannon&#39;s source coding theorem">Shannon's source coding theorem</a> says that a lossless compression scheme cannot compress messages, on average, to have more than one bit of information per bit of message. The entropy of a message multiplied by the length of that message is a measure of how much information the message contains.</p>
<p>Shannon's theorem also implies that no lossless compression scheme can compress <i>all</i> messages. If some messages come out smaller, at least one must come out larger due to the <a href="http://en.wikipedia.org/wiki/Pigeonhole_principle" title="Pigeonhole principle">pigeonhole principle</a>. In practical use, this is generally not a problem, because we are usually only interested in compressing certain types of messages, for example English documents as opposed to gibberish text, or digital photographs rather than noise, and it is unimportant if a compression algorithm makes some unlikely or uninteresting sequences larger. However, the problem can still arise even in everyday use when applying a compression algorithm to already compressed data: for example, making a ZIP file of music in the <a href="http://en.wikipedia.org/wiki/FLAC" title="FLAC">FLAC</a> audio format is unlikely to achieve much extra saving in space.</p>
<h2><span class="mw-headline" id="Definition">Definition</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="http://en.wikipedia.org/w/index.php?title=Entropy_(information_theory)&action=edit&section=2" title="Edit section: Definition">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Named after <a href="http://en.wikipedia.org/wiki/H-theorem" title="H-theorem">Boltzmann's H-theorem</a>, Shannon defined the entropy <i>H</i> (Greek letter <a href="http://en.wikipedia.org/wiki/Eta" title="Eta">Eta</a>) of a <a href="http://en.wikipedia.org/wiki/Discrete_random_variable" title="Discrete random variable" class="mw-redirect">discrete random variable</a> <i>X</i> with possible values {<i>x</i><sub>1</sub>, ..., <i>x</i><sub><i>n</i></sub>} and <a href="http://en.wikipedia.org/wiki/Probability_mass_function" title="Probability mass function">probability mass function</a> <i>P(X)</i> as:</p>
<dl>
<dd><img class="mwe-math-fallback-image-inline tex" alt="H(X)  =  E[I(X)]  =  E[-\ln(P(X))]." src="./Entropy (information theory) - Wikipedia, the free encyclopedia_files/38ad97542e92e1f9b6a99279e0057304.png"></dd>
</dl>
<p>Here E is the <a href="http://en.wikipedia.org/wiki/Expected_value" title="Expected value">expected value operator</a>, and <i>I</i> is the <a href="http://en.wikipedia.org/wiki/Self-information" title="Self-information">information content</a> of <i>X</i>.<sup id="cite_ref-5" class="reference"><a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29#cite_note-5"><span>[</span>5<span>]</span></a></sup><sup id="cite_ref-6" class="reference"><a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29#cite_note-6"><span>[</span>6<span>]</span></a></sup> <i>I</i>(<i>X</i>) is itself a random variable.</p>
<p>When taken from a finite sample, the entropy can explicitly be written as</p>
<dl>
<dd><img class="mwe-math-fallback-image-inline tex" alt="H(X) = \sum_{i} {P(x_i)\,I(x_i)} = -\sum_{i} {P(x_i) \log_b P(x_i)}" src="./Entropy (information theory) - Wikipedia, the free encyclopedia_files/49343eb4fda021767005bc902299bb00.png"></dd>
</dl>
<p>where <i>b</i> is the <a href="http://en.wikipedia.org/wiki/Base_(exponentiation)" title="Base (exponentiation)">base</a> of the <a href="http://en.wikipedia.org/wiki/Logarithm" title="Logarithm">logarithm</a> used. Common values of <i>b</i> are 2, <a href="http://en.wikipedia.org/wiki/E_(mathematical_constant)" title="E (mathematical constant)">Euler's number <span class="texhtml"><i>e</i></span></a>, and 10, and the unit of entropy is <a href="http://en.wikipedia.org/wiki/Bit" title="Bit">bit</a> for <i>b</i>&nbsp;=&nbsp;2, <a href="http://en.wikipedia.org/wiki/Nat_(unit)" title="Nat (unit)">nat</a> for <i>b</i>&nbsp;=&nbsp;<span class="texhtml"><i>e</i></span>, and <a href="http://en.wikipedia.org/wiki/Dit_(information)" title="Dit (information)" class="mw-redirect">dit</a> (or digit) for <i>b</i>&nbsp;=&nbsp;10.<sup id="cite_ref-7" class="reference"><a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29#cite_note-7"><span>[</span>7<span>]</span></a></sup></p>
<p>In the case of <i>p</i>(<i>x</i><sub><i>i</i></sub>)&nbsp;=&nbsp;0 for some <i>i</i>, the value of the corresponding summand 0&nbsp;log<sub><i>b</i></sub>(0) is taken to be 0, which is consistent with the well-known <a href="http://en.wikipedia.org/wiki/Limit_of_a_function" title="Limit of a function">limit</a>:</p>
<dl>
<dd><img class="mwe-math-fallback-image-inline tex" alt="\lim_{p\to0+}p\log (p) = 0" src="./Entropy (information theory) - Wikipedia, the free encyclopedia_files/700dfd496533cb05a810021fdf9f0b45.png">.</dd>
</dl>
<p>One may also define the <a href="http://en.wikipedia.org/wiki/Conditional_entropy" title="Conditional entropy">conditional entropy</a> of two events <i>X</i> and <i>Y</i> taking values <i>x<sub>i</sub></i> and <i>y<sub>j</sub></i> respectively, as</p>
<dl>
<dd><img class="mwe-math-fallback-image-inline tex" alt=" H(X|Y)=\sum_{i,j}p(x_{i},y_{j})\log\frac{p(y_{j})}{p(x_{i},y_{j})}" src="./Entropy (information theory) - Wikipedia, the free encyclopedia_files/67ae8a64f4bdcd4d26082346d6df5f52.png"></dd>
</dl>
<p>where <i>p(x<sub>i</sub>,y<sub>j</sub>)</i> is the probability that <i>X=x<sub>i</sub></i> and <i>Y=y<sub>j</sub></i>. This quantity should be understood as the amount of randomness in the random variable <i>X</i> given that you know the value of <i>Y</i>.</p>
<h2><span class="mw-headline" id="Example">Example</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="http://en.wikipedia.org/w/index.php?title=Entropy_(information_theory)&action=edit&section=3" title="Edit section: Example">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div class="thumb tright">
<div class="thumbinner" style="width:202px;"><a href="http://en.wikipedia.org/wiki/File:Binary_entropy_plot.svg" class="image"><img alt="" src="./Entropy (information theory) - Wikipedia, the free encyclopedia_files/200px-Binary_entropy_plot.svg.png" width="200" height="193" class="thumbimage" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/2/22/Binary_entropy_plot.svg/300px-Binary_entropy_plot.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/2/22/Binary_entropy_plot.svg/400px-Binary_entropy_plot.svg.png 2x" data-file-width="169" data-file-height="163"></a>
<div class="thumbcaption">
<div class="magnify"><a href="http://en.wikipedia.org/wiki/File:Binary_entropy_plot.svg" class="internal" title="Enlarge"></a></div>
Entropy <i>H</i>(<i>X</i>) (i.e. the <a href="http://en.wikipedia.org/wiki/Expected_value" title="Expected value">expected</a> surprisal) of a coin flip, measured in shannons, graphed versus the fairness of the coin Pr(<i>X</i>=1), where <i>X</i>=1 represents a result of heads.<br>
<br>
Note that the maximum of the graph depends on the distribution. Here, the entropy is at most 1 shannon, and to communicate the outcome of a fair coin flip (2 possible values) will require an average of at most 1 bit. The result of a fair die (6 possible values) would require on average log<sub>2</sub>6 bits.</div>
</div>
</div>
<div class="hatnote relarticle mainarticle">Main article: <a href="http://en.wikipedia.org/wiki/Binary_entropy_function" title="Binary entropy function">Binary entropy function</a></div>
<div class="hatnote relarticle mainarticle">Main article: <a href="http://en.wikipedia.org/wiki/Bernoulli_process" title="Bernoulli process">Bernoulli process</a></div>
<p>Consider tossing a coin with known, not necessarily fair, probabilities of coming up heads or tails; this is known as the <a href="http://en.wikipedia.org/wiki/Bernoulli_process" title="Bernoulli process">Bernoulli process</a>.</p>
<p>The entropy of the unknown result of the next toss of the coin is maximized if the coin is fair (that is, if heads and tails both have equal probability 1/2). This is the situation of maximum uncertainty as it is most difficult to predict the outcome of the next toss; the result of each toss of the coin delivers one full <a href="http://en.wikipedia.org/wiki/Bit" title="Bit">bit</a> of information.</p>
<p>However, if we know the coin is not fair, but comes up heads or tails with probabilities <i>p</i> and <i>q</i>, where <i>p</i> ≠ <i>q</i>, then there is less uncertainty. Every time it is tossed, one side is more likely to come up than the other. The reduced uncertainty is quantified in a lower entropy: on average each toss of the coin delivers less than one full bit of information.</p>
<p>The extreme case is that of a double-headed coin that never comes up tails, or a double-tailed coin that never results in a head. Then there is no uncertainty. The entropy is zero: each toss of the coin delivers no new information as the outcome of each coin toss is always certain. In this respect, entropy can be normalized by dividing it by information length. This ratio is called <a href="http://en.wikipedia.org/wiki/Metric_entropy" title="Metric entropy" class="mw-redirect">metric entropy</a> and is a measure of the randomness of the information.</p>
<h2><span class="mw-headline" id="Rationale">Rationale</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="http://en.wikipedia.org/w/index.php?title=Entropy_(information_theory)&action=edit&section=4" title="Edit section: Rationale">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>To understand the meaning of <img class="mwe-math-fallback-image-inline tex" alt="\sum{p_i\log{1\over p_i}}" src="./Entropy (information theory) - Wikipedia, the free encyclopedia_files/d9b91e57176dadef26b3336c1425ffff.png">, at first, try to define an information function, <i>I</i>, in terms of an event <i>i</i> with probability, <img class="mwe-math-fallback-image-inline tex" alt="p_i" src="./Entropy (information theory) - Wikipedia, the free encyclopedia_files/8a4bbd153c74655abb7ca04c0fa901d8.png">. How much information is acquired due to the observation of event <i>i</i>? Shannon's solution follows from the fundamental <a href="http://en.wikipedia.org/wiki/Entropy_(information_theory)#Characterization" title="Entropy (information theory)">properties</a> of information:<sup id="cite_ref-8" class="reference"><a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29#cite_note-8"><span>[</span>8<span>]</span></a></sup></p>
<ol>
<li><i>I</i>(<i>p</i>) ≥ 0 – information is a non-negative quantity</li>
<li><i>I</i>(1) = 0 – events that always occur do not communicate information</li>
<li><i>I</i>(<i>p</i><sub>1</sub> <i>p</i><sub>2</sub>) = <i>I</i>(<i>p</i><sub>1</sub>) + <i>I</i>(<i>p</i><sub>2</sub>) – information due to independent events is additive</li>
</ol>
<p>The latter is a crucial property. It states that joint probability communicates as much information as two individual events separately. Particularly, if the first event can yield one of n equiprobable outcomes and another has one of <i>m</i> equiprobable outcomes then there are <i>mn</i> possible outcomes of the joint event. This means that if log<sub>2</sub>(<i>n</i>) bits are needed to encode the first value and log<sub>2</sub>(<i>m</i>) to encode the second, one needs log<sub>2</sub>(<i>mn</i>) = log<sub>2</sub>(<i>m</i>) + log<sub>2</sub>(<i>n</i>) to encode both. Shannon discovered that the proper choice of function to quantify information, preserving this additivity, is logarithmic, i.e.,</p>
<dl>
<dd><img class="mwe-math-fallback-image-inline tex" alt="I(p) = \log(1/p)" src="./Entropy (information theory) - Wikipedia, the free encyclopedia_files/2d53aca19c29252c638a0cabc10da28c.png"></dd>
</dl>
<p>The base of logarithm does not matter; any can be used. The different units of information (bits for log<sub>2</sub>, trits for log<sub>3</sub>, nats for ln and so on) are just constant multiples of each other. For instance, in case of a fair coin toss, heads provides log<sub>2</sub>(2) = 1 bit of information. Because of additivity, n tosses provide n bits of information.</p>
<p>Now, suppose we have a distribution where every event can happen with probability <i>p<sub>i</sub></i>. Suppose we have sampled it <i>N</i> times and outcome <i>i</i> was, thus, seen <img class="mwe-math-fallback-image-inline tex" alt="n_i = N p_i" src="./Entropy (information theory) - Wikipedia, the free encyclopedia_files/aadfb9bd829eca7cc1f4139bef2ed2b8.png"> times. The total amount of information we received is</p>
<dl>
<dd><img class="mwe-math-fallback-image-inline tex" alt="\sum {n_i I(p_i)} = \sum {(N p_i) \log(1/p_i)}" src="./Entropy (information theory) - Wikipedia, the free encyclopedia_files/eecae39325a6faa6b72eefbf4507c063.png"></dd>
</dl>
<p>and average amount of information that we receive with every event is N times less</p>
<dl>
<dd><img class="mwe-math-fallback-image-inline tex" alt="\sum {p_i \log {1\over p_i}}." src="./Entropy (information theory) - Wikipedia, the free encyclopedia_files/328a1ee1f0db87e7c6aefa87efcac491.png"></dd>
</dl>
<h2><span class="mw-headline" id="Aspects">Aspects</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="http://en.wikipedia.org/w/index.php?title=Entropy_(information_theory)&action=edit&section=5" title="Edit section: Aspects">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<h3><span class="mw-headline" id="Relationship_to_thermodynamic_entropy">Relationship to thermodynamic entropy</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="http://en.wikipedia.org/w/index.php?title=Entropy_(information_theory)&action=edit&section=6" title="Edit section: Relationship to thermodynamic entropy">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<div class="hatnote relarticle mainarticle">Main article: <a href="http://en.wikipedia.org/wiki/Entropy_in_thermodynamics_and_information_theory" title="Entropy in thermodynamics and information theory">Entropy in thermodynamics and information theory</a></div>
<p>The inspiration for adopting the word <i>entropy</i> in information theory came from the close resemblance between Shannon's formula and very similar known formulae from <a href="http://en.wikipedia.org/wiki/Statistical_mechanics" title="Statistical mechanics">statistical mechanics</a>.</p>
<p>In <a href="http://en.wikipedia.org/wiki/Statistical_thermodynamics" title="Statistical thermodynamics" class="mw-redirect">statistical thermodynamics</a> the most general formula for the thermodynamic <a href="http://en.wikipedia.org/wiki/Entropy" title="Entropy">entropy</a> <i>S</i> of a <a href="http://en.wikipedia.org/wiki/Thermodynamic_system" title="Thermodynamic system">thermodynamic system</a> is the <a href="http://en.wikipedia.org/wiki/Gibbs_entropy" title="Gibbs entropy" class="mw-redirect">Gibbs entropy</a>,</p>
<dl>
<dd><img class="mwe-math-fallback-image-inline tex" alt="S = - k_\mathrm{B} \sum p_i \ln p_i \," src="./Entropy (information theory) - Wikipedia, the free encyclopedia_files/56c88b14e24da21ae1ab01f93e4e7d6c.png"></dd>
</dl>
<p>where <i>k</i><sub>B</sub> is the <a href="http://en.wikipedia.org/wiki/Boltzmann_constant" title="Boltzmann constant">Boltzmann constant</a>, and <i>p<sub>i</sub></i> is the probability of a microstate. The Gibbs entropy was defined by <a href="http://en.wikipedia.org/wiki/J._Willard_Gibbs" title="J. Willard Gibbs" class="mw-redirect">J. Willard Gibbs</a> in 1878 after earlier work by <a href="http://en.wikipedia.org/wiki/Ludwig_Boltzmann" title="Ludwig Boltzmann">Boltzmann</a> (1872).<sup id="cite_ref-9" class="reference"><a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29#cite_note-9"><span>[</span>9<span>]</span></a></sup></p>
<p>The Gibbs entropy translates over almost unchanged into the world of <a href="http://en.wikipedia.org/wiki/Quantum_physics" title="Quantum physics">quantum physics</a> to give the <a href="http://en.wikipedia.org/wiki/Von_Neumann_entropy" title="Von Neumann entropy">von Neumann entropy</a>, introduced by <a href="http://en.wikipedia.org/wiki/John_von_Neumann" title="John von Neumann">John von Neumann</a> in 1927,</p>
<dl>
<dd><img class="mwe-math-fallback-image-inline tex" alt="S = - k_\mathrm{B} \,{\rm Tr}(\rho \ln \rho) \," src="./Entropy (information theory) - Wikipedia, the free encyclopedia_files/7cabb27d3b0b24f24012e48bcf96de6d.png"></dd>
</dl>
<p>where ρ is the <a href="http://en.wikipedia.org/wiki/Density_matrix" title="Density matrix">density matrix</a> of the quantum mechanical system and Tr is the <a href="http://en.wikipedia.org/wiki/Trace_(linear_algebra)" title="Trace (linear algebra)">trace</a>.</p>
<p>At an everyday practical level the links between information entropy and thermodynamic entropy are not evident. Physicists and chemists are apt to be more interested in <i>changes</i> in entropy as a system spontaneously evolves away from its initial conditions, in accordance with the <a href="http://en.wikipedia.org/wiki/Second_law_of_thermodynamics" title="Second law of thermodynamics">second law of thermodynamics</a>, rather than an unchanging probability distribution. And, as the minuteness of <a href="http://en.wikipedia.org/wiki/Boltzmann%27s_constant" title="Boltzmann&#39;s constant" class="mw-redirect">Boltzmann's constant</a> <i>k<sub>B</sub></i> indicates, the changes in <i>S</i>/<i>k<sub>B</sub></i> for even tiny amounts of substances in chemical and physical processes represent amounts of entropy that are extremely large compared to anything in data compression or signal processing. Furthermore, in classical thermodynamics the entropy is defined in terms of macroscopic measurements and makes no reference to any probability distribution, which is central to the definition of information entropy.</p>
<p>At a multidisciplinary level, however, connections can be made between thermodynamic and informational entropy, although it took many years in the development of the theories of statistical mechanics and information theory to make the relationship fully apparent. In fact, in the view of <a href="http://en.wikipedia.org/wiki/Edwin_Thompson_Jaynes" title="Edwin Thompson Jaynes">Jaynes</a> (1957), thermodynamic entropy, as explained by <a href="http://en.wikipedia.org/wiki/Statistical_mechanics" title="Statistical mechanics">statistical mechanics</a>, should be seen as an <i>application</i> of Shannon's information theory: the thermodynamic entropy is interpreted as being proportional to the amount of further Shannon information needed to define the detailed microscopic state of the system, that remains uncommunicated by a description solely in terms of the macroscopic variables of classical thermodynamics, with the constant of proportionality being just the <a href="http://en.wikipedia.org/wiki/Boltzmann_constant" title="Boltzmann constant">Boltzmann constant</a>. For example, adding heat to a system increases its thermodynamic entropy because it increases the number of possible microscopic states of the system that are consistent with the measurable values of its macroscopic variables, thus making any complete state description longer. (See article: <i><a href="http://en.wikipedia.org/wiki/Maximum_entropy_thermodynamics" title="Maximum entropy thermodynamics">maximum entropy thermodynamics</a></i>). <a href="http://en.wikipedia.org/wiki/Maxwell%27s_demon" title="Maxwell&#39;s demon">Maxwell's demon</a> can (hypothetically) reduce the thermodynamic entropy of a system by using information about the states of individual molecules; but, as <a href="http://en.wikipedia.org/wiki/Rolf_Landauer" title="Rolf Landauer">Landauer</a> (from 1961) and co-workers have shown, to function the demon himself must increase thermodynamic entropy in the process, by at least the amount of Shannon information he proposes to first acquire and store; and so the total thermodynamic entropy does not decrease (which resolves the paradox). <a href="http://en.wikipedia.org/wiki/Landauer%27s_principle" title="Landauer&#39;s principle">Landauer's principle</a> has implications on the amount of heat a computer must dissipate to process a given amount of information, though modern computers are nowhere near the efficiency limit.</p>
<h3><span class="mw-headline" id="Entropy_as_information_content">Entropy as information content</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="http://en.wikipedia.org/w/index.php?title=Entropy_(information_theory)&action=edit&section=7" title="Edit section: Entropy as information content">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<div class="hatnote relarticle mainarticle">Main article: <a href="http://en.wikipedia.org/wiki/Shannon%27s_source_coding_theorem" title="Shannon&#39;s source coding theorem">Shannon's source coding theorem</a></div>
<p>Entropy is defined in the context of a probabilistic model. Independent fair coin flips have an entropy of 1 bit per flip. A source that always generates a long string of B's has an entropy of 0, since the next character will always be a 'B'.</p>
<p>The entropy rate of a data source means the average number of <a href="http://en.wikipedia.org/wiki/Bit" title="Bit">bits</a> per symbol needed to encode it. Shannon's experiments with human predictors show an information rate between 0.6 and 1.3 bits per character in English;<sup id="cite_ref-10" class="reference"><a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29#cite_note-10"><span>[</span>10<span>]</span></a></sup> the <a href="http://en.wikipedia.org/wiki/PPM_compression_algorithm" title="PPM compression algorithm" class="mw-redirect">PPM compression algorithm</a> can achieve a compression ratio of 1.5 bits per character in English text.</p>
<p>From the preceding example, note the following points:</p>
<ol>
<li>The amount of entropy is not always an integer number of bits.</li>
<li>Many data bits may not convey information. For example, data structures often store information redundantly, or have identical sections regardless of the information in the data structure.</li>
</ol>
<p>Shannon's definition of entropy, when applied to an information source, can determine the minimum channel capacity required to reliably transmit the source as encoded binary digits (see caveat below in italics). The formula can be derived by calculating the mathematical expectation of the <i>amount of information</i> contained in a digit from the information source. <i>See also</i> <a href="http://en.wikipedia.org/wiki/Shannon-Hartley_theorem" title="Shannon-Hartley theorem" class="mw-redirect">Shannon-Hartley theorem</a>.</p>
<p>Shannon's entropy measures the information contained in a message as opposed to the portion of the message that is determined (or predictable). <i>Examples of the latter include redundancy in language structure or statistical properties relating to the occurrence frequencies of letter or word pairs, triplets etc.</i> See <a href="http://en.wikipedia.org/wiki/Markov_chain" title="Markov chain">Markov chain</a>.</p>
<h3><span class="mw-headline" id="Data_compression">Data compression</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="http://en.wikipedia.org/w/index.php?title=Entropy_(information_theory)&action=edit&section=8" title="Edit section: Data compression">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<div class="hatnote relarticle mainarticle">Main article: <a href="http://en.wikipedia.org/wiki/Data_compression" title="Data compression">Data compression</a></div>
<p>Entropy effectively bounds the performance of the strongest lossless compression possible, which can be realized in theory by using the <a href="http://en.wikipedia.org/wiki/Typical_set" title="Typical set">typical set</a> or in practice using <a href="http://en.wikipedia.org/wiki/Huffman_coding" title="Huffman coding">Huffman</a>, <a href="http://en.wikipedia.org/wiki/LZW" title="LZW" class="mw-redirect">Lempel-Ziv</a> or <a href="http://en.wikipedia.org/wiki/Arithmetic_coding" title="Arithmetic coding">arithmetic coding</a>. The performance of existing data compression algorithms is often used as a rough estimate of the entropy of a block of data.<sup id="cite_ref-11" class="reference"><a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29#cite_note-11"><span>[</span>11<span>]</span></a></sup><sup id="cite_ref-12" class="reference"><a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29#cite_note-12"><span>[</span>12<span>]</span></a></sup> See also <a href="http://en.wikipedia.org/wiki/Kolmogorov_complexity" title="Kolmogorov complexity">Kolmogorov complexity</a>. In practice, compression algorithms deliberately include some judicious redundancy in the form of <a href="http://en.wikipedia.org/wiki/Checksum" title="Checksum">checksums</a> to protect against errors.</p>
<h3><span class="mw-headline" id="World.27s_technological_capacity_to_store_and_communicate_entropic_information">World's technological capacity to store and communicate entropic information</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="http://en.wikipedia.org/w/index.php?title=Entropy_(information_theory)&action=edit&section=9" title="Edit section: World&#39;s technological capacity to store and communicate entropic information">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>A 2011 study in <i><a href="http://en.wikipedia.org/wiki/Science_(journal)" title="Science (journal)">Science</a></i> estimates the world's technological capacity to store and communicate optimally compressed information normalized on the most effective compression algorithms available in the year 2007, therefore estimating the entropy of the technologically available sources.<sup id="cite_ref-HilbertLopez2011_13-0" class="reference"><a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29#cite_note-HilbertLopez2011-13"><span>[</span>13<span>]</span></a></sup></p>
<table class="wikitable">
<caption>All figures in entropically compressed <a href="http://en.wikipedia.org/wiki/Exabytes" title="Exabytes" class="mw-redirect">exabytes</a></caption>
<tbody><tr>
<th>Type of Information</th>
<th>1986</th>
<th>2007</th>
<th>Increase</th>
</tr>
<tr>
<td>Storage</td>
<td>2.6</td>
<td>295</td>
<td>113.5x</td>
</tr>
<tr>
<td>Broadcast</td>
<td>432</td>
<td>1900</td>
<td>4.398x</td>
</tr>
<tr>
<td>Telecommunications</td>
<td>0.281</td>
<td>65</td>
<td>231.3x</td>
</tr>
</tbody></table>
<p>The authors estimate humankind technological capacity to store information (fully entropically compressed) in 1986 and again in 2007. They break the information into three categories - To store information on a medium, to receive information through a one-way <a href="http://en.wikipedia.org/wiki/Broadcast" title="Broadcast" class="mw-redirect">broadcast</a> networks, to exchange information through two-way <a href="http://en.wikipedia.org/wiki/Telecommunication" title="Telecommunication">telecommunication</a> networks.<sup id="cite_ref-HilbertLopez2011_13-1" class="reference"><a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29#cite_note-HilbertLopez2011-13"><span>[</span>13<span>]</span></a></sup></p>
<h3><span class="mw-headline" id="Limitations_of_entropy_as_information_content">Limitations of entropy as information content</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="http://en.wikipedia.org/w/index.php?title=Entropy_(information_theory)&action=edit&section=10" title="Edit section: Limitations of entropy as information content">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>There are a number of entropy-related concepts that mathematically quantify information content in some way:</p>
<ul>
<li>the <b><a href="http://en.wikipedia.org/wiki/Self-information" title="Self-information">self-information</a></b> of an individual message or symbol taken from a given probability distribution,</li>
<li>the <b>entropy</b> of a given probability distribution of messages or symbols, and</li>
<li>the <b><a href="http://en.wikipedia.org/wiki/Entropy_rate" title="Entropy rate">entropy rate</a></b> of a <a href="http://en.wikipedia.org/wiki/Stochastic_process" title="Stochastic process">stochastic process</a>.</li>
</ul>
<p>(The "rate of self-information" can also be defined for a particular sequence of messages or symbols generated by a given stochastic process: this will always be equal to the entropy rate in the case of a <a href="http://en.wikipedia.org/wiki/Stationary_process" title="Stationary process">stationary process</a>.) Other <a href="http://en.wikipedia.org/wiki/Quantities_of_information" title="Quantities of information">quantities of information</a> are also used to compare or relate different sources of information.</p>
<p>It is important not to confuse the above concepts. Often it is only clear from context which one is meant. For example, when someone says that the "entropy" of the English language is about 1 bit per character, they are actually modeling the English language as a stochastic process and talking about its entropy <i>rate</i>.</p>
<p>Although entropy is often used as a characterization of the information content of a data source, this information content is not absolute: it depends crucially on the probabilistic model. A source that always generates the same symbol has an <a href="http://en.wikipedia.org/wiki/Entropy_rate" title="Entropy rate">entropy rate</a> of 0, but the definition of what a symbol is depends on the alphabet. Consider a source that produces the string ABABABABAB... in which A is always followed by B and vice versa. If the probabilistic model considers individual letters as <a href="http://en.wikipedia.org/wiki/Statistical_independence" title="Statistical independence" class="mw-redirect">independent</a>, the entropy rate of the sequence is 1 bit per character. But if the sequence is considered as "AB AB AB AB AB..." with symbols as two-character blocks, then the entropy rate is 0 bits per character.</p>
<p>However, if we use very large blocks, then the estimate of per-character entropy rate may become artificially low. This is because in reality, the probability distribution of the sequence is not knowable exactly; it is only an estimate. For example, suppose one considers the text of every book ever published as a sequence, with each symbol being the text of a complete book. If there are <i>N</i> published books, and each book is only published once, the estimate of the probability of each book is 1/<i>N</i>, and the entropy (in bits) is −log<sub>2</sub>(1/<i>N</i>) = log<sub>2</sub>(<i>N</i>). As a practical code, this corresponds to assigning each book a <a href="http://en.wikipedia.org/wiki/ISBN" title="ISBN" class="mw-redirect">unique identifier</a> and using it in place of the text of the book whenever one wants to refer to the book. This is enormously useful for talking about books, but it is not so useful for characterizing the information content of an individual book, or of language in general: it is not possible to reconstruct the book from its identifier without knowing the probability distribution, that is, the complete text of all the books. The key idea is that the complexity of the probabilistic model must be considered. <a href="http://en.wikipedia.org/wiki/Kolmogorov_complexity" title="Kolmogorov complexity">Kolmogorov complexity</a> is a theoretical generalization of this idea that allows the consideration of the information content of a sequence independent of any particular probability model; it considers the shortest <a href="http://en.wikipedia.org/wiki/Computer_program" title="Computer program">program</a> for a <a href="http://en.wikipedia.org/wiki/Universal_computer" title="Universal computer" class="mw-redirect">universal computer</a> that outputs the sequence. A code that achieves the entropy rate of a sequence for a given model, plus the codebook (i.e. the probabilistic model), is one such program, but it may not be the shortest.</p>
<p>For example, the Fibonacci sequence is 1, 1, 2, 3, 5, 8, 13, ... . Treating the sequence as a message and each number as a symbol, there are almost as many symbols as there are characters in the message, giving an entropy of approximately log<sub>2</sub>(<i>n</i>). So the first 128 symbols of the Fibonacci sequence has an entropy of approximately 7 bits/symbol. However, the sequence can be expressed using a formula [F(<i>n</i>) = F(<i>n</i>−1) + F(<i>n</i>−2) for <i>n</i>={3,4,5,...}, F(1)=1, F(2)=1] and this formula has a much lower entropy and applies to any length of the Fibonacci sequence.</p>
<h3><span class="mw-headline" id="Limitations_of_entropy_as_a_measure_of_unpredictability">Limitations of entropy as a measure of unpredictability</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="http://en.wikipedia.org/w/index.php?title=Entropy_(information_theory)&action=edit&section=11" title="Edit section: Limitations of entropy as a measure of unpredictability">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>In <a href="http://en.wikipedia.org/wiki/Cryptanalysis" title="Cryptanalysis">cryptanalysis</a>, entropy is often roughly used as a measure of the unpredictability of a cryptographic key. For example, a 128-bit key that is randomly generated has 128 bits of entropy. It takes (on average) <img class="mwe-math-fallback-image-inline tex" alt="2^{128-1}" src="./Entropy (information theory) - Wikipedia, the free encyclopedia_files/2d1fdc985a090358026bafc8c9c71027.png"> guesses to break by brute force. If the key's first digit is 0, and the others random, then the entropy is 127 bits, and it takes (on average) <img class="mwe-math-fallback-image-inline tex" alt="2^{127-1}" src="./Entropy (information theory) - Wikipedia, the free encyclopedia_files/9194cb736f4a7d44a6f704f6f0c4baed.png"> guesses.</p>
<p>However, entropy fails to capture the number of guesses required if the possible keys are not of equal probability.<sup id="cite_ref-14" class="reference"><a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29#cite_note-14"><span>[</span>14<span>]</span></a></sup><sup id="cite_ref-15" class="reference"><a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29#cite_note-15"><span>[</span>15<span>]</span></a></sup> If the key is half the time "password" and half the time a true random 128-bit key, then the entropy is approximately 65 bits. Yet half the time the key may be guessed on the first try, if your first guess is "password", and on average, it takes around <img class="mwe-math-fallback-image-inline tex" alt="2^{126}" src="./Entropy (information theory) - Wikipedia, the free encyclopedia_files/0a6ec2f02951c6a821e14d177704044a.png"> guesses (not <img class="mwe-math-fallback-image-inline tex" alt="2^{65-1}" src="./Entropy (information theory) - Wikipedia, the free encyclopedia_files/c545b37ff46c957bc921d947b8ff872e.png">) to break this password.</p>
<p>Similarly, consider a 1000000-digit binary <a href="http://en.wikipedia.org/wiki/One-time_pad" title="One-time pad">one-time pad</a>. If the pad has 1000000 bits of entropy, it is perfect. If the pad has 999999 bits of entropy, evenly distributed (each individual bit of the pad having 0.999999 bits of entropy) it may still be considered very good. But if the pad has 999999 bits of entropy, where the first digit is fixed and the remaining 999999 digits are perfectly random, then the first digit of the ciphertext will not be encrypted at all.</p>
<h3><span class="mw-headline" id="Data_as_a_Markov_process">Data as a Markov process</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="http://en.wikipedia.org/w/index.php?title=Entropy_(information_theory)&action=edit&section=12" title="Edit section: Data as a Markov process">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>A common way to define entropy for text is based on the <a href="http://en.wikipedia.org/wiki/Markov_model" title="Markov model">Markov model</a> of text. For an order-0 source (each character is selected independent of the last characters), the binary entropy is:</p>
<dl>
<dd><img class="mwe-math-fallback-image-inline tex" alt="H(\mathcal{S}) = - \sum p_i \log_2 p_i, \,\!" src="./Entropy (information theory) - Wikipedia, the free encyclopedia_files/ca29f373129e5aa2a5d1ed24e51920dc.png"></dd>
</dl>
<p>where <i>p</i><sub><i>i</i></sub> is the probability of <i>i</i>. For a first-order <a href="http://en.wikipedia.org/wiki/Markov_source" title="Markov source" class="mw-redirect">Markov source</a> (one in which the probability of selecting a character is dependent only on the immediately preceding character), the <b><a href="http://en.wikipedia.org/wiki/Entropy_rate" title="Entropy rate">entropy rate</a></b> is:</p>
<dl>
<dd><img class="mwe-math-fallback-image-inline tex" alt="H(\mathcal{S}) = - \sum_i p_i \sum_j  \  p_i (j) \log_2 p_i (j), \,\!" src="./Entropy (information theory) - Wikipedia, the free encyclopedia_files/23e272a12e0592a866901203df55457c.png"><sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">[<i><a href="http://en.wikipedia.org/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (April 2013)">citation needed</span></a></i>]</sup></dd>
</dl>
<p>where <i>i</i> is a <b>state</b> (certain preceding characters) and <img class="mwe-math-fallback-image-inline tex" alt="p_i(j)" src="./Entropy (information theory) - Wikipedia, the free encyclopedia_files/ff80e0c579eff2d8e4c51db5775b992d.png"> is the probability of <i>j</i> given <i>i</i> as the previous character.</p>
<p>For a second order Markov source, the entropy rate is</p>
<dl>
<dd><img class="mwe-math-fallback-image-inline tex" alt="H(\mathcal{S}) = -\sum_i p_i \sum_j p_i(j) \sum_k p_{i,j}(k)\ \log_2 \  p_{i,j}(k). \,\!" src="./Entropy (information theory) - Wikipedia, the free encyclopedia_files/a63d721bd4b3cd3ed586f8f45b3ede51.png"></dd>
</dl>
<h3><span class="mw-headline" id="b-ary_entropy"><i>b</i>-ary entropy</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="http://en.wikipedia.org/w/index.php?title=Entropy_(information_theory)&action=edit&section=13" title="Edit section: b-ary entropy">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>In general the <b><i>b</i>-ary entropy</b> of a source <img class="mwe-math-fallback-image-inline tex" alt="\mathcal{S}" src="./Entropy (information theory) - Wikipedia, the free encyclopedia_files/3791545a70a6e462451c97ad925d43a4.png"> = (<i>S</i>,<i>P</i>) with <a href="http://en.wikipedia.org/w/index.php?title=Source_alphabet&action=edit&redlink=1" class="new" title="Source alphabet (page does not exist)">source alphabet</a> <i>S</i> = {<i>a</i><sub>1</sub>, ..., <i>a<sub>n</sub></i>} and <a href="http://en.wikipedia.org/wiki/Discrete_probability_distribution" title="Discrete probability distribution" class="mw-redirect">discrete probability distribution</a> <i>P</i> = {<i>p</i><sub>1</sub>, ..., <i>p<sub>n</sub></i>} where <i>p<sub>i</sub></i> is the probability of <i>a<sub>i</sub></i> (say <i>p<sub>i</sub></i> = <i>p</i>(<i>a<sub>i</sub></i>)) is defined by:</p>
<dl>
<dd><img class="mwe-math-fallback-image-inline tex" alt=" H_b(\mathcal{S}) = - \sum_{i=1}^n p_i \log_b p_i, \,\!" src="./Entropy (information theory) - Wikipedia, the free encyclopedia_files/202ea0fca52e0f9154d56c29d7778fa6.png"></dd>
</dl>
<p>Note: the <i>b</i> in "<i>b</i>-ary entropy" is the number of different symbols of the <i>ideal alphabet</i> used as a standard yardstick to measure source alphabets. In information theory, two symbols are <a href="http://en.wikipedia.org/wiki/Necessary_and_sufficient" title="Necessary and sufficient" class="mw-redirect">necessary and sufficient</a> for an alphabet to encode information. Therefore, the default is to let <i>b</i> = 2 ("binary entropy"). Thus, the entropy of the source alphabet, with its given empiric probability distribution, is a number equal to the number (possibly fractional) of symbols of the "ideal alphabet", with an optimal probability distribution, necessary to encode for each symbol of the source alphabet. Also note that "optimal probability distribution" here means a <a href="http://en.wikipedia.org/wiki/Uniform_distribution_(discrete)" title="Uniform distribution (discrete)">uniform distribution</a>: a source alphabet with <i>n</i> symbols has the highest possible entropy (for an alphabet with <i>n</i> symbols) when the probability distribution of the alphabet is uniform. This optimal entropy turns out to be log<sub><i>b</i></sub>(<i>n</i>).</p>
<h2><span class="mw-headline" id="Efficiency">Efficiency</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="http://en.wikipedia.org/w/index.php?title=Entropy_(information_theory)&action=edit&section=14" title="Edit section: Efficiency">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>A source alphabet with non-uniform distribution will have less entropy than if those symbols had uniform distribution (i.e. the "optimized alphabet"). This deficiency in entropy can be expressed as a ratio called efficiency<sup class="noprint Inline-Template" style="white-space:nowrap;">[<i><a href="http://en.wikipedia.org/wiki/Wikipedia:Quotations" title="Wikipedia:Quotations"><span title="The text near this tag needs citation. (July 2014)">this quote needs a citation</span></a></i>]</sup>:</p>
<dl>
<dd><img class="mwe-math-fallback-image-inline tex" alt="\eta(X) = -\sum_{i=1}^n \frac{p(x_i) \log_b (p(x_i))}{\log_b (n)}" src="./Entropy (information theory) - Wikipedia, the free encyclopedia_files/b3d921e699a8b783c1444030c3c4f207.png"><sup class="noprint Inline-Template" style="white-space:nowrap;">[<i><a href="http://en.wikipedia.org/wiki/Wikipedia:Please_clarify" title="Wikipedia:Please clarify"><span title="The text near this tag may need clarification or removal of jargon. (July 2014)">clarification needed</span></a></i>]</sup></dd>
</dl>
<p>Efficiency has utility in quantifying the effective use of a communications channel. This formulation is also referred to as the normalized entropy, as the entropy is divided by the maximum entropy <img class="mwe-math-fallback-image-inline tex" alt="{\log_b (n)}" src="./Entropy (information theory) - Wikipedia, the free encyclopedia_files/ffd8ef3eae1d07b52a14e1a9ed04a7c7.png">.</p>
<h2><span class="mw-headline" id="Characterization">Characterization</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="http://en.wikipedia.org/w/index.php?title=Entropy_(information_theory)&action=edit&section=15" title="Edit section: Characterization">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Shannon entropy is <a href="http://en.wikipedia.org/wiki/Characterization_(mathematics)" title="Characterization (mathematics)">characterized</a> by a small number of criteria, listed below. Any definition of entropy satisfying these assumptions has the form</p>
<dl>
<dd><img class="mwe-math-fallback-image-inline tex" alt="-K\sum_{i=1}^np_i\log (p_i)" src="./Entropy (information theory) - Wikipedia, the free encyclopedia_files/d4ba18355fab2cd42a15f1c31469ccf5.png"></dd>
</dl>
<p>where <i>K</i> is a constant corresponding to a choice of measurement units.</p>
<p>In the following, <i>p<sub>i</sub></i> = Pr (<i>X</i> = <i>x<sub>i</sub></i>) and <img class="mwe-math-fallback-image-inline tex" alt="H_n(p_1,\ldots,p_n)=H(X)" src="./Entropy (information theory) - Wikipedia, the free encyclopedia_files/b5d6ad4a679d927a41c1d972bd912383.png">.</p>
<h3><span class="mw-headline" id="Continuity">Continuity</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="http://en.wikipedia.org/w/index.php?title=Entropy_(information_theory)&action=edit&section=16" title="Edit section: Continuity">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>The measure should be <a href="http://en.wikipedia.org/wiki/Continuous_function" title="Continuous function">continuous</a>, so that changing the values of the probabilities by a very small amount should only change the entropy by a small amount.</p>
<h3><span class="mw-headline" id="Symmetry">Symmetry</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="http://en.wikipedia.org/w/index.php?title=Entropy_(information_theory)&action=edit&section=17" title="Edit section: Symmetry">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>The measure should be unchanged if the outcomes <i>x<sub>i</sub></i> are re-ordered.</p>
<dl>
<dd><img class="mwe-math-fallback-image-inline tex" alt="H_n\left(p_1, p_2, \ldots \right) = H_n\left(p_2, p_1, \ldots \right)" src="./Entropy (information theory) - Wikipedia, the free encyclopedia_files/3afd0f61009766137def9924a2e5556b.png"> etc.</dd>
</dl>
<h3><span class="mw-headline" id="Maximum">Maximum</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="http://en.wikipedia.org/w/index.php?title=Entropy_(information_theory)&action=edit&section=18" title="Edit section: Maximum">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>The measure should be maximal if all the outcomes are equally likely (uncertainty is highest when all possible events are equiprobable).</p>
<dl>
<dd><img class="mwe-math-fallback-image-inline tex" alt=" H_n(p_1,\ldots,p_n) \le H_n\left(\frac{1}{n}, \ldots, \frac{1}{n}\right) = \log_b (n)." src="./Entropy (information theory) - Wikipedia, the free encyclopedia_files/cabda3ce6963f9bfbee00a22674a400b.png"></dd>
</dl>
<p>For equiprobable events the entropy should increase with the number of outcomes.</p>
<dl>
<dd><img class="mwe-math-fallback-image-inline tex" alt="H_n\bigg(\underbrace{\frac{1}{n}, \ldots, \frac{1}{n}}_{n}\bigg) = \log_b(n) &lt; \log_b (n+1) = H_{n+1}\bigg(\underbrace{\frac{1}{n+1}, \ldots, \frac{1}{n+1}}_{n+1}\bigg)." src="./Entropy (information theory) - Wikipedia, the free encyclopedia_files/07aeb69f268e95be1350f895443d0343.png"></dd>
</dl>
<h3><span class="mw-headline" id="Additivity">Additivity</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="http://en.wikipedia.org/w/index.php?title=Entropy_(information_theory)&action=edit&section=19" title="Edit section: Additivity">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>The amount of entropy should be independent of how the process is regarded as being divided into parts.</p>
<p>This last functional relationship characterizes the entropy of a system with sub-systems. It demands that the entropy of a system can be calculated from the entropies of its sub-systems if the interactions between the sub-systems are known.</p>
<p>Given an ensemble of <i>n</i> uniformly distributed elements that are divided into <i>k</i> boxes (sub-systems) with <i>b</i><sub>1</sub>, ..., <i>b<sub>k</sub></i> elements each, the entropy of the whole ensemble should be equal to the sum of the entropy of the system of boxes and the individual entropies of the boxes, each weighted with the probability of being in that particular box.</p>
<p>For <a href="http://en.wikipedia.org/wiki/Positive_integers" title="Positive integers" class="mw-redirect">positive integers</a> <i>b<sub>i</sub></i> where <i>b</i><sub>1</sub> + ... + <i>b<sub>k</sub></i> = <i>n</i>,</p>
<dl>
<dd><img class="mwe-math-fallback-image-inline tex" alt="H_n\left(\frac{1}{n}, \ldots, \frac{1}{n}\right) = H_k\left(\frac{b_1}{n}, \ldots, \frac{b_k}{n}\right) + \sum_{i=1}^k \frac{b_i}{n} \, H_{b_i}\left(\frac{1}{b_i}, \ldots, \frac{1}{b_i}\right)." src="./Entropy (information theory) - Wikipedia, the free encyclopedia_files/932bdd0c9854b571a7461189283eaabc.png"></dd>
</dl>
<p>Choosing <i>k</i>&nbsp;=&nbsp;<i>n</i>, <i>b</i><sub>1</sub> = ... = <i>b<sub>n</sub></i> = 1 this implies that the entropy of a certain outcome is zero: <i>H</i><sub>1</sub>(1) = 0. This implies that the efficiency of a source alphabet with <i>n</i> symbols can be defined simply as being equal to its <i>n</i>-ary entropy. See also <a href="http://en.wikipedia.org/wiki/Redundancy_(information_theory)" title="Redundancy (information theory)">Redundancy (information theory)</a>.</p>
<h2><span class="mw-headline" id="Further_properties">Further properties</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="http://en.wikipedia.org/w/index.php?title=Entropy_(information_theory)&action=edit&section=20" title="Edit section: Further properties">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>The Shannon entropy satisfies the following properties, for some of which it is useful to interpret entropy as the amount of information learned (or uncertainty eliminated) by revealing the value of a random variable <i>X</i>:</p>
<ul>
<li>Adding or removing an event with probability zero does not contribute to the entropy:</li>
</ul>
<dl>
<dd>
<dl>
<dd><img class="mwe-math-fallback-image-inline tex" alt="H_{n+1}(p_1,\ldots,p_n,0) = H_n(p_1,\ldots,p_n)" src="./Entropy (information theory) - Wikipedia, the free encyclopedia_files/91634d31e2670e8e826aeed172583eb0.png">.</dd>
</dl>
</dd>
</dl>
<ul>
<li>It can be confirmed using the <a href="http://en.wikipedia.org/wiki/Jensen_inequality" title="Jensen inequality" class="mw-redirect">Jensen inequality</a> that</li>
</ul>
<dl>
<dd>
<dl>
<dd><img class="mwe-math-fallback-image-inline tex" alt="H(X) = \operatorname{E}\left[\log_b \left( \frac{1}{p(X)}\right) \right] \leq \log_b \left( \operatorname{E}\left[ \frac{1}{p(X)} \right] \right) = \log_b(n)" src="./Entropy (information theory) - Wikipedia, the free encyclopedia_files/d0a1dd074f1813294ffc60aa5ef47fb5.png">.</dd>
</dl>
</dd>
<dd>This maximal entropy of log<sub><i>b</i></sub>(<i>n</i>) is effectively attained by a source alphabet having a uniform probability distribution: uncertainty is maximal when all possible events are equiprobable.</dd>
</dl>
<ul>
<li>The entropy or the amount of information revealed by evaluating (<i>X</i>,<i>Y</i>) (that is, evaluating <i>X</i> and <i>Y</i> simultaneously) is equal to the information revealed by conducting two consecutive experiments: first evaluating the value of <i>Y</i>, then revealing the value of <i>X</i> given that you know the value of <i>Y</i>. This may be written as</li>
</ul>
<dl>
<dd>
<dl>
<dd><img class="mwe-math-fallback-image-inline tex" alt=" H[(X,Y)]=H(X|Y)+H(Y)=H(Y|X)+H(X)." src="./Entropy (information theory) - Wikipedia, the free encyclopedia_files/ad4aec19545a1e3c668f9558d38f4dce.png"></dd>
</dl>
</dd>
</dl>
<ul>
<li>If <i>Y=f(X)</i> where <i>f</i> is deterministic, then <i>H</i>(<i>f</i>(<i>X</i>)|<i>X</i>) = 0. Applying the previous formula to <i>H</i>(<i>X</i>, <i>f</i>(<i>X</i>)) yields</li>
</ul>
<dl>
<dd>
<dl>
<dd><img class="mwe-math-fallback-image-inline tex" alt=" H(X)+H(f(X)|X)=H(f(X))+H(X|f(X))," src="./Entropy (information theory) - Wikipedia, the free encyclopedia_files/1859f792e2f41bc40a067321284b0650.png"></dd>
</dl>
</dd>
<dd>so <i>H</i>(<i>f</i>(<i>X</i>)) ≤ <i>H</i>(<i>X</i>), thus the entropy of a variable can only decrease when the latter is passed through a deterministic function.</dd>
</dl>
<ul>
<li>If <i>X</i> and <i>Y</i> are two independent experiments, then knowing the value of <i>Y</i> doesn't influence our knowledge of the value of <i>X</i> (since the two don't influence each other by independence):</li>
</ul>
<dl>
<dd>
<dl>
<dd><img class="mwe-math-fallback-image-inline tex" alt=" H(X|Y)=H(X)." src="./Entropy (information theory) - Wikipedia, the free encyclopedia_files/6d011f73a19ebca1d045345f48766de7.png"></dd>
</dl>
</dd>
</dl>
<ul>
<li>The entropy of two simultaneous events is no more than the sum of the entropies of each individual event, and are equal if the two events are independent. More specifically, if <i>X</i> and <i>Y</i> are two random variables on the same probability space, and <i>(X,Y)</i> denotes their Cartesian product, then</li>
</ul>
<dl>
<dd>
<dl>
<dd><img class="mwe-math-fallback-image-inline tex" alt=" H[(X,Y)]\leq H(X)+H(Y)." src="./Entropy (information theory) - Wikipedia, the free encyclopedia_files/3a4d0c6ba32f90befe93ee8362587811.png"></dd>
</dl>
</dd>
</dl>
<p>Proving this mathematically follows easily from the previous two properties of entropy.</p>
<h2><span class="mw-headline" id="Extending_discrete_entropy_to_the_continuous_case">Extending discrete entropy to the continuous case</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="http://en.wikipedia.org/w/index.php?title=Entropy_(information_theory)&action=edit&section=21" title="Edit section: Extending discrete entropy to the continuous case">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<h3><span class="mw-headline" id="Differential_entropy">Differential entropy</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="http://en.wikipedia.org/w/index.php?title=Entropy_(information_theory)&action=edit&section=22" title="Edit section: Differential entropy">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<div class="hatnote relarticle mainarticle">Main article: <a href="http://en.wikipedia.org/wiki/Differential_entropy" title="Differential entropy">Differential entropy</a></div>
<p>The Shannon entropy is restricted to random variables taking discrete values. The corresponding formula for a continuous random variable with <a href="http://en.wikipedia.org/wiki/Probability_density_function" title="Probability density function">probability density function</a> <i>f(x)</i> with finite or infinite support <img class="mwe-math-fallback-image-inline tex" alt="\mathbb X" src="./Entropy (information theory) - Wikipedia, the free encyclopedia_files/208f2b85d3349015a345e6eb1c614412.png"> on the real line is defined by analogy, using the above form of the entropy as an expectation:</p>
<dl>
<dd><img class="mwe-math-fallback-image-inline tex" alt="h[f] = \operatorname{E}[-\ln (f(x))] = -\int_\mathbb X f(x) \ln (f(x))\, dx." src="./Entropy (information theory) - Wikipedia, the free encyclopedia_files/d47ae464b7e0c4192361d6cc07fad604.png"></dd>
</dl>
<p>This formula is usually referred to as the <b>continuous entropy</b>, or <a href="http://en.wikipedia.org/wiki/Differential_entropy" title="Differential entropy">differential entropy</a>. A precursor of the continuous entropy <i>h</i>[<i>f</i>] is the expression for the functional <i>H</i> in the <a href="http://en.wikipedia.org/wiki/H-theorem" title="H-theorem">H-theorem</a> of <a href="http://en.wikipedia.org/wiki/Boltzmann" title="Boltzmann" class="mw-redirect">Boltzmann</a>.</p>
<p>Although the analogy between both functions is suggestive, the following question must be set: is the differential entropy a valid extension of the Shannon discrete entropy? Differential entropy lacks a number of properties that the Shannon discrete entropy has&nbsp;– it can even be negative&nbsp;– and thus corrections have been suggested, notably <a href="http://en.wikipedia.org/wiki/Limiting_density_of_discrete_points" title="Limiting density of discrete points">limiting density of discrete points</a>.</p>
<p>To answer this question, we must establish a connection between the two functions:</p>
<p>We wish to obtain a generally finite measure as the <a href="http://en.wikipedia.org/wiki/Bin_size" title="Bin size" class="mw-redirect">bin size</a> goes to zero. In the discrete case, the bin size is the (implicit) width of each of the <i>n</i> (finite or infinite) bins whose probabilities are denoted by <i>p<sub>n</sub></i>. As we generalize to the continuous domain, we must make this width explicit.</p>
<p>To do this, start with a continuous function <i>f</i> discretized into bins of size <img class="mwe-math-fallback-image-inline tex" alt="\Delta" src="./Entropy (information theory) - Wikipedia, the free encyclopedia_files/659d23f0ed16cdb87b1d41c7b58b52f4.png">. By the mean-value theorem there exists a value <i>x<sub>i</sub></i> in each bin such that</p>
<dl>
<dd><img class="mwe-math-fallback-image-inline tex" alt="f(x_i) \Delta = \int_{i\Delta}^{(i+1)\Delta} f(x)\, dx" src="./Entropy (information theory) - Wikipedia, the free encyclopedia_files/daf8aedf3eef9d77e23f39bc67feb716.png"></dd>
</dl>
<p>and thus the integral of the function <i>f</i> can be approximated (in the Riemannian sense) by</p>
<dl>
<dd><img class="mwe-math-fallback-image-inline tex" alt="\int_{-\infty}^{\infty} f(x)\, dx = \lim_{\Delta \to 0} \sum_{i = -\infty}^{\infty} f(x_i) \Delta" src="./Entropy (information theory) - Wikipedia, the free encyclopedia_files/440fde8068dfc0b0e9dd3766278a9055.png"></dd>
</dl>
<p>where this limit and "bin size goes to zero" are equivalent.</p>
<p>We will denote</p>
<dl>
<dd><img class="mwe-math-fallback-image-inline tex" alt="H^{\Delta} :=- \sum_{i=-\infty}^{\infty} f(x_i)  \Delta \log \left(  f(x_i)  \Delta \right)" src="./Entropy (information theory) - Wikipedia, the free encyclopedia_files/b6fe1521f16290a9b7025c307a356ac7.png"></dd>
</dl>
<p>and expanding the logarithm, we have</p>
<dl>
<dd><img class="mwe-math-fallback-image-inline tex" alt="H^{\Delta} = - \sum_{i=-\infty}^{\infty}  f(x_i)  \Delta \log (f(x_i)) -\sum_{i=-\infty}^{\infty} f(x_i) \Delta \log (\Delta)." src="./Entropy (information theory) - Wikipedia, the free encyclopedia_files/8f472dadccf4d71f02d3651ae95e9ed8.png"></dd>
</dl>
<p>As Δ → 0, we have</p>
<dl>
<dd><img class="mwe-math-fallback-image-inline tex" alt="\begin{align}
\sum_{i=-\infty}^{\infty} f(x_i) \Delta &amp;\to \int_{-\infty}^{\infty} f(x)\, dx = 1 \\
\sum_{i=-\infty}^{\infty} f(x_i) \Delta \log (f(x_i)) &amp;\to \int_{-\infty}^{\infty} f(x) \log f(x)\, dx.
\end{align}" src="./Entropy (information theory) - Wikipedia, the free encyclopedia_files/ca768b8e96cd4a05c58aafc078edc7e4.png"></dd>
</dl>
<p>But note that log(Δ) → −∞ as Δ → 0, therefore we need a special definition of the differential or continuous entropy:</p>
<dl>
<dd><img class="mwe-math-fallback-image-inline tex" alt="h[f] = \lim_{\Delta \to 0} \left(H^{\Delta} + \log \Delta\right) = -\int_{-\infty}^{\infty} f(x) \log f(x)\,dx," src="./Entropy (information theory) - Wikipedia, the free encyclopedia_files/536793456be62eae1397a8885ce672d2.png"></dd>
</dl>
<p>which is, as said before, referred to as the <b>differential entropy</b>. This means that the differential entropy <i>is not</i> a limit of the Shannon entropy for <i>n</i> → ∞. Rather, it differs from the limit of the Shannon entropy by an infinite offset.</p>
<p>It turns out as a result that, unlike the Shannon entropy, the differential entropy is <i>not</i> in general a good measure of uncertainty or information. For example, the differential entropy can be negative; also it is not invariant under continuous co-ordinate transformations.</p>
<h3><span class="mw-headline" id="Relative_entropy">Relative entropy</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="http://en.wikipedia.org/w/index.php?title=Entropy_(information_theory)&action=edit&section=23" title="Edit section: Relative entropy">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<div class="hatnote relarticle mainarticle">Main article: <a href="http://en.wikipedia.org/wiki/Generalized_relative_entropy" title="Generalized relative entropy">Generalized relative entropy</a></div>
<p>Another useful measure of entropy that works equally well in the discrete and the continuous case is the <b>relative entropy</b> of a distribution. It is defined as the <a href="http://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence" title="Kullback–Leibler divergence">Kullback–Leibler divergence</a> from the distribution to a reference measure <i>m</i> as follows. Assume that a probability distribution <i>p</i> is <a href="http://en.wikipedia.org/wiki/Absolutely_continuous" title="Absolutely continuous" class="mw-redirect">absolutely continuous</a> with respect to a measure <i>m</i>, i.e. is of the form <i>p</i>(<i>dx</i>) = <i>f</i>(<i>x</i>)<i>m</i>(<i>dx</i>) for some non-negative <i>m</i>-integrable function <i>f</i> with <i>m</i>-integral 1, then the relative entropy can be defined as</p>
<dl>
<dd><img class="mwe-math-fallback-image-inline tex" alt="D_{\mathrm{KL}}(p \| m ) = \int \log (f(x)) p(dx) = \int f(x)\log (f(x)) m(dx) ." src="./Entropy (information theory) - Wikipedia, the free encyclopedia_files/fadb67b95c5be349a35f4a3d33bdebfe.png"></dd>
</dl>
<p>In this form the relative entropy generalises (up to change in sign) both the discrete entropy, where the measure <i>m</i> is the <a href="http://en.wikipedia.org/wiki/Counting_measure" title="Counting measure">counting measure</a>, and the differential entropy, where the measure <i>m</i> is the <a href="http://en.wikipedia.org/wiki/Lebesgue_measure" title="Lebesgue measure">Lebesgue measure</a>. If the measure <i>m</i> is itself a probability distribution, the relative entropy is non-negative, and zero if <i>p</i> = <i>m</i> as measures. It is defined for any measure space, hence coordinate independent and invariant under co-ordinate reparameterizations if one properly takes into account the transformation of the measure <i>m</i>. The relative entropy, and implicitly entropy and differential entropy, do depend on the "reference" measure <i>m</i>.</p>
<h2><span class="mw-headline" id="Use_in_combinatorics">Use in combinatorics</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="http://en.wikipedia.org/w/index.php?title=Entropy_(information_theory)&action=edit&section=24" title="Edit section: Use in combinatorics">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Entropy has become a useful quantity in <a href="http://en.wikipedia.org/wiki/Combinatorics" title="Combinatorics">combinatorics</a>.</p>
<h3><span class="mw-headline" id="Loomis-Whitney_inequality">Loomis-Whitney inequality</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="http://en.wikipedia.org/w/index.php?title=Entropy_(information_theory)&action=edit&section=25" title="Edit section: Loomis-Whitney inequality">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>A simple example of this is an alternate proof of the <a href="http://en.wikipedia.org/wiki/Loomis-Whitney_inequality" title="Loomis-Whitney inequality" class="mw-redirect">Loomis-Whitney inequality</a>: for every subset <i>A</i> ⊆ <b>Z</b><sup><i>d</i></sup>, we have</p>
<dl>
<dd><img class="mwe-math-fallback-image-inline tex" alt=" |A|^{d-1}\leq \prod_{i=1}^{d} |P_{i}(A)|" src="./Entropy (information theory) - Wikipedia, the free encyclopedia_files/5a77e51d9cc1147ae69b0ff7daee8568.png"></dd>
</dl>
<p>where <i>P<sub>i</sub></i> is the <a href="http://en.wikipedia.org/wiki/Orthogonal_projection" title="Orthogonal projection" class="mw-redirect">orthogonal projection</a> in the ith coordinate:</p>
<dl>
<dd><img class="mwe-math-fallback-image-inline tex" alt=" P_{i}(A)=\{(x_{1}, ..., x_{i-1}, x_{i+1}, ..., x_{d}) : (x_{1}, ..., x_{d})\in A\}." src="./Entropy (information theory) - Wikipedia, the free encyclopedia_files/c7c07787befe6cb52f5cff6f66509fa2.png"></dd>
</dl>
<p>The proof follows as a simple corollary of <a href="http://en.wikipedia.org/wiki/Shearer%27s_inequality" title="Shearer&#39;s inequality">Shearer's inequality</a>: if <i>X</i><sub>1</sub>, ..., <i>X<sub>d</sub></i> are random variables and <i>S</i><sub>1</sub>, ..., <i>S<sub>n</sub></i> are subsets of {1, ..., <i>d</i>} such that every integer between <i>1</i> and <i>d</i> lies in exactly <i>r</i> of these subsets, then</p>
<dl>
<dd><img class="mwe-math-fallback-image-inline tex" alt=" H[(X_{1},...,X_{d})]\leq \frac{1}{r}\sum_{i=1}^{n}H[(X_{j})_{j\in S_{i}}]" src="./Entropy (information theory) - Wikipedia, the free encyclopedia_files/31a85f9c121805adfc75f3e458e049c9.png"></dd>
</dl>
<p>where <img class="mwe-math-fallback-image-inline tex" alt=" (X_{j})_{j\in S_{i}}" src="./Entropy (information theory) - Wikipedia, the free encyclopedia_files/0fd8197d4910819dacbe3e85ad5e07e4.png"> is the Cartesian product of random variables <i>X<sub>j</sub></i> with indexes <i>j</i> in <i>S<sub>i</sub></i> (so the dimension of this vector is equal to the size of <i>S<sub>i</sub></i>).</p>
<p>We sketch how Loomis-Whitney follows from this: Indeed, let <i>X</i> be a uniformly distributed random variable with values in <i>A</i> and so that each point in <i>A</i> occurs with equal probability. Then (by the further properties of entropy mentioned above) <i>H</i>(<i>X</i>) = log|<i>A</i>|, where <i>|A|</i> denotes the cardinality of <i>A</i>. Let <i>S<sub>i</sub></i> = {1, 2, ..., <i>i</i>−1, <i>i</i>+1, ..., <i>d</i>}. The range of <img class="mwe-math-fallback-image-inline tex" alt="(X_{j})_{j\in S_{i}}" src="./Entropy (information theory) - Wikipedia, the free encyclopedia_files/0fd8197d4910819dacbe3e85ad5e07e4.png"> is contained in <i>P<sub>i</sub></i>(<i>A</i>) and hence <img class="mwe-math-fallback-image-inline tex" alt=" H[(X_{j})_{j\in S_{i}}]\leq \log |P_{i}(A)|" src="./Entropy (information theory) - Wikipedia, the free encyclopedia_files/c8a2949b3b77933bccc24b4be6e2d3d1.png">. Now use this to bound the right side of Shearer's inequality and exponentiate the opposite sides of the resulting inequality you obtain.</p>
<h3><span class="mw-headline" id="Approximation_to_binomial_coefficient">Approximation to binomial coefficient</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="http://en.wikipedia.org/w/index.php?title=Entropy_(information_theory)&action=edit&section=26" title="Edit section: Approximation to binomial coefficient">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>For integers 0 &lt; <i>k</i> &lt; <i>n</i> let <i>q</i> = <i>k</i>/<i>n</i>. Then</p>
<dl>
<dd><img class="mwe-math-fallback-image-inline tex" alt="\frac{2^{nH(q)}}{n+1} \leq \tbinom nk \leq 2^{nH(q)}," src="./Entropy (information theory) - Wikipedia, the free encyclopedia_files/6ff115fba156eea8ba3670a18935e9f3.png"></dd>
</dl>
<p>where</p>
<dl>
<dd><img class="mwe-math-fallback-image-inline tex" alt="H(q) = -q \log_2(q) - (1-q) \log_2(1-q)." src="./Entropy (information theory) - Wikipedia, the free encyclopedia_files/b8ec80d3be8d33da41fcf4aa13d0a70e.png"><sup id="cite_ref-16" class="reference"><a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29#cite_note-16"><span>[</span>16<span>]</span></a></sup></dd>
</dl>
<p>Here is a sketch proof. Note that <img class="mwe-math-fallback-image-inline tex" alt="\tbinom nk q^{qn}(1-q)^{n-nq}" src="./Entropy (information theory) - Wikipedia, the free encyclopedia_files/94512ae8768feb7b6a814f5da2a0bfd3.png"> is one term of the expression</p>
<dl>
<dd><img class="mwe-math-fallback-image-inline tex" alt="\sum_{i=0}^n \tbinom ni q^i(1-q)^{n-i} = (q + (1-q))^n = 1." src="./Entropy (information theory) - Wikipedia, the free encyclopedia_files/cd0d32326fbc6d2dd8362b19d40f5bad.png"></dd>
</dl>
<p>Rearranging gives the upper bound. For the lower bound one first shows, using some algebra, that it is the largest term in the summation. But then,</p>
<dl>
<dd><img class="mwe-math-fallback-image-inline tex" alt="\tbinom nk q^{qn}(1-q)^{n-nq} \geq \tfrac{1}{n+1}" src="./Entropy (information theory) - Wikipedia, the free encyclopedia_files/f39b136ba4c61a64f87ce4e652682ecd.png"></dd>
</dl>
<p>since there are <i>n</i>+1 terms in the summation. Rearranging gives the lower bound.</p>
<p>A nice interpretation of this is that the number of binary strings of length <i>n</i> with exactly <i>k</i> many 1's is approximately <img class="mwe-math-fallback-image-inline tex" alt="2^{nH(k/n)}" src="./Entropy (information theory) - Wikipedia, the free encyclopedia_files/8bb24584c4325d5f36d82eaf52b7e30e.png">.<sup id="cite_ref-17" class="reference"><a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29#cite_note-17"><span>[</span>17<span>]</span></a></sup></p>
<h2><span class="mw-headline" id="See_also">See also</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="http://en.wikipedia.org/w/index.php?title=Entropy_(information_theory)&action=edit&section=27" title="Edit section: See also">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div class="noprint tright portal" style="border:solid #aaa 1px;margin:0.5em 0 0.5em 1em;">
<table style="background:#f9f9f9;font-size:85%;line-height:110%;max-width:175px;">
<tbody><tr style="vertical-align:middle;">
<td style="text-align:center;"><a href="http://en.wikipedia.org/wiki/File:Fisher_iris_versicolor_sepalwidth.svg" class="image"><img alt="Portal icon" src="./Entropy (information theory) - Wikipedia, the free encyclopedia_files/32px-Fisher_iris_versicolor_sepalwidth.svg.png" width="32" height="22" class="noviewer" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/4/40/Fisher_iris_versicolor_sepalwidth.svg/48px-Fisher_iris_versicolor_sepalwidth.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/4/40/Fisher_iris_versicolor_sepalwidth.svg/64px-Fisher_iris_versicolor_sepalwidth.svg.png 2x" data-file-width="822" data-file-height="567"></a></td>
<td style="padding:0 0.2em;vertical-align:middle;font-style:italic;font-weight:bold;"><a href="http://en.wikipedia.org/wiki/Portal:Statistics" title="Portal:Statistics">Statistics portal</a></td>
</tr>
<tr style="vertical-align:middle;">
<td style="text-align:center;"><a href="http://en.wikipedia.org/wiki/File:Crypto_key.svg" class="image"><img alt="Portal icon" src="./Entropy (information theory) - Wikipedia, the free encyclopedia_files/32px-Crypto_key.svg.png" width="32" height="17" class="noviewer" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Crypto_key.svg/48px-Crypto_key.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/6/65/Crypto_key.svg/64px-Crypto_key.svg.png 2x" data-file-width="671" data-file-height="349"></a></td>
<td style="padding:0 0.2em;vertical-align:middle;font-style:italic;font-weight:bold;"><a href="http://en.wikipedia.org/wiki/Portal:Cryptography" title="Portal:Cryptography">Cryptography portal</a></td>
</tr>
</tbody></table>
</div>
<ul>
<li><a href="http://en.wikipedia.org/wiki/Conditional_entropy" title="Conditional entropy">Conditional entropy</a></li>
<li><a href="http://en.wikipedia.org/wiki/Cross_entropy" title="Cross entropy">Cross entropy</a> – is a measure of the average number of bits needed to identify an event from a set of possibilities between two probability distributions</li>
<li><a href="http://en.wikipedia.org/wiki/Entropy_(arrow_of_time)" title="Entropy (arrow of time)">Entropy (arrow of time)</a></li>
<li><a href="http://en.wikipedia.org/wiki/Entropy_encoding" title="Entropy encoding">Entropy encoding</a> – a coding scheme that assigns codes to symbols so as to match code lengths with the probabilities of the symbols.</li>
<li><a href="http://en.wikipedia.org/wiki/Entropy_estimation" title="Entropy estimation">Entropy estimation</a></li>
<li><a href="http://en.wikipedia.org/wiki/Entropy_power_inequality" title="Entropy power inequality">Entropy power inequality</a></li>
<li><a href="http://en.wikipedia.org/wiki/Entropy_rate" title="Entropy rate">Entropy rate</a></li>
<li><a href="http://en.wikipedia.org/wiki/Fisher_information" title="Fisher information">Fisher information</a></li>
<li><a href="http://en.wikipedia.org/wiki/Hamming_distance" title="Hamming distance">Hamming distance</a></li>
<li><a href="http://en.wikipedia.org/wiki/History_of_entropy" title="History of entropy">History of entropy</a></li>
<li><a href="http://en.wikipedia.org/wiki/History_of_information_theory" title="History of information theory">History of information theory</a></li>
<li><a href="http://en.wikipedia.org/wiki/Information_geometry" title="Information geometry">Information geometry</a></li>
<li><a href="http://en.wikipedia.org/wiki/Joint_entropy" title="Joint entropy">Joint entropy</a> – is the measure how much entropy is contained in a joint system of two random variables.</li>
<li><a href="http://en.wikipedia.org/wiki/Kolmogorov-Sinai_entropy" title="Kolmogorov-Sinai entropy" class="mw-redirect">Kolmogorov-Sinai entropy</a> in <a href="http://en.wikipedia.org/wiki/Dynamical_system" title="Dynamical system">dynamical systems</a></li>
<li><a href="http://en.wikipedia.org/wiki/Levenshtein_distance" title="Levenshtein distance">Levenshtein distance</a></li>
<li><a href="http://en.wikipedia.org/wiki/Mutual_information" title="Mutual information">Mutual information</a></li>
<li><a href="http://en.wikipedia.org/wiki/Negentropy" title="Negentropy">Negentropy</a></li>
<li><a href="http://en.wikipedia.org/wiki/Perplexity" title="Perplexity">Perplexity</a></li>
<li><a href="http://en.wikipedia.org/wiki/Qualitative_variation" title="Qualitative variation">Qualitative variation</a> – other measures of <a href="http://en.wikipedia.org/wiki/Statistical_dispersion" title="Statistical dispersion">statistical dispersion</a> for <a href="http://en.wikipedia.org/wiki/Nominal_distributions" title="Nominal distributions" class="mw-redirect">nominal distributions</a></li>
<li><a href="http://en.wikipedia.org/wiki/Quantum_relative_entropy" title="Quantum relative entropy">Quantum relative entropy</a> – a measure of distinguishability between two quantum states.</li>
<li><a href="http://en.wikipedia.org/wiki/R%C3%A9nyi_entropy" title="Rényi entropy">Rényi entropy</a> – a generalisation of Shannon entropy; it is one of a family of functionals for quantifying the diversity, uncertainty or randomness of a system.</li>
<li><a href="http://en.wikipedia.org/wiki/Shannon_index" title="Shannon index" class="mw-redirect">Shannon index</a></li>
<li><a href="http://en.wikipedia.org/wiki/Theil_index" title="Theil index">Theil index</a></li>
<li><a href="http://en.wikipedia.org/wiki/Typoglycemia" title="Typoglycemia">Typoglycemia</a></li>
</ul>
<h2><span class="mw-headline" id="References">References</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="http://en.wikipedia.org/w/index.php?title=Entropy_(information_theory)&action=edit&section=28" title="Edit section: References">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div class="reflist columns references-column-count references-column-count-2" style="-moz-column-count: 2; -webkit-column-count: 2; column-count: 2; list-style-type: decimal;">
<ol class="references">
<li id="cite_note-shannonPaper-1"><span class="mw-cite-backlink"><b><a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29#cite_ref-shannonPaper_1-0"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text"><span class="citation journal"><a href="http://en.wikipedia.org/wiki/Claude_Shannon" title="Claude Shannon">Shannon, Claude E.</a> (July–October 1948). "<a href="http://en.wikipedia.org/wiki/A_Mathematical_Theory_of_Communication" title="A Mathematical Theory of Communication">A Mathematical Theory of Communication</a>". <i>Bell System Technical Journal</i> <b>27</b> (3): 379–423. <a href="http://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="http://dx.doi.org/10.1002%2Fj.1538-7305.1948.tb01338.x">10.1002/j.1538-7305.1948.tb01338.x</a>.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEntropy+%28information+theory%29&amp;rft.atitle=A+Mathematical+Theory+of+Communication&amp;rft.aulast=Shannon%2C+Claude+E.&amp;rft.au=Shannon%2C+Claude+E.&amp;rft.date=July-October+1948&amp;rft.genre=article&amp;rft_id=info%3Adoi%2F10.1002%2Fj.1538-7305.1948.tb01338.x&amp;rft.issue=3&amp;rft.jtitle=Bell+System+Technical+Journal&amp;rft.pages=379-423&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.volume=27" class="Z3988"><span style="display:none;">&nbsp;</span></span> (<a rel="nofollow" class="external text" href="http://www.alcatel-lucent.com/bstj/vol27-1948/articles/bstj27-3-379.pdf">PDF</a>)</span></li>
<li id="cite_note-2"><span class="mw-cite-backlink"><b><a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29#cite_ref-2"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text"><span class="citation book">Goise, François &amp; Olla, Stefano (2008). <a rel="nofollow" class="external text" href="http://books.google.com/books?id=K3meGQ9ntjgC&pg=PA14"><i>Entropy methods for the Boltzmann equation: lectures from a special semester at the Centre Émile Borel, Institut H. Poincaré, Paris, 2001</i></a>. Springer. p.&nbsp;14. <a href="http://en.wikipedia.org/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&nbsp;<a href="http://en.wikipedia.org/wiki/Special:BookSources/978-3-540-73704-9" title="Special:BookSources/978-3-540-73704-9">978-3-540-73704-9</a>.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEntropy+%28information+theory%29&amp;rft.au=Goise%2C+Fran%C3%A7ois+%26+Olla%2C+Stefano&amp;rft.aulast=Goise%2C+Fran%C3%A7ois+%26+Olla%2C+Stefano&amp;rft.btitle=Entropy+methods+for+the+Boltzmann+equation%3A+lectures+from+a+special+semester+at+the+Centre+%C3%89mile+Borel%2C+Institut+H.+Poincar%C3%A9%2C+Paris%2C+2001&amp;rft.date=2008&amp;rft.genre=book&amp;rft_id=http%3A%2F%2Fbooks.google.com%2Fbooks%3Fid%3DK3meGQ9ntjgC%26pg%3DPA14&amp;rft.isbn=978-3-540-73704-9&amp;rft.pages=14&amp;rft.pub=Springer&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&nbsp;</span></span></span></li>
<li id="cite_note-Schneier.2C_B_page_234-3"><span class="mw-cite-backlink"><b><a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29#cite_ref-Schneier.2C_B_page_234_3-0"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text">Schneier, B: <i>Applied Cryptography</i>, Second edition, page 234. John Wiley and Sons.</span></li>
<li id="cite_note-Shannon.2C_Claude_E._1951-4"><span class="mw-cite-backlink"><b><a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29#cite_ref-Shannon.2C_Claude_E._1951_4-0"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text"><span class="citation journal">Shannon, C. E. (January 1951). <a rel="nofollow" class="external text" href="http://cs.brown.edu/courses/archive/2006-2007/cs195-5/extras/shannon-1951.pdf">"Prediction and Entropy of Printed English"</a>. <i><a href="http://en.wikipedia.org/wiki/Bell_System_Technical_Journal" title="Bell System Technical Journal">Bell System Technical Journal</a></i> <b>30</b> (1): 50–64. <a href="http://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="http://dx.doi.org/10.1002%2Fj.1538-7305.1951.tb01366.x">10.1002/j.1538-7305.1951.tb01366.x</a><span class="reference-accessdate">. Retrieved 30 March 2014</span>.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEntropy+%28information+theory%29&amp;rft.atitle=Prediction+and+Entropy+of+Printed+English&amp;rft.aufirst=C.+E.&amp;rft.aulast=Shannon&amp;rft.au=Shannon%2C+C.+E.&amp;rft.date=January+1951&amp;rft.genre=article&amp;rft_id=http%3A%2F%2Fcs.brown.edu%2Fcourses%2Farchive%2F2006-2007%2Fcs195-5%2Fextras%2Fshannon-1951.pdf&amp;rft_id=info%3Adoi%2F10.1002%2Fj.1538-7305.1951.tb01366.x&amp;rft.issue=1&amp;rft.jtitle=Bell+System+Technical+Journal&amp;rft.pages=50-64&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.volume=30" class="Z3988"><span style="display:none;">&nbsp;</span></span></span></li>
<li id="cite_note-5"><span class="mw-cite-backlink"><b><a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29#cite_ref-5"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text"><span class="citation book">Borda, Monica (2011). <a rel="nofollow" class="external text" href="http://books.google.com/books?id=Lyte2yl1SPAC&pg=PA11"><i>Fundamentals in Information Theory and Coding</i></a>. Springer. p.&nbsp;11. <a href="http://en.wikipedia.org/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&nbsp;<a href="http://en.wikipedia.org/wiki/Special:BookSources/978-3-642-20346-6" title="Special:BookSources/978-3-642-20346-6">978-3-642-20346-6</a>.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEntropy+%28information+theory%29&amp;rft.au=Borda%2C+Monica&amp;rft.aulast=Borda%2C+Monica&amp;rft.btitle=Fundamentals+in+Information+Theory+and+Coding&amp;rft.date=2011&amp;rft.genre=book&amp;rft_id=http%3A%2F%2Fbooks.google.com%2Fbooks%3Fid%3DLyte2yl1SPAC%26pg%3DPA11&amp;rft.isbn=978-3-642-20346-6&amp;rft.pages=11&amp;rft.pub=Springer&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&nbsp;</span></span></span></li>
<li id="cite_note-6"><span class="mw-cite-backlink"><b><a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29#cite_ref-6"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text"><span class="citation book">Han, Te Sun &amp; Kobayashi, Kingo (2002). <a rel="nofollow" class="external text" href="http://books.google.com/books?id=VpRESN24Zj0C&pg=PA19"><i>Mathematics of Information and Coding</i></a>. American Mathematical Society. pp.&nbsp;19–20. <a href="http://en.wikipedia.org/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&nbsp;<a href="http://en.wikipedia.org/wiki/Special:BookSources/978-0-8218-4256-0" title="Special:BookSources/978-0-8218-4256-0">978-0-8218-4256-0</a>.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEntropy+%28information+theory%29&amp;rft.au=Han%2C+Te+Sun+%26+Kobayashi%2C+Kingo&amp;rft.aulast=Han%2C+Te+Sun+%26+Kobayashi%2C+Kingo&amp;rft.btitle=Mathematics+of+Information+and+Coding&amp;rft.date=2002&amp;rft.genre=book&amp;rft_id=http%3A%2F%2Fbooks.google.com%2Fbooks%3Fid%3DVpRESN24Zj0C%26pg%3DPA19&amp;rft.isbn=978-0-8218-4256-0&amp;rft.pages=19-20&amp;rft.pub=American+Mathematical+Society&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&nbsp;</span></span></span></li>
<li id="cite_note-7"><span class="mw-cite-backlink"><b><a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29#cite_ref-7"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text">Schneider, T.D, <a rel="nofollow" class="external text" href="http://alum.mit.edu/www/toms/paper/primer/primer.pdf">Information theory primer with an appendix on logarithms</a>, National Cancer Institute, 14 April 2007.</span></li>
<li id="cite_note-8"><span class="mw-cite-backlink"><b><a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29#cite_ref-8"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text"><span class="citation book">Carter, Tom (March 2014). <a rel="nofollow" class="external text" href="http://csustan.csustan.edu/~tom/Lecture-Notes/Information-Theory/info-lec.pdf"><i>An introduction to information theory and entropy</i></a>. Santa Fe<span class="reference-accessdate">. Retrieved Aug 2014</span>.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEntropy+%28information+theory%29&amp;rft.au=Carter%2C+Tom&amp;rft.aufirst=Tom&amp;rft.aulast=Carter&amp;rft.btitle=An+introduction+to+information+theory+and+entropy&amp;rft.date=March+2014&amp;rft.genre=book&amp;rft_id=http%3A%2F%2Fcsustan.csustan.edu%2F~tom%2FLecture-Notes%2FInformation-Theory%2Finfo-lec.pdf&amp;rft.place=Santa+Fe&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&nbsp;</span></span></span></li>
<li id="cite_note-9"><span class="mw-cite-backlink"><b><a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29#cite_ref-9"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text">Compare: Boltzmann, Ludwig (1896, 1898). Vorlesungen über Gastheorie&nbsp;: 2 Volumes – Leipzig 1895/98 UB: O 5262-6. English version: Lectures on gas theory. Translated by Stephen G. Brush (1964) Berkeley: University of California Press; (1995) New York: Dover <a href="http://en.wikipedia.org/wiki/Special:BookSources/0486684555" class="internal mw-magiclink-isbn">ISBN 0-486-68455-5</a></span></li>
<li id="cite_note-10"><span class="mw-cite-backlink"><b><a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29#cite_ref-10"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text"><span class="citation web">Mark Nelson (24 August 2006). <a rel="nofollow" class="external text" href="http://marknelson.us/2006/08/24/the-hutter-prize/">"The Hutter Prize"</a><span class="reference-accessdate">. Retrieved 2008-11-27</span>.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEntropy+%28information+theory%29&amp;rft.aulast=Mark+Nelson&amp;rft.au=Mark+Nelson&amp;rft.btitle=The+Hutter+Prize&amp;rft.date=24+August+2006&amp;rft.genre=book&amp;rft_id=http%3A%2F%2Fmarknelson.us%2F2006%2F08%2F24%2Fthe-hutter-prize%2F&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&nbsp;</span></span></span></li>
<li id="cite_note-11"><span class="mw-cite-backlink"><b><a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29#cite_ref-11"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text">T. Schürmann and P. Grassberger, <a rel="nofollow" class="external text" href="http://arxiv.org/abs/cond-mat/0203436">Entropy Estimation of Symbol Sequences</a>, <i>CHAOS</i>,Vol. 6, No. 3 (1996) 414–427</span></li>
<li id="cite_note-12"><span class="mw-cite-backlink"><b><a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29#cite_ref-12"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text">T. Schürmann, <a rel="nofollow" class="external text" href="http://arxiv.org/abs/cond-mat/0403192">Bias Analysis in Entropy Estimation</a> J. Phys. A: Math. Gen. 37 (2004) L295-L301.</span></li>
<li id="cite_note-HilbertLopez2011-13"><span class="mw-cite-backlink">^ <a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29#cite_ref-HilbertLopez2011_13-0"><span class="cite-accessibility-label">Jump up to: </span><sup><i><b>a</b></i></sup></a> <a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29#cite_ref-HilbertLopez2011_13-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><a rel="nofollow" class="external text" href="http://www.sciencemag.org/content/332/6025/60">"The World's Technological Capacity to Store, Communicate, and Compute Information"</a>, Martin Hilbert and Priscila López (2011), <a href="http://en.wikipedia.org/wiki/Science_(journal)" title="Science (journal)">Science (journal)</a>, 332(6025), 60–65; free access to the article through here: martinhilbert.net/WorldInfoCapacity.html</span></li>
<li id="cite_note-14"><span class="mw-cite-backlink"><b><a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29#cite_ref-14"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text"><span class="citation conference">Massey, James (1994). <a rel="nofollow" class="external text" href="http://www.isiweb.ee.ethz.ch/archive/massey_pub/pdf/BI633.pdf">"Guessing and Entropy"</a>. "Proc. IEEE International Symposium on Information Theory"<span class="reference-accessdate">. Retrieved December 31, 2013</span>.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEntropy+%28information+theory%29&amp;rft.atitle=Proc.+IEEE+International+Symposium+on+Information+Theory&amp;rft.aufirst=James&amp;rft.aulast=Massey&amp;rft.au=Massey%2C+James&amp;rft.btitle=Guessing+and+Entropy&amp;rft.date=1994&amp;rft.genre=bookitem&amp;rft_id=http%3A%2F%2Fwww.isiweb.ee.ethz.ch%2Farchive%2Fmassey_pub%2Fpdf%2FBI633.pdf&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&nbsp;</span></span></span></li>
<li id="cite_note-15"><span class="mw-cite-backlink"><b><a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29#cite_ref-15"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text"><span class="citation conference">Malone, David; Sullivan, Wayne (2005). <a rel="nofollow" class="external text" href="http://www.maths.tcd.ie/~dwmalone/p/itt05.pdf">"Guesswork is not a Substitute for Entropy"</a>. "Proceedings of the Information Technology &amp; Telecommunications Conference"<span class="reference-accessdate">. Retrieved December 31, 2013</span>.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEntropy+%28information+theory%29&amp;rft.atitle=Proceedings+of+the+Information+Technology+%26+Telecommunications+Conference&amp;rft.aufirst=David&amp;rft.aulast=Malone&amp;rft.au=Malone%2C+David&amp;rft.au=Sullivan%2C+Wayne&amp;rft.btitle=Guesswork+is+not+a+Substitute+for+Entropy&amp;rft.date=2005&amp;rft.genre=bookitem&amp;rft_id=http%3A%2F%2Fwww.maths.tcd.ie%2F~dwmalone%2Fp%2Fitt05.pdf&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&nbsp;</span></span></span></li>
<li id="cite_note-16"><span class="mw-cite-backlink"><b><a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29#cite_ref-16"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text">Aoki, New Approaches to Macroeconomic Modeling. page 43.</span></li>
<li id="cite_note-17"><span class="mw-cite-backlink"><b><a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29#cite_ref-17"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text">Probability and Computing, M. Mitzenmacher and E. Upfal, Cambridge University Press</span></li>
</ol>
</div>
<p><i>This article incorporates material from <a href="http://planetmath.org/node/968" class="extiw" title="planetmath:968">Shannon's entropy</a> on <a href="http://en.wikipedia.org/wiki/PlanetMath" title="PlanetMath">PlanetMath</a>, which is licensed under the <a href="http://en.wikipedia.org/wiki/Wikipedia:CC-BY-SA" title="Wikipedia:CC-BY-SA" class="mw-redirect">Creative Commons Attribution/Share-Alike License</a>.</i></p>
<h2><span class="mw-headline" id="Further_reading">Further reading</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="http://en.wikipedia.org/w/index.php?title=Entropy_(information_theory)&action=edit&section=29" title="Edit section: Further reading">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<h3><span class="mw-headline" id="Textbooks_on_information_theory">Textbooks on information theory</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="http://en.wikipedia.org/w/index.php?title=Entropy_(information_theory)&action=edit&section=30" title="Edit section: Textbooks on information theory">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<ul>
<li>Arndt, C. <i>Information Measures, Information and its Description in Science and Engineering</i> (Springer Series: Signals and Communication Technology), 2004, <a href="http://en.wikipedia.org/wiki/Special:BookSources/9783540408550" class="internal mw-magiclink-isbn">ISBN 978-3-540-40855-0</a></li>
</ul>
<ul>
<li>Ash, RB. <i>Information Theory</i>. New York: Interscience, 1965. <a href="http://en.wikipedia.org/wiki/Special:BookSources/0470034459" class="internal mw-magiclink-isbn">ISBN 0-470-03445-9</a>. New York: Dover 1990. <a href="http://en.wikipedia.org/wiki/Special:BookSources/0486665216" class="internal mw-magiclink-isbn">ISBN 0-486-66521-6</a></li>
</ul>
<ul>
<li><a href="http://en.wikipedia.org/w/index.php?title=Gallager,_R&action=edit&redlink=1" class="new" title="Gallager, R (page does not exist)">Gallager, R</a>. <i>Information Theory and Reliable Communication.</i> New York: John Wiley and Sons, 1968. <a href="http://en.wikipedia.org/wiki/Special:BookSources/0471290483" class="internal mw-magiclink-isbn">ISBN 0-471-29048-3</a></li>
</ul>
<ul>
<li>Goldman, S. <i>Information Theory</i>. New York: Prentice Hall, 1953. New York: Dover 1968 <a href="http://en.wikipedia.org/wiki/Special:BookSources/0486622096" class="internal mw-magiclink-isbn">ISBN 0-486-62209-6</a>, 2005 <a href="http://en.wikipedia.org/wiki/Special:BookSources/0486442713" class="internal mw-magiclink-isbn">ISBN 0-486-44271-3</a></li>
</ul>
<ul>
<li><a href="http://en.wikipedia.org/w/index.php?title=Cover,_TM&action=edit&redlink=1" class="new" title="Cover, TM (page does not exist)">Cover, TM</a>, Thomas, JA. <i>Elements of information theory</i>, 1st Edition. New York: Wiley-Interscience, 1991. <a href="http://en.wikipedia.org/wiki/Special:BookSources/0471062596" class="internal mw-magiclink-isbn">ISBN 0-471-06259-6</a>.</li>
</ul>
<dl>
<dd>2nd Edition. New York: Wiley-Interscience, 2006. <a href="http://en.wikipedia.org/wiki/Special:BookSources/0471241954" class="internal mw-magiclink-isbn">ISBN 0-471-24195-4</a>.</dd>
</dl>
<ul>
<li>MacKay, DJC. <i><a rel="nofollow" class="external text" href="http://www.inference.phy.cam.ac.uk/mackay/itila/book.html">Information Theory, Inference, and Learning Algorithms</a></i> Cambridge: Cambridge University Press, 2003. <a href="http://en.wikipedia.org/wiki/Special:BookSources/0521642981" class="internal mw-magiclink-isbn">ISBN 0-521-64298-1</a></li>
</ul>
<ul>
<li><span class="citation book">Martin, Nathaniel F.G. &amp; England, James W. (2011). <a rel="nofollow" class="external text" href="http://books.google.com/books?id=_77lvx7y8joC"><i>Mathematical Theory of Entropy</i></a>. Cambridge University Press. <a href="http://en.wikipedia.org/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&nbsp;<a href="http://en.wikipedia.org/wiki/Special:BookSources/978-0-521-17738-2" title="Special:BookSources/978-0-521-17738-2">978-0-521-17738-2</a>.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEntropy+%28information+theory%29&amp;rft.aulast=Martin%2C+Nathaniel+F.G.+%26+England%2C+James+W.&amp;rft.au=Martin%2C+Nathaniel+F.G.+%26+England%2C+James+W.&amp;rft.btitle=Mathematical+Theory+of+Entropy&amp;rft.date=2011&amp;rft.genre=book&amp;rft_id=http%3A%2F%2Fbooks.google.com%2Fbooks%3Fid%3D_77lvx7y8joC&amp;rft.isbn=978-0-521-17738-2&amp;rft.pub=Cambridge+University+Press&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&nbsp;</span></span></li>
</ul>
<ul>
<li>Mansuripur, M. <i>Introduction to Information Theory</i>. New York: Prentice Hall, 1987. <a href="http://en.wikipedia.org/wiki/Special:BookSources/0134846680" class="internal mw-magiclink-isbn">ISBN 0-13-484668-0</a></li>
</ul>
<ul>
<li>Pierce, JR. "An introduction to information theory: symbols, signals and noise". Dover (2nd Edition). 1961 (reprinted by Dover 1980).</li>
</ul>
<ul>
<li><a href="http://en.wikipedia.org/w/index.php?title=Reza,_F&action=edit&redlink=1" class="new" title="Reza, F (page does not exist)">Reza, F</a>. <i>An Introduction to Information Theory</i>. New York: McGraw-Hill 1961. New York: Dover 1994. <a href="http://en.wikipedia.org/wiki/Special:BookSources/0486682102" class="internal mw-magiclink-isbn">ISBN 0-486-68210-2</a></li>
</ul>
<ul>
<li><a href="http://en.wikipedia.org/w/index.php?title=Shannon,_CE&action=edit&redlink=1" class="new" title="Shannon, CE (page does not exist)">Shannon, CE</a>. <a href="http://en.wikipedia.org/wiki/Warren_Weaver" title="Warren Weaver">Warren Weaver</a>. <i>The Mathematical Theory of Communication.</i> Univ of Illinois Press, 1949. <a href="http://en.wikipedia.org/wiki/Special:BookSources/0252725484" class="internal mw-magiclink-isbn">ISBN 0-252-72548-4</a></li>
</ul>
<ul>
<li>Stone, JV. Chapter 1 of book <a rel="nofollow" class="external text" href="http://jim-stone.staff.shef.ac.uk/BookInfoTheory/InfoTheoryBookMain.html">"Information Theory: A Tutorial Introduction"</a>, University of Sheffield, England, 2014. <a href="http://en.wikipedia.org/wiki/Special:BookSources/9780956372857" class="internal mw-magiclink-isbn">ISBN 978-0956372857</a>.</li>
</ul>
<h2><span class="mw-headline" id="External_links">External links</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="http://en.wikipedia.org/w/index.php?title=Entropy_(information_theory)&action=edit&section=31" title="Edit section: External links">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<ul>
<li><span id="CITEREFHazewinkel2001" class="citation">Hazewinkel, Michiel, ed. (2001), <a rel="nofollow" class="external text" href="http://www.encyclopediaofmath.org/index.php?title=p/e035740">"Entropy"</a>, <i><a href="http://en.wikipedia.org/wiki/Encyclopedia_of_Mathematics" title="Encyclopedia of Mathematics">Encyclopedia of Mathematics</a></i>, <a href="http://en.wikipedia.org/wiki/Springer_Science%2BBusiness_Media" title="Springer Science+Business Media">Springer</a>, <a href="http://en.wikipedia.org/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&nbsp;<a href="http://en.wikipedia.org/wiki/Special:BookSources/978-1-55608-010-4" title="Special:BookSources/978-1-55608-010-4">978-1-55608-010-4</a></span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEntropy+%28information+theory%29&amp;rft.atitle=Encyclopedia+of+Mathematics&amp;rft.btitle=Entropy&amp;rft.date=2001&amp;rft.genre=bookitem&amp;rft_id=http%3A%2F%2Fwww.encyclopediaofmath.org%2Findex.php%3Ftitle%3Dp%2Fe035740&amp;rft.isbn=978-1-55608-010-4&amp;rft.pub=Springer&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&nbsp;</span></span></li>
<li><a rel="nofollow" class="external text" href="http://pespmc1.vub.ac.be/ENTRINFO.html">Introduction to entropy and information</a> on <a href="http://en.wikipedia.org/wiki/Principia_Cybernetica_Web" title="Principia Cybernetica Web" class="mw-redirect">Principia Cybernetica Web</a></li>
<li><i><a rel="nofollow" class="external text" href="http://www.mdpi.com/journal/entropy">Entropy</a></i> an interdisciplinary journal on all aspect of the entropy concept. Open access.</li>
<li><a rel="nofollow" class="external text" href="http://alum.mit.edu/www/toms/information.is.not.uncertainty.html">Information is not entropy, information is not uncertainty&nbsp;!</a> – a discussion of the use of the terms "information" and "entropy".</li>
<li><a rel="nofollow" class="external text" href="http://alum.mit.edu/www/toms/bionet.info-theory.faq.html#Information.Equal.Entropy">I'm Confused: How Could Information Equal Entropy?</a> – a similar discussion on the bionet.info-theory FAQ.</li>
<li><a rel="nofollow" class="external text" href="http://www.rheingold.com/texts/tft/6.html">Description of information entropy from "Tools for Thought" by Howard Rheingold</a></li>
<li><a rel="nofollow" class="external text" href="http://math.ucsd.edu/~crypto/java/ENTROPY/">A java applet representing Shannon's Experiment to Calculate the Entropy of English</a></li>
<li><a rel="nofollow" class="external text" href="http://www.autonlab.org/tutorials/infogain.html">Slides on information gain and entropy</a></li>
<li><a class="external text" href="http://en.wikibooks.org/wiki/An_Intuitive_Guide_to_the_Concept_of_Entropy_Arising_in_Various_Sectors_of_Science"><i>An Intuitive Guide to the Concept of Entropy Arising in Various Sectors of Science</i></a> – a wikibook on the interpretation of the concept of entropy.</li>
<li><a rel="nofollow" class="external text" href="http://www.shannonentropy.netmark.pl/">Calculator for Shannon entropy estimation and interpretation</a></li>
<li><a rel="nofollow" class="external text" href="http://arxiv.org/abs/1404.1998">A Light Discussion and Derivation of Entropy</a></li>
<li><a rel="nofollow" class="external text" href="https://researchspace.auckland.ac.nz/handle/2292/3427">Network Event Detection With Entropy Measures</a>, Dr. Raimund Eimann, University of Auckland, PDF; 5993&nbsp;kB – a PhD thesis demonstrating how entropy measures may be used in network anomaly detection.</li>
</ul>
<table class="navbox" style="border-spacing:0">
<tbody><tr>
<td style="padding:2px">
<table class="nowraplinks hlist collapsible autocollapse navbox-inner" style="border-spacing:0;background:transparent;color:inherit" id="collapsibleTable0">
<tbody><tr>
<th scope="col" class="navbox-title" colspan="2"><span class="collapseButton">[<a id="collapseButton0" href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29#">hide</a>]</span>
<div class="plainlinks hlist navbar mini">
<ul>
<li class="nv-view"><a href="http://en.wikipedia.org/wiki/Template:Compression_methods" title="Template:Compression methods"><span title="View this template" style=";;background:none transparent;border:none;;">v</span></a></li>
<li class="nv-talk"><a href="http://en.wikipedia.org/wiki/Template_talk:Compression_methods" title="Template talk:Compression methods"><span title="Discuss this template" style=";;background:none transparent;border:none;;">t</span></a></li>
<li class="nv-edit"><a class="external text" href="http://en.wikipedia.org/w/index.php?title=Template:Compression_methods&action=edit"><span title="Edit this template" style=";;background:none transparent;border:none;;">e</span></a></li>
</ul>
</div>
<div style="font-size:110%"><a href="http://en.wikipedia.org/wiki/Data_compression" title="Data compression">Data compression</a> methods</div>
</th>
</tr>
<tr style="height:2px">
<td colspan="2"></td>
</tr>
<tr>
<th scope="row" class="navbox-group"><a href="http://en.wikipedia.org/wiki/Lossless_compression" title="Lossless compression">Lossless</a></th>
<td class="navbox-list navbox-odd" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px">
<div style="padding:0em 0.25em"></div>
<table class="nowraplinks navbox-subgroup" style="border-spacing:0">
<tbody><tr>
<th scope="row" class="navbox-group" style="width:7.0em;font-weight:normal;"><a href="http://en.wikipedia.org/wiki/Entropy_encoding" title="Entropy encoding">Entropy type</a></th>
<td class="navbox-list navbox-odd" style="text-align:left;border-left-width:2px;border-left-style:solid;padding:0px">
<div style="padding:0em 0.25em">
<ul>
<li><a href="http://en.wikipedia.org/wiki/Unary_coding" title="Unary coding">Unary</a></li>
<li><a href="http://en.wikipedia.org/wiki/Arithmetic_coding" title="Arithmetic coding">Arithmetic</a></li>
<li><a href="http://en.wikipedia.org/wiki/Golomb_coding" title="Golomb coding">Golomb</a></li>
<li><a href="http://en.wikipedia.org/wiki/Huffman_coding" title="Huffman coding">Huffman</a>
<ul>
<li><a href="http://en.wikipedia.org/wiki/Adaptive_Huffman_coding" title="Adaptive Huffman coding">Adaptive</a></li>
<li><a href="http://en.wikipedia.org/wiki/Canonical_Huffman_code" title="Canonical Huffman code">Canonical</a></li>
<li><a href="http://en.wikipedia.org/wiki/Modified_Huffman_coding" title="Modified Huffman coding">Modified</a></li>
</ul>
</li>
<li><a href="http://en.wikipedia.org/wiki/Range_encoding" title="Range encoding">Range</a></li>
<li><a href="http://en.wikipedia.org/wiki/Shannon%E2%80%93Fano_coding" title="Shannon–Fano coding">Shannon–Fano</a></li>
<li><a href="http://en.wikipedia.org/wiki/Shannon%E2%80%93Fano%E2%80%93Elias_coding" title="Shannon–Fano–Elias coding">Shannon–Fano–Elias</a></li>
<li><a href="http://en.wikipedia.org/wiki/Tunstall_coding" title="Tunstall coding">Tunstall</a></li>
<li><a href="http://en.wikipedia.org/wiki/Universal_code_(data_compression)" title="Universal code (data compression)">Universal</a>
<ul>
<li><a href="http://en.wikipedia.org/wiki/Exponential-Golomb_coding" title="Exponential-Golomb coding">Exp-Golomb</a></li>
<li><a href="http://en.wikipedia.org/wiki/Fibonacci_coding" title="Fibonacci coding">Fibonacci</a></li>
<li><a href="http://en.wikipedia.org/wiki/Elias_gamma_coding" title="Elias gamma coding">Gamma</a></li>
<li><a href="http://en.wikipedia.org/wiki/Levenshtein_coding" title="Levenshtein coding">Levenshtein</a></li>
</ul>
</li>
</ul>
</div>
</td>
</tr>
<tr style="height:2px">
<td colspan="2"></td>
</tr>
<tr>
<th scope="row" class="navbox-group" style="width:7.0em;font-weight:normal;"><a href="http://en.wikipedia.org/wiki/Dictionary_coder" title="Dictionary coder">Dictionary type</a></th>
<td class="navbox-list navbox-even" style="text-align:left;border-left-width:2px;border-left-style:solid;padding:0px">
<div style="padding:0em 0.25em">
<ul>
<li><a href="http://en.wikipedia.org/wiki/Byte_pair_encoding" title="Byte pair encoding">Byte pair encoding</a></li>
<li><a href="http://en.wikipedia.org/wiki/DEFLATE" title="DEFLATE">DEFLATE</a></li>
<li><a href="http://en.wikipedia.org/wiki/LZ77_and_LZ78" title="LZ77 and LZ78">Lempel–Ziv</a>
<ul>
<li><a href="http://en.wikipedia.org/wiki/LZ77_and_LZ78" title="LZ77 and LZ78">LZ77&nbsp;/ LZ78 (LZ1&nbsp;/ LZ2)</a></li>
<li><a href="http://en.wikipedia.org/wiki/LZJB" title="LZJB">LZJB</a></li>
<li><a href="http://en.wikipedia.org/wiki/Lempel%E2%80%93Ziv%E2%80%93Markov_chain_algorithm" title="Lempel–Ziv–Markov chain algorithm">LZMA</a></li>
<li><a href="http://en.wikipedia.org/wiki/Lempel%E2%80%93Ziv%E2%80%93Oberhumer" title="Lempel–Ziv–Oberhumer">LZO</a></li>
<li><a href="http://en.wikipedia.org/wiki/LZRW" title="LZRW">LZRW</a></li>
<li><a href="http://en.wikipedia.org/wiki/Lempel%E2%80%93Ziv%E2%80%93Stac" title="Lempel–Ziv–Stac">LZS</a></li>
<li><a href="http://en.wikipedia.org/wiki/Lempel%E2%80%93Ziv%E2%80%93Storer%E2%80%93Szymanski" title="Lempel–Ziv–Storer–Szymanski">LZSS</a></li>
<li><a href="http://en.wikipedia.org/wiki/Lempel%E2%80%93Ziv%E2%80%93Welch" title="Lempel–Ziv–Welch">LZW</a></li>
<li><a href="http://en.wikipedia.org/wiki/LZWL" title="LZWL">LZWL</a></li>
<li><a href="http://en.wikipedia.org/wiki/LZX_(algorithm)" title="LZX (algorithm)">LZX</a></li>
<li><a href="http://en.wikipedia.org/wiki/LZ4_(compression_algorithm)" title="LZ4 (compression algorithm)">LZ4</a></li>
<li><a href="http://en.wikipedia.org/wiki/Statistical_Lempel%E2%80%93Ziv" title="Statistical Lempel–Ziv">Statistical</a></li>
</ul>
</li>
<li><a href="http://en.wikipedia.org/wiki/Run-length_encoding" title="Run-length encoding">RLE</a></li>
</ul>
</div>
</td>
</tr>
<tr style="height:2px">
<td colspan="2"></td>
</tr>
<tr>
<th scope="row" class="navbox-group" style="width:7.0em;font-weight:normal;">Other types</th>
<td class="navbox-list navbox-odd" style="text-align:left;border-left-width:2px;border-left-style:solid;padding:0px">
<div style="padding:0em 0.25em">
<ul>
<li><a href="http://en.wikipedia.org/wiki/Burrows%E2%80%93Wheeler_transform" title="Burrows–Wheeler transform">BWT</a></li>
<li><a href="http://en.wikipedia.org/wiki/Context_tree_weighting" title="Context tree weighting">CTW</a></li>
<li><a href="http://en.wikipedia.org/wiki/Delta_encoding" title="Delta encoding">Delta</a></li>
<li><a href="http://en.wikipedia.org/wiki/Dynamic_Markov_compression" title="Dynamic Markov compression">DMC</a></li>
<li><a href="http://en.wikipedia.org/wiki/Move-to-front_transform" title="Move-to-front transform">MTF</a></li>
<li><a href="http://en.wikipedia.org/wiki/Prediction_by_partial_matching" title="Prediction by partial matching">PPM</a></li>
</ul>
</div>
</td>
</tr>
</tbody></table>
</td>
</tr>
<tr style="height:2px">
<td colspan="2"></td>
</tr>
<tr>
<th scope="row" class="navbox-group"><a href="http://en.wikipedia.org/wiki/Audio_compression_(data)" title="Audio compression (data)" class="mw-redirect">Audio</a></th>
<td class="navbox-list navbox-even" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px">
<div style="padding:0em 0.25em"></div>
<table class="nowraplinks navbox-subgroup" style="border-spacing:0">
<tbody><tr>
<th scope="row" class="navbox-group" style="width:7.0em;font-weight:normal;">Concepts</th>
<td class="navbox-list navbox-even" style="text-align:left;border-left-width:2px;border-left-style:solid;padding:0px">
<div style="padding:0em 0.25em">
<ul>
<li><a href="http://en.wikipedia.org/wiki/Bit_rate" title="Bit rate">Bit rate</a>
<ul>
<li><a href="http://en.wikipedia.org/wiki/Average_bitrate" title="Average bitrate">average (ABR)</a></li>
<li><a href="http://en.wikipedia.org/wiki/Constant_bitrate" title="Constant bitrate">constant (CBR)</a></li>
<li><a href="http://en.wikipedia.org/wiki/Variable_bitrate" title="Variable bitrate">variable (VBR)</a></li>
</ul>
</li>
<li><a href="http://en.wikipedia.org/wiki/Companding" title="Companding">Companding</a></li>
<li><a href="http://en.wikipedia.org/wiki/Convolution" title="Convolution">Convolution</a></li>
<li><a href="http://en.wikipedia.org/wiki/Dynamic_range" title="Dynamic range">Dynamic range</a></li>
<li><a href="http://en.wikipedia.org/wiki/Latency_(audio)" title="Latency (audio)">Latency</a></li>
<li><a href="http://en.wikipedia.org/wiki/Nyquist%E2%80%93Shannon_sampling_theorem" title="Nyquist–Shannon sampling theorem">Nyquist–Shannon theorem</a></li>
<li><a href="http://en.wikipedia.org/wiki/Sampling_(signal_processing)" title="Sampling (signal processing)">Sampling</a></li>
<li><a href="http://en.wikipedia.org/wiki/Sound_quality" title="Sound quality">Sound quality</a></li>
<li><a href="http://en.wikipedia.org/wiki/Speech_coding" title="Speech coding">Speech coding</a></li>
<li><a href="http://en.wikipedia.org/wiki/Sub-band_coding" title="Sub-band coding">Sub-band coding</a></li>
</ul>
</div>
</td>
</tr>
<tr style="height:2px">
<td colspan="2"></td>
</tr>
<tr>
<th scope="row" class="navbox-group" style="width:7.0em;font-weight:normal;"><a href="http://en.wikipedia.org/wiki/Audio_codec" title="Audio codec">Codec</a> parts</th>
<td class="navbox-list navbox-odd" style="text-align:left;border-left-width:2px;border-left-style:solid;padding:0px">
<div style="padding:0em 0.25em">
<ul>
<li><a href="http://en.wikipedia.org/wiki/A-law_algorithm" title="A-law algorithm">A-law</a></li>
<li><a href="http://en.wikipedia.org/wiki/%CE%9C-law_algorithm" title="Μ-law algorithm">μ-law</a></li>
<li><a href="http://en.wikipedia.org/wiki/Algebraic_code-excited_linear_prediction" title="Algebraic code-excited linear prediction">ACELP</a></li>
<li><a href="http://en.wikipedia.org/wiki/Adaptive_differential_pulse-code_modulation" title="Adaptive differential pulse-code modulation">ADPCM</a></li>
<li><a href="http://en.wikipedia.org/wiki/Code-excited_linear_prediction" title="Code-excited linear prediction">CELP</a></li>
<li><a href="http://en.wikipedia.org/wiki/Differential_pulse-code_modulation" title="Differential pulse-code modulation">DPCM</a></li>
<li><a href="http://en.wikipedia.org/wiki/Fourier_transform" title="Fourier transform">Fourier transform</a></li>
<li><a href="http://en.wikipedia.org/wiki/Linear_predictive_coding" title="Linear predictive coding">LPC</a>
<ul>
<li><a href="http://en.wikipedia.org/wiki/Log_area_ratio" title="Log area ratio">LAR</a></li>
<li><a href="http://en.wikipedia.org/wiki/Line_spectral_pairs" title="Line spectral pairs">LSP</a></li>
</ul>
</li>
<li><a href="http://en.wikipedia.org/wiki/Modified_discrete_cosine_transform" title="Modified discrete cosine transform">MDCT</a></li>
<li><a href="http://en.wikipedia.org/wiki/Psychoacoustics" title="Psychoacoustics">Psychoacoustic model</a></li>
<li><a href="http://en.wikipedia.org/wiki/Warped_linear_predictive_coding" title="Warped linear predictive coding">WLPC</a></li>
</ul>
</div>
</td>
</tr>
</tbody></table>
</td>
</tr>
<tr style="height:2px">
<td colspan="2"></td>
</tr>
<tr>
<th scope="row" class="navbox-group"><a href="http://en.wikipedia.org/wiki/Image_compression" title="Image compression">Image</a></th>
<td class="navbox-list navbox-odd" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px">
<div style="padding:0em 0.25em"></div>
<table class="nowraplinks navbox-subgroup" style="border-spacing:0">
<tbody><tr>
<th scope="row" class="navbox-group" style="width:7.0em;font-weight:normal;">Concepts</th>
<td class="navbox-list navbox-even" style="text-align:left;border-left-width:2px;border-left-style:solid;padding:0px">
<div style="padding:0em 0.25em">
<ul>
<li><a href="http://en.wikipedia.org/wiki/Chroma_subsampling" title="Chroma subsampling">Chroma subsampling</a></li>
<li><a href="http://en.wikipedia.org/wiki/Coding_tree_unit" title="Coding tree unit">Coding tree unit</a></li>
<li><a href="http://en.wikipedia.org/wiki/Color_space" title="Color space">Color space</a></li>
<li><a href="http://en.wikipedia.org/wiki/Compression_artifact" title="Compression artifact">Compression artifact</a></li>
<li><a href="http://en.wikipedia.org/wiki/Image_resolution" title="Image resolution">Image resolution</a></li>
<li><a href="http://en.wikipedia.org/wiki/Macroblock" title="Macroblock">Macroblock</a></li>
<li><a href="http://en.wikipedia.org/wiki/Pixel" title="Pixel">Pixel</a></li>
<li><a href="http://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio" title="Peak signal-to-noise ratio">PSNR</a></li>
<li><a href="http://en.wikipedia.org/wiki/Quantization_(image_processing)" title="Quantization (image processing)">Quantization</a></li>
<li><a href="http://en.wikipedia.org/wiki/Standard_test_image" title="Standard test image">Standard test image</a></li>
</ul>
</div>
</td>
</tr>
<tr style="height:2px">
<td colspan="2"></td>
</tr>
<tr>
<th scope="row" class="navbox-group" style="width:7.0em;font-weight:normal;">Methods</th>
<td class="navbox-list navbox-odd" style="text-align:left;border-left-width:2px;border-left-style:solid;padding:0px">
<div style="padding:0em 0.25em">
<ul>
<li><a href="http://en.wikipedia.org/wiki/Chain_code" title="Chain code">Chain code</a></li>
<li><a href="http://en.wikipedia.org/wiki/Discrete_cosine_transform" title="Discrete cosine transform">DCT</a></li>
<li><a href="http://en.wikipedia.org/wiki/Embedded_Zerotrees_of_Wavelet_transforms" title="Embedded Zerotrees of Wavelet transforms">EZW</a></li>
<li><a href="http://en.wikipedia.org/wiki/Fractal_compression" title="Fractal compression">Fractal</a></li>
<li><a href="http://en.wikipedia.org/wiki/Karhunen%E2%80%93Lo%C3%A8ve_theorem" title="Karhunen–Loève theorem">KLT</a></li>
<li><a href="http://en.wikipedia.org/wiki/Pyramid_(image_processing)" title="Pyramid (image processing)">LP</a></li>
<li><a href="http://en.wikipedia.org/wiki/Run-length_encoding" title="Run-length encoding">RLE</a></li>
<li><a href="http://en.wikipedia.org/wiki/Set_partitioning_in_hierarchical_trees" title="Set partitioning in hierarchical trees">SPIHT</a></li>
<li><a href="http://en.wikipedia.org/wiki/Wavelet_compression" title="Wavelet compression" class="mw-redirect">Wavelet</a></li>
</ul>
</div>
</td>
</tr>
</tbody></table>
</td>
</tr>
<tr style="height:2px">
<td colspan="2"></td>
</tr>
<tr>
<th scope="row" class="navbox-group"><a href="http://en.wikipedia.org/wiki/Video_compression" title="Video compression" class="mw-redirect">Video</a></th>
<td class="navbox-list navbox-even" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px">
<div style="padding:0em 0.25em"></div>
<table class="nowraplinks navbox-subgroup" style="border-spacing:0">
<tbody><tr>
<th scope="row" class="navbox-group" style="width:7.0em;font-weight:normal;">Concepts</th>
<td class="navbox-list navbox-even" style="text-align:left;border-left-width:2px;border-left-style:solid;padding:0px">
<div style="padding:0em 0.25em">
<ul>
<li><a href="http://en.wikipedia.org/wiki/Bit_rate" title="Bit rate">Bit rate</a>
<ul>
<li><a href="http://en.wikipedia.org/wiki/Average_bitrate" title="Average bitrate">average (ABR)</a></li>
<li><a href="http://en.wikipedia.org/wiki/Constant_bitrate" title="Constant bitrate">constant (CBR)</a></li>
<li><a href="http://en.wikipedia.org/wiki/Variable_bitrate" title="Variable bitrate">variable (VBR)</a></li>
</ul>
</li>
<li><a href="http://en.wikipedia.org/wiki/Display_resolution" title="Display resolution">Display resolution</a></li>
<li><a href="http://en.wikipedia.org/wiki/Film_frame" title="Film frame">Frame</a></li>
<li><a href="http://en.wikipedia.org/wiki/Frame_rate" title="Frame rate">Frame rate</a></li>
<li><a href="http://en.wikipedia.org/wiki/Video_compression_picture_types" title="Video compression picture types">Frame types</a></li>
<li><a href="http://en.wikipedia.org/wiki/Interlaced_video" title="Interlaced video">Interlace</a></li>
<li><a href="http://en.wikipedia.org/wiki/Video#Characteristics_of_video_streams" title="Video">Video characteristics</a></li>
<li><a href="http://en.wikipedia.org/wiki/Video_quality" title="Video quality">Video quality</a></li>
</ul>
</div>
</td>
</tr>
<tr style="height:2px">
<td colspan="2"></td>
</tr>
<tr>
<th scope="row" class="navbox-group" style="width:7.0em;font-weight:normal;"><a href="http://en.wikipedia.org/wiki/Video_codec" title="Video codec">Codec</a> parts</th>
<td class="navbox-list navbox-odd" style="text-align:left;border-left-width:2px;border-left-style:solid;padding:0px">
<div style="padding:0em 0.25em">
<ul>
<li><a href="http://en.wikipedia.org/wiki/Discrete_cosine_transform" title="Discrete cosine transform">DCT</a></li>
<li><a href="http://en.wikipedia.org/wiki/Deblocking_filter" title="Deblocking filter">Deblocking filter</a></li>
<li><a href="http://en.wikipedia.org/wiki/Motion_compensation" title="Motion compensation">Motion compensation</a></li>
</ul>
</div>
</td>
</tr>
</tbody></table>
</td>
</tr>
<tr style="height:2px">
<td colspan="2"></td>
</tr>
<tr>
<th scope="row" class="navbox-group"><a href="http://en.wikipedia.org/wiki/Information_theory" title="Information theory">Theory</a></th>
<td class="navbox-list navbox-even" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px">
<div style="padding:0em 0.25em">
<ul>
<li><strong class="selflink">Entropy</strong></li>
<li><a href="http://en.wikipedia.org/wiki/Kolmogorov_complexity" title="Kolmogorov complexity">Kolmogorov complexity</a></li>
<li><a href="http://en.wikipedia.org/wiki/Lossy_compression" title="Lossy compression">Lossy</a></li>
<li><a href="http://en.wikipedia.org/wiki/Quantization_(signal_processing)" title="Quantization (signal processing)">Quantization</a></li>
<li><a href="http://en.wikipedia.org/wiki/Rate%E2%80%93distortion_theory" title="Rate–distortion theory">Rate–distortion</a></li>
<li><a href="http://en.wikipedia.org/wiki/Redundancy_(information_theory)" title="Redundancy (information theory)">Redundancy</a></li>
<li><a href="http://en.wikipedia.org/wiki/Timeline_of_information_theory" title="Timeline of information theory">Timeline of information theory</a></li>
</ul>
</div>
</td>
</tr>
<tr style="height:2px">
<td colspan="2"></td>
</tr>
<tr>
<td class="navbox-abovebelow" colspan="2">
<div>
<ul>
<li><img alt="Template" src="./Entropy (information theory) - Wikipedia, the free encyclopedia_files/16px-Symbol_template_class.svg.png" width="16" height="16" srcset="//upload.wikimedia.org/wikipedia/en/thumb/5/5c/Symbol_template_class.svg/23px-Symbol_template_class.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/5/5c/Symbol_template_class.svg/31px-Symbol_template_class.svg.png 2x" data-file-width="180" data-file-height="185"> <a href="http://en.wikipedia.org/wiki/Template:Compression_formats" title="Template:Compression formats">Compression formats</a></li>
<li><img alt="Template" src="./Entropy (information theory) - Wikipedia, the free encyclopedia_files/16px-Symbol_template_class.svg.png" width="16" height="16" srcset="//upload.wikimedia.org/wikipedia/en/thumb/5/5c/Symbol_template_class.svg/23px-Symbol_template_class.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/5/5c/Symbol_template_class.svg/31px-Symbol_template_class.svg.png 2x" data-file-width="180" data-file-height="185"> <a href="http://en.wikipedia.org/wiki/Template:Compression_software" title="Template:Compression software">Compression software (codecs)</a></li>
</ul>
</div>
</td>
</tr>
</tbody></table>
</td>
</tr>
</tbody></table>


<!-- 
NewPP limit report
Parsed by mw1007
CPU time usage: 1.676 seconds
Real time usage: 1.984 seconds
Preprocessor visited node count: 2422/1000000
Preprocessor generated node count: 10638/1500000
Post‐expand include size: 74011/2097152 bytes
Template argument size: 1330/2097152 bytes
Highest expansion depth: 14/40
Expensive parser function count: 7/500
Lua time usage: 0.215/10.000 seconds
Lua memory usage: 3.82 MB/50 MB
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:15445-0!*!0!!en!4!*!math=0 and timestamp 20141123174549 and revision id 633857614
 -->
<noscript>&lt;img src="//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" alt="" title="" width="1" height="1" style="border: none; position: absolute;" /&gt;</noscript></div>									<div class="printfooter">
						Retrieved from "<a dir="ltr" href="http://en.wikipedia.org/w/index.php?title=Entropy_(information_theory)&oldid=633857614">http://en.wikipedia.org/w/index.php?title=Entropy_(information_theory)&amp;oldid=633857614</a>"					</div>
													<div id="catlinks" class="catlinks"><div id="mw-normal-catlinks" class="mw-normal-catlinks"><a href="http://en.wikipedia.org/wiki/Help:Category" title="Help:Category">Categories</a>: <ul><li><a href="http://en.wikipedia.org/wiki/Category:Entropy_and_information" title="Category:Entropy and information">Entropy and information</a></li><li><a href="http://en.wikipedia.org/wiki/Category:Information_theory" title="Category:Information theory">Information theory</a></li><li><a href="http://en.wikipedia.org/wiki/Category:Statistical_theory" title="Category:Statistical theory">Statistical theory</a></li><li><a href="http://en.wikipedia.org/wiki/Category:Randomness" title="Category:Randomness">Randomness</a></li></ul></div><div id="mw-hidden-catlinks" class="mw-hidden-catlinks mw-hidden-cats-hidden">Hidden categories: <ul><li><a href="http://en.wikipedia.org/wiki/Category:Use_dmy_dates_from_July_2013" title="Category:Use dmy dates from July 2013">Use dmy dates from July 2013</a></li><li><a href="http://en.wikipedia.org/wiki/Category:Articles_needing_additional_references_from_April_2012" title="Category:Articles needing additional references from April 2012">Articles needing additional references from April 2012</a></li><li><a href="http://en.wikipedia.org/wiki/Category:All_articles_needing_additional_references" title="Category:All articles needing additional references">All articles needing additional references</a></li><li><a href="http://en.wikipedia.org/wiki/Category:All_articles_with_unsourced_statements" title="Category:All articles with unsourced statements">All articles with unsourced statements</a></li><li><a href="http://en.wikipedia.org/wiki/Category:Articles_with_unsourced_statements_from_April_2013" title="Category:Articles with unsourced statements from April 2013">Articles with unsourced statements from April 2013</a></li><li><a href="http://en.wikipedia.org/wiki/Category:Articles_with_unsourced_quotes" title="Category:Articles with unsourced quotes">Articles with unsourced quotes</a></li><li><a href="http://en.wikipedia.org/wiki/Category:Wikipedia_articles_needing_clarification_from_July_2014" title="Category:Wikipedia articles needing clarification from July 2014">Wikipedia articles needing clarification from July 2014</a></li><li><a href="http://en.wikipedia.org/wiki/Category:Wikipedia_articles_incorporating_text_from_PlanetMath" title="Category:Wikipedia articles incorporating text from PlanetMath">Wikipedia articles incorporating text from PlanetMath</a></li></ul></div></div>												<div class="visualClear"></div>
							</div>
		</div>
		<div id="mw-navigation">
			<h2>Navigation menu</h2>

			<div id="mw-head">
									<div id="p-personal" role="navigation" class="" aria-labelledby="p-personal-label">
						<h3 id="p-personal-label">Personal tools</h3>
						<ul>
							<li id="pt-createaccount"><a href="http://en.wikipedia.org/w/index.php?title=Special:UserLogin&returnto=Entropy+%28information+theory%29&type=signup" title="You are encouraged to create an account and log in; however, it is not mandatory">Create account</a></li><li id="pt-login"><a href="http://en.wikipedia.org/w/index.php?title=Special:UserLogin&returnto=Entropy+%28information+theory%29" title="You&#39;re encouraged to log in; however, it&#39;s not mandatory. [shift-esc-o]" accesskey="o">Log in</a></li>						</ul>
					</div>
									<div id="left-navigation">
										<div id="p-namespaces" role="navigation" class="vectorTabs" aria-labelledby="p-namespaces-label">
						<h3 id="p-namespaces-label">Namespaces</h3>
						<ul>
															<li id="ca-nstab-main" class="selected"><span><a href="http://en.wikipedia.org/wiki/Entropy_(information_theory)" title="View the content page [shift-esc-c]" accesskey="c">Article</a></span></li>
															<li id="ca-talk"><span><a href="http://en.wikipedia.org/wiki/Talk:Entropy_(information_theory)" title="Discussion about the content page [shift-esc-t]" accesskey="t">Talk</a></span></li>
													</ul>
					</div>
										<div id="p-variants" role="navigation" class="vectorMenu emptyPortlet" aria-labelledby="p-variants-label">
												<h3 id="p-variants-label" tabindex="0"><span>Variants</span><a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29#" tabindex="-1"></a></h3>

						<div class="menu">
							<ul>
															</ul>
						</div>
					</div>
									</div>
				<div id="right-navigation">
										<div id="p-views" role="navigation" class="vectorTabs" aria-labelledby="p-views-label">
						<h3 id="p-views-label">Views</h3>
						<ul>
															<li id="ca-view" class="selected"><span><a href="http://en.wikipedia.org/wiki/Entropy_(information_theory)">Read</a></span></li>
															<li id="ca-edit"><span><a href="http://en.wikipedia.org/w/index.php?title=Entropy_(information_theory)&action=edit" title="You can edit this page. Please use the preview button before saving [shift-esc-e]" accesskey="e">Edit</a></span></li>
															<li id="ca-history" class="collapsible"><span><a href="http://en.wikipedia.org/w/index.php?title=Entropy_(information_theory)&action=history" title="Past versions of this page [shift-esc-h]" accesskey="h">View history</a></span></li>
													</ul>
					</div>
										<div id="p-cactions" role="navigation" class="vectorMenu emptyPortlet" aria-labelledby="p-cactions-label">
						<h3 id="p-cactions-label" tabindex="0"><span>More</span><a href="http://en.wikipedia.org/wiki/Entropy_%28information_theory%29#" tabindex="-1"></a></h3>

						<div class="menu">
							<ul>
															</ul>
						</div>
					</div>
										<div id="p-search" role="search">
						<h3>
							<label for="searchInput">Search</label>
						</h3>

						<form action="http://en.wikipedia.org/w/index.php" id="searchform">
														<div id="simpleSearch">
															<input type="search" name="search" placeholder="Search" title="Search Wikipedia [shift-esc-f]" accesskey="f" id="searchInput" tabindex="1" autocomplete="off"><input type="hidden" value="Special:Search" name="title"><input type="submit" name="go" value="Go" title="Go to a page with this exact name if one exists" id="searchButton" class="searchButton">								</div>
						</form>
					</div>
									</div>
			</div>
			<div id="mw-panel">
				<div id="p-logo" role="banner"><a class="mw-wiki-logo" href="http://en.wikipedia.org/wiki/Main_Page" title="Visit the main page"></a></div>
						<div class="portal first" role="navigation" id="p-navigation" aria-labelledby="p-navigation-label">
			<h3 id="p-navigation-label">Navigation</h3>

			<div class="body">
									<ul>
													<li id="n-mainpage-description"><a href="http://en.wikipedia.org/wiki/Main_Page" title="Visit the main page [shift-esc-z]" accesskey="z">Main page</a></li>
													<li id="n-contents"><a href="http://en.wikipedia.org/wiki/Portal:Contents" title="Guides to browsing Wikipedia">Contents</a></li>
													<li id="n-featuredcontent"><a href="http://en.wikipedia.org/wiki/Portal:Featured_content" title="Featured content – the best of Wikipedia">Featured content</a></li>
													<li id="n-currentevents"><a href="http://en.wikipedia.org/wiki/Portal:Current_events" title="Find background information on current events">Current events</a></li>
													<li id="n-randompage"><a href="http://en.wikipedia.org/wiki/Special:Random" title="Load a random article [shift-esc-x]" accesskey="x">Random article</a></li>
													<li id="n-sitesupport"><a href="https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&utm_medium=sidebar&utm_campaign=C13_en.wikipedia.org&uselang=en" title="Support us">Donate to Wikipedia</a></li>
													<li id="n-shoplink"><a href="http://shop.wikimedia.org/" title="Visit the Wikimedia Shop">Wikimedia Shop</a></li>
											</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id="p-interaction" aria-labelledby="p-interaction-label">
			<h3 id="p-interaction-label">Interaction</h3>

			<div class="body">
									<ul>
													<li id="n-help"><a href="http://en.wikipedia.org/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia">Help</a></li>
													<li id="n-aboutsite"><a href="http://en.wikipedia.org/wiki/Wikipedia:About" title="Find out about Wikipedia">About Wikipedia</a></li>
													<li id="n-portal"><a href="http://en.wikipedia.org/wiki/Wikipedia:Community_portal" title="About the project, what you can do, where to find things">Community portal</a></li>
													<li id="n-recentchanges"><a href="http://en.wikipedia.org/wiki/Special:RecentChanges" title="A list of recent changes in the wiki [shift-esc-r]" accesskey="r">Recent changes</a></li>
													<li id="n-contactpage"><a href="http://en.wikipedia.org/wiki/Wikipedia:Contact_us">Contact page</a></li>
											</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id="p-tb" aria-labelledby="p-tb-label">
			<h3 id="p-tb-label">Tools</h3>

			<div class="body">
									<ul>
													<li id="t-whatlinkshere"><a href="http://en.wikipedia.org/wiki/Special:WhatLinksHere/Entropy_(information_theory)" title="List of all English Wikipedia pages containing links to this page [shift-esc-j]" accesskey="j">What links here</a></li>
													<li id="t-recentchangeslinked"><a href="http://en.wikipedia.org/wiki/Special:RecentChangesLinked/Entropy_(information_theory)" title="Recent changes in pages linked from this page [shift-esc-k]" accesskey="k">Related changes</a></li>
													<li id="t-upload"><a href="http://en.wikipedia.org/wiki/Wikipedia:File_Upload_Wizard" title="Upload files [shift-esc-u]" accesskey="u">Upload file</a></li>
													<li id="t-specialpages"><a href="http://en.wikipedia.org/wiki/Special:SpecialPages" title="A list of all special pages [shift-esc-q]" accesskey="q">Special pages</a></li>
													<li id="t-permalink"><a href="http://en.wikipedia.org/w/index.php?title=Entropy_(information_theory)&oldid=633857614" title="Permanent link to this revision of the page">Permanent link</a></li>
													<li id="t-info"><a href="http://en.wikipedia.org/w/index.php?title=Entropy_(information_theory)&action=info" title="More information about this page">Page information</a></li>
													<li id="t-wikibase"><a href="http://www.wikidata.org/wiki/Q204570" title="Link to connected data repository item [shift-esc-g]" accesskey="g">Wikidata item</a></li>
						<li id="t-cite"><a href="http://en.wikipedia.org/w/index.php?title=Special:CiteThisPage&page=Entropy_%28information_theory%29&id=633857614" title="Information on how to cite this page">Cite this page</a></li>					</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id="p-coll-print_export" aria-labelledby="p-coll-print_export-label">
			<h3 id="p-coll-print_export-label">Print/export</h3>

			<div class="body">
									<ul>
													<li id="coll-create_a_book"><a href="http://en.wikipedia.org/w/index.php?title=Special:Book&bookcmd=book_creator&referer=Entropy+%28information+theory%29">Create a book</a></li>
													<li id="coll-download-as-rdf2latex"><a href="http://en.wikipedia.org/w/index.php?title=Special:Book&bookcmd=render_article&arttitle=Entropy+%28information+theory%29&oldid=633857614&writer=rdf2latex">Download as PDF</a></li>
													<li id="t-print"><a href="http://en.wikipedia.org/w/index.php?title=Entropy_(information_theory)&printable=yes" title="Printable version of this page [shift-esc-p]" accesskey="p">Printable version</a></li>
											</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id="p-lang" aria-labelledby="p-lang-label"><span class="uls-settings-trigger" tabindex="0" role="button" aria-haspopup="true" original-title="Language settings"></span>
			<h3 id="p-lang-label">Languages</h3>

			<div class="body">
									<ul>
													<li class="interlanguage-link interwiki-ar"><a href="http://ar.wikipedia.org/wiki/%D8%A7%D8%B9%D8%AA%D9%84%D8%A7%D8%AC_(%D9%85%D8%B9%D9%84%D9%88%D9%85%D8%A7%D8%AA)" title="اعتلاج (معلومات) – Arabic" lang="ar" hreflang="ar">العربية</a></li>
													<li class="interlanguage-link interwiki-bg"><a href="http://bg.wikipedia.org/wiki/%D0%95%D0%BD%D1%82%D1%80%D0%BE%D0%BF%D0%B8%D1%8F_%D0%BD%D0%B0_%D0%A8%D0%B0%D0%BD%D1%8A%D0%BD" title="Ентропия на Шанън – Bulgarian" lang="bg" hreflang="bg">Български</a></li>
													<li class="interlanguage-link interwiki-bar"><a href="http://bar.wikipedia.org/wiki/Entropie_(Informationstheorie)" title="Entropie (Informationstheorie) – Bavarian" lang="bar" hreflang="bar">Boarisch</a></li>
													<li class="interlanguage-link interwiki-bs"><a href="http://bs.wikipedia.org/wiki/Entropija_(teorija_informacija)" title="Entropija (teorija informacija) – Bosnian" lang="bs" hreflang="bs">Bosanski</a></li>
													<li class="interlanguage-link interwiki-ca"><a href="http://ca.wikipedia.org/wiki/Entropia_de_Shannon" title="Entropia de Shannon – Catalan" lang="ca" hreflang="ca">Català</a></li>
													<li class="interlanguage-link interwiki-cy"><a href="http://cy.wikipedia.org/wiki/Entropi_gwybodaeth" title="Entropi gwybodaeth – Welsh" lang="cy" hreflang="cy">Cymraeg</a></li>
													<li class="interlanguage-link interwiki-da"><a href="http://da.wikipedia.org/wiki/Entropi_(informationsteori)" title="Entropi (informationsteori) – Danish" lang="da" hreflang="da">Dansk</a></li>
													<li class="interlanguage-link interwiki-de"><a href="http://de.wikipedia.org/wiki/Entropie_(Informationstheorie)" title="Entropie (Informationstheorie) – German" lang="de" hreflang="de">Deutsch</a></li>
													<li class="interlanguage-link interwiki-el"><a href="http://el.wikipedia.org/wiki/%CE%95%CE%BD%CF%84%CF%81%CE%BF%CF%80%CE%AF%CE%B1_%CF%80%CE%BB%CE%B7%CF%81%CE%BF%CF%86%CE%BF%CF%81%CE%B9%CF%8E%CE%BD" title="Εντροπία πληροφοριών – Greek" lang="el" hreflang="el">Ελληνικά</a></li>
													<li class="interlanguage-link interwiki-es"><a href="http://es.wikipedia.org/wiki/Entrop%C3%ADa_(informaci%C3%B3n)" title="Entropía (información) – Spanish" lang="es" hreflang="es">Español</a></li>
													<li class="interlanguage-link interwiki-fa"><a href="http://fa.wikipedia.org/wiki/%D8%A2%D9%86%D8%AA%D8%B1%D9%88%D9%BE%DB%8C_%D8%A7%D8%B7%D9%84%D8%A7%D8%B9%D8%A7%D8%AA" title="آنتروپی اطلاعات – Persian" lang="fa" hreflang="fa">فارسی</a></li>
													<li class="interlanguage-link interwiki-fr"><a href="http://fr.wikipedia.org/wiki/Entropie_de_Shannon" title="Entropie de Shannon – French" lang="fr" hreflang="fr">Français</a></li>
													<li class="interlanguage-link interwiki-ko"><a href="http://ko.wikipedia.org/wiki/%EC%A0%95%EB%B3%B4_%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC" title="정보 엔트로피 – Korean" lang="ko" hreflang="ko">한국어</a></li>
													<li class="interlanguage-link interwiki-it"><a href="http://it.wikipedia.org/wiki/Entropia_(teoria_dell%27informazione)" title="Entropia (teoria dell&#39;informazione) – Italian" lang="it" hreflang="it">Italiano</a></li>
													<li class="interlanguage-link interwiki-he"><a href="http://he.wikipedia.org/wiki/%D7%90%D7%A0%D7%98%D7%A8%D7%95%D7%A4%D7%99%D7%94_(%D7%A1%D7%98%D7%98%D7%99%D7%A1%D7%98%D7%99%D7%A7%D7%94)" title="אנטרופיה (סטטיסטיקה) – Hebrew" lang="he" hreflang="he">עברית</a></li>
													<li class="interlanguage-link interwiki-lt"><a href="http://lt.wikipedia.org/wiki/Entropija_(informacijos_teorija)" title="Entropija (informacijos teorija) – Lithuanian" lang="lt" hreflang="lt">Lietuvių</a></li>
													<li class="interlanguage-link interwiki-hu"><a href="http://hu.wikipedia.org/wiki/Shannon-entr%C3%B3piaf%C3%BCggv%C3%A9ny" title="Shannon-entrópiafüggvény – Hungarian" lang="hu" hreflang="hu">Magyar</a></li>
													<li class="interlanguage-link interwiki-nl"><a href="http://nl.wikipedia.org/wiki/Entropie_(informatietheorie)" title="Entropie (informatietheorie) – Dutch" lang="nl" hreflang="nl">Nederlands</a></li>
													<li class="interlanguage-link interwiki-ja"><a href="http://ja.wikipedia.org/wiki/%E6%83%85%E5%A0%B1%E9%87%8F" title="情報量 – Japanese" lang="ja" hreflang="ja">日本語</a></li>
													<li class="interlanguage-link interwiki-mhr"><a href="http://mhr.wikipedia.org/wiki/%D0%A8%D0%B5%D0%BD%D0%BD%D0%BE%D0%BD%D1%8B%D0%BD_%D1%84%D0%BE%D1%80%D0%BC%D1%83%D0%BB%D0%B6%D0%BE" title="Шеннонын формулжо – Eastern Mari" lang="mhr" hreflang="mhr">Олык марий</a></li>
													<li class="interlanguage-link interwiki-pl"><a href="http://pl.wikipedia.org/wiki/Entropia_(teoria_informacji)" title="Entropia (teoria informacji) – Polish" lang="pl" hreflang="pl">Polski</a></li>
													<li class="interlanguage-link interwiki-pt"><a href="http://pt.wikipedia.org/wiki/Entropia_da_informa%C3%A7%C3%A3o" title="Entropia da informação – Portuguese" lang="pt" hreflang="pt">Português</a></li>
													<li class="interlanguage-link interwiki-ro"><a href="http://ro.wikipedia.org/wiki/Entropie_informa%C8%9Bional%C4%83" title="Entropie informațională – Romanian" lang="ro" hreflang="ro">Română</a></li>
													<li class="interlanguage-link interwiki-ru"><a href="http://ru.wikipedia.org/wiki/%D0%98%D0%BD%D1%84%D0%BE%D1%80%D0%BC%D0%B0%D1%86%D0%B8%D0%BE%D0%BD%D0%BD%D0%B0%D1%8F_%D1%8D%D0%BD%D1%82%D1%80%D0%BE%D0%BF%D0%B8%D1%8F" title="Информационная энтропия – Russian" lang="ru" hreflang="ru">Русский</a></li>
													<li class="interlanguage-link interwiki-simple"><a href="http://simple.wikipedia.org/wiki/Information_entropy" title="Information entropy – Simple English" lang="simple" hreflang="simple">Simple English</a></li>
													<li class="interlanguage-link interwiki-sk"><a href="http://sk.wikipedia.org/wiki/Entropia_(te%C3%B3ria_inform%C3%A1ci%C3%AD)" title="Entropia (teória informácií) – Slovak" lang="sk" hreflang="sk">Slovenčina</a></li>
													<li class="interlanguage-link interwiki-sl"><a href="http://sl.wikipedia.org/wiki/Entropija_(informatika)" title="Entropija (informatika) – Slovenian" lang="sl" hreflang="sl">Slovenščina</a></li>
													<li class="interlanguage-link interwiki-ckb"><a href="http://ckb.wikipedia.org/wiki/%D8%A6%D8%A7%D9%86%D8%AA%D8%B1%DB%86%D9%BE%DB%8C%DB%8C_%D8%B2%D8%A7%D9%86%DB%8C%D8%A7%D8%B1%DB%8C" title="ئانترۆپیی زانیاری – Sorani Kurdish" lang="ckb" hreflang="ckb">کوردی</a></li>
													<li class="interlanguage-link interwiki-sr"><a href="http://sr.wikipedia.org/wiki/%D0%95%D0%BD%D1%82%D1%80%D0%BE%D0%BF%D0%B8%D1%98%D0%B0_(%D1%82%D0%B5%D0%BE%D1%80%D0%B8%D1%98%D0%B0_%D0%B8%D0%BD%D1%84%D0%BE%D1%80%D0%BC%D0%B0%D1%86%D0%B8%D1%98%D0%B0)" title="Ентропија (теорија информација) – Serbian" lang="sr" hreflang="sr">Српски / srpski</a></li>
													<li class="interlanguage-link interwiki-su"><a href="http://su.wikipedia.org/wiki/%C3%89ntropi_informasi" title="Éntropi informasi – Sundanese" lang="su" hreflang="su">Basa Sunda</a></li>
													<li class="interlanguage-link interwiki-sv"><a href="http://sv.wikipedia.org/wiki/Entropi_(informationsteori)" title="Entropi (informationsteori) – Swedish" lang="sv" hreflang="sv">Svenska</a></li>
													<li class="interlanguage-link interwiki-te"><a href="http://te.wikipedia.org/wiki/%E0%B0%B8%E0%B0%AE%E0%B0%BE%E0%B0%9A%E0%B0%BE%E0%B0%B0_%E0%B0%B8%E0%B0%82%E0%B0%95%E0%B0%B0%E0%B0%A4_(%E0%B0%87%E0%B0%A8%E0%B1%8D%E0%B0%AB%E0%B0%B0%E0%B1%8D%E0%B0%AE%E0%B1%87%E0%B0%B7%E0%B0%A8%E0%B1%8D_%E0%B0%8E%E0%B0%82%E0%B0%9F%E0%B1%8D%E0%B0%B0%E0%B1%8A%E0%B0%AA%E0%B1%80)" title="సమాచార సంకరత (ఇన్ఫర్మేషన్ ఎంట్రొపీ) – Telugu" lang="te" hreflang="te">తెలుగు</a></li>
													<li class="interlanguage-link interwiki-th"><a href="http://th.wikipedia.org/wiki/%E0%B9%80%E0%B8%AD%E0%B8%99%E0%B9%82%E0%B8%97%E0%B8%A3%E0%B8%9B%E0%B8%B5%E0%B8%82%E0%B8%AD%E0%B8%87%E0%B8%82%E0%B9%89%E0%B8%AD%E0%B8%A1%E0%B8%B9%E0%B8%A5" title="เอนโทรปีของข้อมูล – Thai" lang="th" hreflang="th">ไทย</a></li>
													<li class="interlanguage-link interwiki-uk"><a href="http://uk.wikipedia.org/wiki/%D0%86%D0%BD%D1%84%D0%BE%D1%80%D0%BC%D0%B0%D1%86%D1%96%D0%B9%D0%BD%D0%B0_%D0%B5%D0%BD%D1%82%D1%80%D0%BE%D0%BF%D1%96%D1%8F" title="Інформаційна ентропія – Ukrainian" lang="uk" hreflang="uk">Українська</a></li>
													<li class="interlanguage-link interwiki-ur"><a href="http://ur.wikipedia.org/wiki/%D8%AF%D8%B1%D9%85%D8%A7%D8%A6%D9%84%D8%AA_(%D8%A7%D8%B7%D9%84%D8%A7%D8%B9%D8%A7%D8%AA%DB%8C_%D9%86%D8%B8%D8%B1%DB%8C%DB%81)" title="درمائلت (اطلاعاتی نظریہ) – Urdu" lang="ur" hreflang="ur">اردو</a></li>
													<li class="interlanguage-link interwiki-vi"><a href="http://vi.wikipedia.org/wiki/Entropy_th%C3%B4ng_tin" title="Entropy thông tin – Vietnamese" lang="vi" hreflang="vi">Tiếng Việt</a></li>
													<li class="interlanguage-link interwiki-zh"><a href="http://zh.wikipedia.org/wiki/%E7%86%B5_(%E4%BF%A1%E6%81%AF%E8%AE%BA)" title="熵 (信息论) – Chinese" lang="zh" hreflang="zh">中文</a></li>
													
											</ul>
				<div class="after-portlet after-portlet-lang"><span class="wb-langlinks-edit wb-langlinks-link"><a action="edit" href="http://www.wikidata.org/wiki/Q204570#sitelinks-wikipedia" text="Edit links" title="Edit interlanguage links" class="wbc-editpage">Edit links</a></span></div>			</div>
		</div>
				</div>
		</div>
		<div id="footer" role="contentinfo">
							<ul id="footer-info">
											<li id="footer-info-lastmod"> This page was last modified on 14 November 2014 at 21:05.<br></li>
											<li id="footer-info-copyright">Text is available under the <a rel="license" href="http://en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License">Creative Commons Attribution-ShareAlike License</a><a rel="license" href="http://creativecommons.org/licenses/by-sa/3.0/" style="display:none;"></a>;
additional terms may apply.  By using this site, you agree to the <a href="http://wikimediafoundation.org/wiki/Terms_of_Use">Terms of Use</a> and <a href="http://wikimediafoundation.org/wiki/Privacy_policy">Privacy Policy</a>. Wikipedia® is a registered trademark of the <a href="http://www.wikimediafoundation.org/">Wikimedia Foundation, Inc.</a>, a non-profit organization.</li>
									</ul>
							<ul id="footer-places">
											<li id="footer-places-privacy"><a href="http://wikimediafoundation.org/wiki/Privacy_policy" title="wikimedia:Privacy policy">Privacy policy</a></li>
											<li id="footer-places-about"><a href="http://en.wikipedia.org/wiki/Wikipedia:About" title="Wikipedia:About">About Wikipedia</a></li>
											<li id="footer-places-disclaimer"><a href="http://en.wikipedia.org/wiki/Wikipedia:General_disclaimer" title="Wikipedia:General disclaimer">Disclaimers</a></li>
											<li id="footer-places-contact"><a href="http://en.wikipedia.org/wiki/Wikipedia:Contact_us">Contact Wikipedia</a></li>
											<li id="footer-places-developers"><a href="https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute">Developers</a></li>
											<li id="footer-places-mobileview"><a href="http://en.m.wikipedia.org/w/index.php?title=Entropy_(information_theory)&mobileaction=toggle_view_mobile" class="noprint stopMobileRedirectToggle">Mobile view</a></li>
									</ul>
										<ul id="footer-icons" class="noprint">
											<li id="footer-copyrightico">
															<a href="http://wikimediafoundation.org/"><img src="./Entropy (information theory) - Wikipedia, the free encyclopedia_files/wikimedia-button.png" srcset="//bits.wikimedia.org/images/wikimedia-button-1.5x.png 1.5x, //bits.wikimedia.org/images/wikimedia-button-2x.png 2x" width="88" height="31" alt="Wikimedia Foundation"></a>
													</li>
											<li id="footer-poweredbyico">
															<a href="http://www.mediawiki.org/"><img src="./Entropy (information theory) - Wikipedia, the free encyclopedia_files/poweredby_mediawiki_88x31.png" alt="Powered by MediaWiki" width="88" height="31"></a>
													</li>
									</ul>
						<div style="clear:both"></div>
		</div>
		<script>/*<![CDATA[*/window.jQuery && jQuery.ready();/*]]>*/</script><script>if(window.mw){
mw.loader.state({"ext.globalCssJs.site":"ready","ext.globalCssJs.user":"ready","site":"loading","user":"ready","user.groups":"ready"});
}</script>
<script>if(window.mw){
mw.loader.load(["ext.cite","mediawiki.toc","mediawiki.action.view.postEdit","mediawiki.user","mediawiki.hidpi","mediawiki.page.ready","mediawiki.searchSuggest","ext.gadget.teahouse","ext.gadget.ReferenceTooltips","ext.gadget.DRN-wizard","ext.gadget.charinsert","ext.gadget.refToolbar","ext.gadget.featured-articles-links","mmv.bootstrap.autostart","ext.eventLogging.subscriber","ext.navigationTiming","schema.UniversalLanguageSelector","ext.uls.eventlogger","ext.uls.interlanguage"],null,true);
}</script>
<script>if(window.mw){
document.write("\u003Cscript src=\"//bits.wikimedia.org/en.wikipedia.org/load.php?debug=false\u0026amp;lang=en\u0026amp;modules=site\u0026amp;only=scripts\u0026amp;skin=vector\u0026amp;*\"\u003E\u003C/script\u003E");
}</script><script src="./Entropy (information theory) - Wikipedia, the free encyclopedia_files/load(2).php"></script>
<script>if(window.mw){
mw.config.set({"wgBackendResponseTime":285,"wgHostname":"mw1044"});
}</script>
	

<div><div id="SL_button" class="ImTranslatorLogo"></div><div id="SL_shadow_translation_result2" style="display: none;"></div><div id="SL_shadow_translator" style="display: none;"><div id="SL_planshet"><div id="SL_TB"><div class="ImTranslatorLogo"></div><table cellspacing="1" border="0" id="SL_tables"><tbody><tr><td class="SL_td" align="left" width="20%"><div id="SL_lng_from">English&nbsp;»</div></td><td class="SL_td" align="left" width="20%"><div id="SL_to_"><div id="SL_tocurlng" style="margin-left:5px;">Georgian</div><div class="SL_arrow"></div><div id="SL_to_list" style="display=" none'=""><div class="SL_DDMENU"><div id="SL_af" class="SL_limenu">Afrikaans</div><div id="SL_sq" class="SL_limenu">Albanian</div><div id="SL_ar" class="SL_limenu">Arabic</div><div id="SL_hy" class="SL_limenu">Armenian</div><div id="SL_az" class="SL_limenu">Azerbaijani</div><div id="SL_eu" class="SL_limenu">Basque</div><div id="SL_bn" class="SL_limenu">Bengali</div><div id="SL_be" class="SL_limenu">Belarusian</div><div id="SL_bg" class="SL_limenu">Bulgarian</div><div id="SL_ca" class="SL_limenu">Catalan</div><div id="SL_zh_CN" class="SL_limenu">Chinese&nbsp;(Simp)</div><div id="SL_zh_TW" class="SL_limenu">Chinese&nbsp;(Trad)</div><div id="SL_hr" class="SL_limenu">Croatian</div><div id="SL_cs" class="SL_limenu">Czech</div><div id="SL_da" class="SL_limenu">Danish</div><div id="SL_nl" class="SL_limenu">Dutch</div><div id="SL_en" class="SL_limenu">English</div><div id="SL_eo" class="SL_limenu">Esperanto</div><div id="SL_et" class="SL_limenu">Estonian</div><div id="SL_tl" class="SL_limenu">Filipino</div><div id="SL_fi" class="SL_limenu">Finnish</div><div id="SL_fr" class="SL_limenu">French</div><div id="SL_gl" class="SL_limenu">Galician</div><div id="SL_ka" class="SL_limenu">Georgian</div><div id="SL_de" class="SL_limenu">German</div><div id="SL_el" class="SL_limenu">Greek</div><div id="SL_gu" class="SL_limenu">Gujarati</div><div id="SL_ht" class="SL_limenu">Haitian&nbsp;Creole</div><div id="SL_iw" class="SL_limenu">Hebrew</div><div id="SL_hi" class="SL_limenu">Hindi</div><div id="SL_hu" class="SL_limenu">Hungarian</div><div id="SL_is" class="SL_limenu">Icelandic</div><div id="SL_id" class="SL_limenu">Indonesian</div><div id="SL_ga" class="SL_limenu">Irish</div><div id="SL_it" class="SL_limenu">Italian</div><div id="SL_ja" class="SL_limenu">Japanese</div><div id="SL_kn" class="SL_limenu">Kannada</div><div id="SL_ko" class="SL_limenu">Korean</div><div id="SL_lo" class="SL_limenu">Lao</div><div id="SL_la" class="SL_limenu">Latin</div><div id="SL_lv" class="SL_limenu">Latvian</div><div id="SL_lt" class="SL_limenu">Lithuanian</div><div id="SL_mk" class="SL_limenu">Macedonian</div><div id="SL_ms" class="SL_limenu">Malay</div><div id="SL_mt" class="SL_limenu">Maltese</div><div id="SL_no" class="SL_limenu">Norwegian</div><div id="SL_fa" class="SL_limenu">Persian</div><div id="SL_pl" class="SL_limenu">Polish</div><div id="SL_pt" class="SL_limenu">Portuguese</div><div id="SL_ro" class="SL_limenu">Romanian</div><div id="SL_ru" class="SL_limenu">Russian</div><div id="SL_sr" class="SL_limenu">Serbian</div><div id="SL_sk" class="SL_limenu">Slovak</div><div id="SL_sl" class="SL_limenu">Slovenian</div><div id="SL_es" class="SL_limenu">Spanish</div><div id="SL_sw" class="SL_limenu">Swahili</div><div id="SL_sv" class="SL_limenu">Swedish</div><div id="SL_ta" class="SL_limenu">Tamil</div><div id="SL_te" class="SL_limenu">Telugu</div><div id="SL_th" class="SL_limenu">Thai</div><div id="SL_tr" class="SL_limenu">Turkish</div><div id="SL_uk" class="SL_limenu">Ukrainian</div><div id="SL_ur" class="SL_limenu">Urdu</div><div id="SL_vi" class="SL_limenu">Vietnamese</div><div id="SL_cy" class="SL_limenu">Welsh</div><div id="SL_yi" class="SL_limenu">Yiddish</div></div></div></div><select id="SL_lng_to"><option value="af">Afrikaans</option><option value="sq">Albanian</option><option value="ar">Arabic</option><option value="hy">Armenian</option><option value="az">Azerbaijani</option><option value="eu">Basque</option><option value="bn">Bengali</option><option value="be">Belarusian</option><option value="bg">Bulgarian</option><option value="ca">Catalan</option><option value="zh-CN">Chinese&nbsp;(Simp)</option><option value="zh-TW">Chinese&nbsp;(Trad)</option><option value="hr">Croatian</option><option value="cs">Czech</option><option value="da">Danish</option><option value="nl">Dutch</option><option value="en">English</option><option value="eo">Esperanto</option><option value="et">Estonian</option><option value="tl">Filipino</option><option value="fi">Finnish</option><option value="fr">French</option><option value="gl">Galician</option><option value="ka">Georgian</option><option value="de">German</option><option value="el">Greek</option><option value="gu">Gujarati</option><option value="ht">Haitian&nbsp;Creole</option><option value="iw">Hebrew</option><option value="hi">Hindi</option><option value="hu">Hungarian</option><option value="is">Icelandic</option><option value="id">Indonesian</option><option value="ga">Irish</option><option value="it">Italian</option><option value="ja">Japanese</option><option value="kn">Kannada</option><option value="ko">Korean</option><option value="lo">Lao</option><option value="la">Latin</option><option value="lv">Latvian</option><option value="lt">Lithuanian</option><option value="mk">Macedonian</option><option value="ms">Malay</option><option value="mt">Maltese</option><option value="no">Norwegian</option><option value="fa">Persian</option><option value="pl">Polish</option><option value="pt">Portuguese</option><option value="ro">Romanian</option><option value="ru">Russian</option><option value="sr">Serbian</option><option value="sk">Slovak</option><option value="sl">Slovenian</option><option selected="" value="es">Spanish</option><option value="sw">Swahili</option><option value="sv">Swedish</option><option value="ta">Tamil</option><option value="te">Telugu</option><option value="th">Thai</option><option value="tr">Turkish</option><option value="uk">Ukrainian</option><option value="ur">Urdu</option><option value="vi">Vietnamese</option><option value="cy">Welsh</option><option value="yi">Yiddish</option></select></td><td class="SL_td" width="13%" align="center">&nbsp;</td><td class="SL_td" width="8%" align="center"><div id="SL_TTS_voice" title=""></div></td><td class="SL_td" width="8%" align="center"><div class="SL_copy_hand" id="SL_copy" title="Select text"></div></td><td class="SL_td" width="8%" align="center"><div class="SL_font_off" id="SL_bbl_font" title="Font size"></div></td><td class="SL_td" width="5%"></td><td class="SL_td" width="8%" align="right"><div class="SL_pin_off" id="SL_pin" title="Pin pup-up bubble"></div></td></tr></tbody></table></div></div><div id="SL_shadow_translation_result"></div><div id="SL_bbl_donate" title="Make a small contribution"></div><div id="SL_Balloon_options"><a href="chrome-extension://pcfaommkmdjacdkbaoohklbccfmbnnod/options-bbl.html" target="_blank" class="SL_options" title="Show options">Options</a></div></div></div><div class="suggestions" style="display: none; font-size: 13px;"><div class="suggestions-results"></div><div class="suggestions-special"></div></div><script id="hiddenlpsubmitdiv" style="display: none;"></script><script>try{for(var lastpass_iter=0; lastpass_iter < document.forms.length; lastpass_iter++){ var lastpass_f = document.forms[lastpass_iter]; if(typeof(lastpass_f.lpsubmitorig2)=="undefined"){ lastpass_f.lpsubmitorig2 = lastpass_f.submit; lastpass_f.submit = function(){ var form=this; var customEvent = document.createEvent("Event"); customEvent.initEvent("lpCustomEvent", true, true); var d = document.getElementById("hiddenlpsubmitdiv"); if (d) {for(var i = 0; i < document.forms.length; i++){ if(document.forms[i]==form){ d.innerText=i; } } d.dispatchEvent(customEvent); }form.lpsubmitorig2(); } } }}catch(e){}</script></body></html>